{
  "hash": "56b04b8b2cd6e654f8af202db9980692",
  "result": {
    "markdown": "\n# Random Variables\n\n\n\n\n\n\n::: {.description}\n* **random variable** a random process that results in a number\n::: \n<!-- end description -->\n\n\nWe have already seen notation for and a few examples of random variables.\nIn this chapter we will learn a bit more about random variables.  We will\nfocus our attention on two types of random variables: discrete random variables\nand continuous random variables.^[There are important examples\nof random variables that are neither discrete nor continuous.]\n\n## Discrete Random Variables\n\nA discrete random variable takes on values from a discrete set of possibilities,\ntypically either a finite set or a subset of the integers.  Here are some examples.\n\n::: {.enumerate}\n\n#. If we roll a die and record the number, there are six possible values,\nso the random variable is discrete.\n#. If we keep flipping a coin until we get a head and record the number of coin\ntosses, then the possible values of the random variable are $1, 2, 3, \\dots\nThis is also a discrete random variable.\n::: \n<!-- end enumerate -->\n\n\n\nFor each of the possible values of a discrete random variable, there is some\nprobability of that value occuring.  So to specify a discrete random variable,\nwe need to specify those probabilities.  When there are only a small number \nof possible value, we can do this with a table.\n\n:::{.center}\n+-------------------+-----+-----+------+\n| value of $X$      |  0  |  1  |   2  |\n+-------------------+-----+-----+------+\n| probability       | 0.2 | 0.5 | 0.3  |\n+-------------------+-----+-----+------+\n:::\n\nThis is really just one way of describing a function, called the \n**probability mass function** (or pmf).  The pmf satisfies\n$$\nf(x) = \\Prob(X = x)\n$$ {#eq-pmf}\n\nSometimes instead of providing a table, we will be able to specify\nthe pmf using a formula.  For example, we could define a pmf $g$ by\n\n$$ \ng(y) = (2 - |1-y|) / 4 \\mbox{ for $y \\in \\{0, 1, 2\\}$} \n$$\nwhich is the same as specifying $g$ with the following table.\n\n\n:::{.center}\n+-------------------+------+------+-------+\n| value of $Y$      |  0   |  1   |  2    |\n+-------------------+------+------+-------+\n| probability       | 0.25 | 0.5  | 0.25  |\n+-------------------+------+------+-------+\n:::\n\nHere's one more example.  Let $W$ be a random varaible that can take on any integer\nvalue and has pmf given by\n\n$$\nh(w) = \\left(\\frac{1}{2}\\right)^{w+1} \\mbox {for $w = 0, 1, 2, 3, \\dots$}\n$$\n\nSo, for example, $\\Prob(W = 2) = h(2) = \\left(\\frac12 \\right)^3 = \\frac18$.\n\nProbabilities can be obtained from the pmf by adding:\n\n\n* $\\Prob( X > 0 ) = 0.5 + 0.3 - 0.8$\n* $\\Prob( Y > 0) = 0.5 + 0.25$\n* $\\Prob(W > 0) = \\frac{1}{4} + \\frac{1}{8} + \\frac{1}{16} + \\cdots = \\frac{1}{2}$\n* $\\Prob(W > 0) = 1 - \\Prob(W = 0) = 1 - \\frac{1}{2} = \\frac{1}{2}$\n\nThe only restrictions on a pmf are that \n\n::: {.enumerate}\n#. the values must all be non-negative, and \n#. the sum (over all possible values of the random variable) must be 1.\n::: \n<!-- end enumerate -->\n\nThat way the probabilities will behave the way probabilities should.\n\n\n::: {.example #exm-freethrow-amy}\n\n**Q.** Amy is a 92% free throw shooter. We watch her take shots until she misses and let \n$X$ be the number of shots.  What is the pmf for $X$?\n\n**A.** This time our table would be infinite (since there is no limit to how many consecutive\nfree throws Amy might make), so we won't be able to write the whole table down.\nBut we can work out the first few probabilities:\n\n::: {.itemize}\n\n* $f(0) = \\Prob(X = 0) = 0.08$.  (She has to miss the first shot.)\n* $f(1) = \\Prob(X = 1) = (0.92)(0.08)$.  (She has to make the first and miss the second.)\n* $f(2) = \\Prob(X = 2) = (0.92)^2(0.08)$.  (She has to make the first two, then miss the third.)\n::: \n<!-- end itemize -->\n\nAt this point, we see there is a general pattern that allows us to write down an \nalgebraic form for the pmf:\n\n$$\nf(x) = \\Prob(X = x) = (0.92)^{x-1}(0.08)\n$$\nwhere $x$ is a positive integer.  (If $x$ is not an integer, or $x < 1$, then $f(x) = 0$, since\nthose values are not possible.)\n::: \n<!-- end example -->\n\n\n\n## Continuous Random Variables\n\n### Density histograms, density plots, density functions\nA histogram is a simple picture describing the \"density\" of data. \nHistogram bars are tall in regions where there is more data -- i.e., where the data are \nmore \"dense\".  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(alr4)\ngf_histogram(  ~ Duration, data = oldfaith)\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/histograms-count-density-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_dhistogram( ~ Duration, data = oldfaith)\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/histograms-count-density-2.png){width=432}\n:::\n:::\n\n\n\nThe density scale is the same scale that is used by `gf_dens()()` and `gf_density()()`, \nand it is the default scale for histograms created using `gf_dhistogram()()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(alr4)\ngf_dhistogram( ~ Duration, data = oldfaith, binwidth = 20, center = 110) |>\ngf_dens( ~ Duration, data = oldfaith)\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/histogram-density-1.png){width=432}\n:::\n:::\n\n\n\nThe density scale is chosen so that the area of each rectangular bar (width times height)\nis equal to the proportion of the data set represented by the rectangle. \n\n::: {.example #exm-old-faithful}\n\n**Q.** Use the histogram of Old Faithful eruption times to estimate\n\tthe proportion of eruptions that last between 100 and 120 seconds.\n\n**A.** In our histogram of Old Faithful eruption durations, the bar corresponding to the \nbin from 100--120 appears to have a height of about 0.09.  That gives an area of \n0.18 and indicates that approximately 18% of the eruptions last between 100 and 120 \nseconds.\n\n::: {.cell}\n\n```{.r .cell-code}\ntally( ~ ( 100 < Duration & Duration <= 120), data = oldfaith, format = \"prop\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(100 < Duration & Duration <= 120)\n     TRUE     FALSE \n0.1888889 0.8111111 \n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\nThe key idea behind the density scale can be expressed as \n\n::: {.boxedText}\n:::: {.center}\nProbability $=$ area\n:::: \n:::\n\n<!-- end center -->\n\nThis association of area with probability means that\nthe total area of all the bars will always be equal to 1 if we use the density\nscale.  \n\nIt also provides us with a way to describe a distribution with a mathematical function.\n\n\n::: {.boxedText #def-pdf}\n\nLet $f: \\reals \\to \\reals$ be a function such that \n\n#. $f(x) \\ge  0$ for all $x$,\n#. $\\displaystyle  \\int_{-\\infty}^{\\infty} f(x) \\; dx = 1$.\n\nThen $f$ is called a **density function** \n(or probability density function, abbreviated pdf) \nand describes a continuous random variable $X$ such that\n\n$$\n   \\Prob(a \\le X \\le b) = \\int_a^b f(x) \\; dx \\;.\n$$ { #eq-integral-for-probability }\n::: \n\n<!-- end boxedText -->\n\n\n:::{.example #exm-triangle}\n\n**Q.** Let $f$ be defined by\n$$\n\tf(x) = \\begin{cases}  \n\t\t1 - |x| & x \\in [-1,1] \\\\\n\t\t0 & \\mbox{otherwise}\\\\\n\t\\end{cases}\n$$\nShow that $f$ is a density function.  Let $X$ be the associated random \nvariable, and compute the following probabilities:\n\n1. $\\Prob(X\\le0)$\n2. $\\Prob(X \\le 1)$\n3. $\\Prob(X \\le \\frac12)$\n4. $\\Prob(-\\frac12 X \\le \\frac12)$\n\n**A.**\nWhile we could set up integrals for these, it is easier to solve them using \ngeometry.^[R cleverly turns TRUE and FALSE into 1 and 0 when you use them in arithmetic\nexpressions.  The definition of `f()` makes use of this conversion to simplify specifying\nthe cases.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x )\ngf_fun( f(x) ~ x, xlim = c(-1.5, 1.5) )\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-3-1.png){width=432}\n:::\n:::\n\n\n\nThe entire area under the curve can be found as the area of a triangle with base 2 and height 1.\n$$\n\\int_{-\\infty}^{\\infty} f(x) \\; dx \n=\n\\int_{-1}^1 f(x) \\; dx\n=\n\\frac 12 \\cdot 2 \\cdot 1 = 1\n$$\nThis implies that $f$ is a density function.\n\n1. $\\Prob(X \\le 1)  = \\int_{-\\infty}^{1} f(x) \\; dx  =\n\\int_{-1}^{1} f(x) \\; dx = 1$.\n\n2. $\\Prob(X \\le \\frac12) = \\int_{-\\infty}^{1/2} f(x) \\; dx  =\n\\int_{-1}^{1/2} f(x) \\; dx = 1 - \\frac12 \\cdot \\frac12 \\cdot \\frac12 = \\frac78$.\n\n3. $\\Prob( -\\frac12 \\le X \\le \\frac 12 )  = \\int_{-1/2}^{1/2} f(x) \\; dx =\n\t1 - \\frac{2}{8} = \\frac{3}{4}$.\n\nWe can also let R do (numerical) integration for us.  There are two ways to do this.\nThe first method uses the `integrate()()` function.\n\n::: {.cell}\n\n```{.r .cell-code}\nintegrate( f, -Inf, 1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1 with absolute error < 9.2e-05\n```\n:::\n\n```{.r .cell-code}\n# this will be more accurate since we aren't asking R to approximate\n# something that we already know is exactly 0\nintegrate( f, -1, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1 with absolute error < 1.1e-14\n```\n:::\n\n```{.r .cell-code}\nintegrate( f, -.5, .5 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.75 with absolute error < 8.3e-15\n```\n:::\n\n```{.r .cell-code}\n# if you just want the value without the text saying how accurate the approximation is\n# here are two equivalent ways to get it\nvalue(integrate( f, -.5, .5 ))        # extract the value using val()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.75\n```\n:::\n\n```{.r .cell-code}\nintegrate( f, -.5, .5 ) |> value()   # |> is the \"then\" operator\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.75\n```\n:::\n:::\n\n\n\nAn alternative approach uses `antiD()()` from the **`mosaicCalc`** package.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mosaicCalc)\nF <- antiD( f(x) ~ x)\nF(1) - F(-1)        # total probability -- better be 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nF(.5) - F(-1)       # P( -1 <= X <= 0.5 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.875\n```\n:::\n\n```{.r .cell-code}\nF(.5) - F(-.5)      # P( -.5 <= X <= .5 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.75\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n### Kernels\n\nThe **kernel** of a continuous random variable is a function that is a \nconstant multiple of the pdf.  The reason that these are interesting\nis that any kernel can be converted into a pdf by dividing by this constant.\nIn particular, if \n\n$$ \n\\int_{-\\infty}^{\\infty} k(x) \\; dx = A \\;, \n$$\nthen $k$ is the kernel of a random variable with pdf\n$$ \nf(x) = \\frac{k(x)}{A} \\;.\n$$\n\n:::{.example #exm-kernel}\n\n**Q.** The kernel of a random variable is given by \n\n$$\nk(x) = x^2 \\; \\boolval{ x \\in [0,2] } \\; .\n$$\nDetermine the pdf.\n\n**A.** First we determine the value of the integral\n$$\n\\int_{-\\infty}^{\\infty} k(x) \\; dx \\;. \n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- makeFun( x^2 * ( 0 <= x & x <= 2) ~ x )\nplotFun(k(x) ~ x, xlim = c(-1,3))\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-6-1.png){width=432}\n:::\n\n```{.r .cell-code}\nintegrate( k, 0, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.666667 with absolute error < 3e-14\n```\n:::\n\n```{.r .cell-code}\nK <- antiD(k(x) ~ x, lower.bound = 0)\nK(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.666667\n```\n:::\n:::\n\n\nSince the total area is $8/3$, if $\\frac{k(x)}{8/3}$ is the pdf.\n:::\n\n### Cumulative distribution functions\n\n\n<!-- % The \\textbf{cumulative distribution function} (cdf) returns the probability of being -->\n<!-- % less than or equal to some value.  We generally use a capital letter ($F$, $G$, etc.) -->\n<!-- % to denote a cdf. -->\n<!-- % $$ -->\n<!-- % F_X(x) = \\Prob( X \\le x)  \\; . -->\n<!-- % $$ -->\n\n<!-- % If we help R choose the anti-derivative, we get a useful function called  -->\n<!-- % the \\term{cumulative distribution function}, abbreviated cdf. -->\n\n\n::: {.boxedText}\n\nIf $X$ is a random variable, then the **cumulative distribution function** (cdf) for \n$X$, often denoted $F_X$, is the function defined by\n$$\nF_X(x)  = \\Prob(X \\le  x)\n$$\nThat is, the output of the cdf reports the probability of being below a particular value.\n::: \n<!-- end boxedText -->\n\n\nFor a continuous random variable, the cdf is a particular anti-derivative\nof the pdf.  The derivative of the cdf is the pdf.\n\n::: {.example #exm-kernel-continued}\n\nContinuing with our previous example, if we choose -1 as our lower endpoint,\nthen the anti-derivative will be the cdf.\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun((1 - abs(x)) * (abs(x) <= 1) ~ x)\nF <- antiD( f(x) ~ x, lower.bound = -1)   # We can use -1 instead of -Inf here.\nF(-1)               # this should be 0 since we chose -1 as the lower bound.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nF(1)                # P(X <= 1); should be 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nF(.5)               # P(X <= 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.875\n```\n:::\n\n```{.r .cell-code}\nF(.5) - F(-.5)      # P( -0.5 <= X <= 0.5 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.75\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n<!-- ## Working with Probability Density Funcitons -->\n\nWe have already seen that we can use a pdf $f$ to calculate probabilities via integration, \nand that there is a special anti-derivative of $f$ called the cdf such that the cdf $F$\nsatisfies\n$$\nF(x) = \\Prob(X \\le x)\n$$\nThis function can also be used to compute probabilities, since\n$$\n\\Prob(a \\le X \\le b) = \\int_a^b f(x) \\; dx = F(b) - F(a)\n$$\nIndeed, once we learn how to get the cdf function in R this will \nbe our primary way to calculate probabilities in applications.\n\n## Mean and Variance\n\n### The mean of a random variable\n\nThe definition for the mean of a random variable will be motivated \nby the calculation of a mean of some data.\n\n::: {.example #exm-gpa}\n\n**Q.**   Suppose a student has taken $10$ courses\n  and received $5$ A's, $4$ B's, and $1$ C.  Using the traditional numerical scale where \n  an A is worth $4$, a B is worth $3$, and a C is worth $2$, what is this student's \n  GPA (grade point average)?\n\n**A.** The first thing to notice is that $\\frac{4 + 3 + 2}{3} = 3$ is \n*not* correct. We cannot simply add up the values and divide by the number of values.  Clearly\nthis student should have a GPA that is higher than $3.0$, since there were more A's than\nC's.\n\nConsider now a correct way to do this calculation:\n\n$$\n\\begin{aligned}\n\t\\mbox{GPA} &= \\frac{4 + 4 + 4 + 4 + 4 + 3 + 3 + 3 + 3 + 2}{10}\n\t\\\\[2mm]\n\t& = \\frac{5\\cdot 4 + 4\\cdot 3 + 1 \\cdot 2}{10} \\\\[1mm]\n\t& = \\frac{5}{10} \\cdot 4 + \\frac{4}{10}\\cdot 3 + \\frac{1}{10} \\cdot 2 \\\\\n\t& = 4 \\cdot \\frac{5}{10} + 3 \\cdot \\frac{4}{10} + 2 \\cdot \\frac{1}{10} \\\\\n\t& =  3.4 \\;.\n\\end{aligned}\n$$\n\n:::\n\nThe key idea here is that the mean is a **sum of values times probabilities**.  \n\n$$\n\\mbox{mean} = \\sum \\mbox{value} \\cdot \\mbox{probability}\n$$\nFor a discrete random variable this translates to\n$$\n\\E(X) = \\sum x f(x)\n$$\nwhere the sum is taken over all possible values of $X$.\n\nThe mean of a random variable also goes by another name: **expected value**.  We \ncan denote the mean of $X$ by either $\\mu_X$ or $\\E(X)$.\n\n::: {.example #exm-mean-exp-value-1}\n\nLet $X$ be discrete random variable with probablilities given in the table \nbelow.\n\n:::: {.center}\n\n+--------------------+-----+-----+-----+\n+ value of $X$       |  0  |  1  | 2   | \n+--------------------+-----+-----+-----+\n+ probability        | 0.2 | 0.5 | 0.3 |\n+--------------------+-----+-----+-----+\n::::\n\n<!-- end center -->\n\n\n**Q.** What is the mean (expected value) of $X$?\n\n**A.** $E(X) = 0 \\cdot 0.2 + 1 \\cdot 0.5 + 2 \\cdot 0.3 = 0.5 + 0.6 = 1.1$\nThis value reflects the fact that the random variable is larger than\n1 a bit more often than it is less than 1.\n::: \n<!-- end example -->\n\n\n\n::: {.example #exm-raffle}\n\nA local charity is holding a raffle.  They are selling $1000$ raffle tickets \nfor \\$$5$ each.  The owners of five of the raffle tickets will win a prize.  The \nfive prizes are valued at \\$$25$, \\$$50$, \\$$100$, \\$$1000$, and \\$$2000$.  Let $X$ \nbe the value of the prize associated with a random raffle ticket ($0$ for \nnon-winning tickets). \nThen:\n\n::: {.itemize}\n\n* $\\evProb{the ticket wins a prize} = \\Prob(X > 0) = 5/1000$.\n* $\\evProb{the ticket wins the grand prize} = \\Prob(X = 2000) = 1/1000$.\n* $\\evProb{the ticket wins a prize worth more than \\$75}  = \\Prob(X > 75) = 3/1000$.\n::: \n<!-- end itemize -->\n\nThe expected value of a ticket is\n  \n$$\n  0 \\cdot\\frac{995}{1000} \n  + 25 \\cdot\\frac{1}{1000}\n  + 50 \\cdot\\frac{1}{1000}\n  + 100 \\cdot\\frac{1}{1000}\n  + 1000 \\cdot\\frac{1}{1000}\n  + 2000 \\cdot\\frac{1}{1000}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n25 * .001 + 50 * 0.001 + 100 * 0.001 + 1000 * 0.001 + 2000 * 0.001\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.175\n```\n:::\n\n```{.r .cell-code}\n# R can help us set up this sum:\nsum(c(25, 50, 100, 1000, 2000) * 0.001)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.175\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\nWhen working with a continuous random variable, \nwe replace the sum with an integral and replace the \nprobabilities with our density function to get the following definition:\n\n$$\n\\E(X) = \\mu_X = \\int_{-\\infty}^{\\infty} x f(x) \\; dx\n$$\n\nIf you recall doing center of mass problems you may recognize this integral as\nthe first moment. (For pdfs, we don't need to divide by the ``mass'' because the\ntotal ``mass'' is the area under the curve, which will always be 1 for a random\nvariable).\n\nNote: It is possible that the integral used to define the mean will fail to converge.\nIn that case, we say that the random variable has no mean or that the mean fails to \nexist.^[Actually, we will require that $\\int_{\\infty}^{\\infty} |x| f(x) \\; dx$ converges.  \nIf this integral fails to converge, we will also say that the distribution has no mean.]\n\n::: {.example #exm-mean-triangle}\n\n**Q.** Compute the mean of our triangle distribution from @exm-triangle-in-R.\n\n**A.** We simply compute the integral from the definition.\n\n$$\n\\begin{aligned}\n\\E(X) & = \\int_{-1}^{1} x f(x) \\; dx \n\\\\\n\t& = \\int_{-1}^{0} x (x-1) \\; dx + \\int_{0}^1 x ( 1-x ) \\; dx \n\t\\\\\n\t& = \\int_{-1}^{0} x^2-x) \\; dx + \\int_{0}^1 x-x^2 ) \\; dx  \n\t\\\\\n\t& = \\left. \\frac{x^3}{3} - \\frac{x^2}{2} \\right|_{-1}^0\n\t+ \\left. \\frac{x^2}{2} - \\frac{x^3}{3} \\right|_{0}^1\n\t\\\\\n\t& = \\frac13 - \\frac12 + \\frac12 - \\frac13 = 0\n\\end{aligned}\n$$\n\nThis isn't surprising, by symmetry we would expect this result.\n\nWe could also calculate this numerically in R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x)\nxf <- makeFun( x * f(x) ~ x )\nintegrate(xf, -1, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0 with absolute error < 3.7e-15\n```\n:::\n\n```{.r .cell-code}\nF <- antiD( x * f(x) ~ x, lower.bound = -1)\nF(-1)  # should be 0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nF(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n### Variance\n\nArguing similarly, we can compute the variance of a discrete or continuous random\nvariable using \n\n::: {.itemize}\n\n* discrete: $\\Var(X) = \\sigma^2_X = \\sum x (x-\\mu_X)^2$\n\n* continuous: $\\Var(X) = \\sigma^2_X = \\int_{-\\infty}^{\\infty} (x-\\mu_X)^2 f(x) \\; dx$\n::: \n<!-- end itemize -->\n\nThese can be combined into a single definition by writing\n$$\n\\Var(X) = \\E((X - \\mu_X)^2) \\;.\n$$\n\nNote: It is possible that the sum or integral used to define the mean (or the variance) will fail \nto converge. In that case, we say that the random variable has no mean (or variance) or that the \nmean (or variance)  fails to exist.^[Actually, we will require that \n$\\int_{\\infty}^{\\infty} |x| f(x) \\; dx$ converges and   \n$\\int_{\\infty}^{\\infty} |x|^2 f(x) \\; dx$ converges.  \nIf these integrals (or the corresponding sums for discrete random variables) fail to converge, \nwe will say that the distribution has no mean (or variance).]\n\n::: {.example #exm-var-triangle}\n\n**Q.** Compute the variance of the triangle random variable from the @exm-triangle.\n\t\n**A.** \n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x)\nxxf <- makeFun( (x-0)^2 * f(x) ~ x )\nintegrate(xxf, -1, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.1666667 with absolute error < 1.9e-15\n```\n:::\n\n```{.r .cell-code}\nG <- antiD( (x-0)^2 * f(x) ~ x)\nG(1) - G(-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1666667\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\nSome simple algebraic manipulations of the sum or integral above shows that\n\n$$\n\\begin{aligned}\n\\Var(X) &= \\E(X^2) - \\E(X)^2\n\\end{aligned}\n$$ {#eq-var-shortcut}\n\n\n::: {.problem #exr-show-pdf}\n \nLet $f(x) = 5/4 - x^3$  on $[0,1]$.  \n\n1. Show that $f$ is a pdf.\n2. Calculate $\\Prob(X \\le\\frac12)$.\n3. Calculate $\\Prob(X \\ge\\frac12)$.\n4. Calculate $\\Prob(X = \\frac12)$.\n\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun( (5/4 - x^3) * ( abs(x-.5) <= .5 ) ~ x )\nplotFun(f(x) ~ x, x.lim = c(-1,2))  # quick plot to make sure things look correct.\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-11-1.png){width=432}\n:::\n\n```{.r .cell-code}\nF <- antiD(f(x) ~x)  \n# part a:  f(x) >=0, so we just need to check that the total area is 1\nF(1) - F(0) == 1  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nF(1/2) - F(0)     # part b\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.609375\n```\n:::\n\n```{.r .cell-code}\nF(1) - F(1/2)     # part c\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.390625\n```\n:::\n\n```{.r .cell-code}\nF(1/2) - F(1/2)   # part d\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n\n\n::: {.example #exm-compute-mean-var}\n\n**Q.** Compute the mean and variance of the random variable with pdf given by\n\t\n$$ \ng(x) = \\frac{3x^2}{8} \\boolval{x \\in[0,2]} \\;. \n$$\n\nThis is the pdf computed in @exm-kernel.\n\t\n**A.** \n\n::: {.cell}\n\n```{.r .cell-code}\ng <- makeFun( (3 * x^2/8 ) * (0 <= x & x <= 2) ~ x )\nm <- antiD( x * g(x) ~ x, lower.bound = 0)(2)  # all in one step instead of defining F or G\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.5\n```\n:::\n\n```{.r .cell-code}\nv <- antiD( (x - m)^2 * g(x) ~ x, m = m, lower.bound = 0)(2)\nv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.15\n```\n:::\n\n```{.r .cell-code}\n# here's the alternate computation\nantiD( x^2 * g(x) ~ x, lower.bound = 0)(2) - m^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.15\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\nAs with data, the standard deviation is the square root of the variance.\n\n### Quantiles\n\nQuantiles solve equations of the form\n\n$$\n\\int_{-\\infty}^x f(t) \\; dt = F(x) = \\Prob(X \\le x) = q\n$$\n\nwhere $q$ is known and $x$ is unknown.  So the 50th percentile (which is the 0.5-quantile\nor the median) is the number such that \n\n$$\n\\Prob(X \\le x) = 0.5 \\;.\n$$\n\n::: {.example #exm-triangle-quantiles}\n\n**Q.** What is the 25th percentile of the triangle distribution in @exm-triangle?\n\n**A.** \tWe need to solve for $x$ in the following equation:\n\n$$\n0.25 = \\Prob(X \\le x)  \\; .\n$$\nWe can do this by working out the integral involved:\n\n$$\n\\begin{aligned}\n0.25 &= \\int_{-1}^{x} 1 - |t| \\; dt \n\t\\\\\n\t &= \\int_{-1}^{x} 1 + t \\; dt \n\t \\\\\n\t &=  \\left( t + t^2/2 \\right) |_{-1}^{x}\n\t \\\\\n\t &=   x + x^2/2 + 1 - 1^2/2  \n\t \\\\\n\t &=  x + x^2/2 + 1/2\n\t \\\\\n\t 0 & =  x^2/2 + x + 1/4\n\t \\\\\n\t 0 & =  2x^2 + 4x + 1\n\\end{aligned}\n$$\n\nSo by the quadratic formula, $x = \\frac{1}{2} \\sqrt{2} - 1 = -0.2928932$.\n\nWe can check this by evaluating the cdf.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1/2*sqrt(2) - 1\nF(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n\nThis could also be done geometrically by solving $\\frac{1}{2} y^2 = \\frac14$ and letting\n$x = -1 + y$.\n::: \n<!-- end example -->\n\n\n\n## Some Important Families of Distributions\n\nFor now, we will consider only distributions of continuous random variables\n(probability density functions).  We will leave set aside discrete random\nvariables (probability mass function) until quite a bit later in the course.\n\nA family of distributions is a collection of distributions that share some\ncommon features.  Typically, these are described by giving a pdf that has one or\nmore **parameters**. A parameter is simply a number that describes (a feature\nof) a distribution that distinguishes between members of the family. In this\nsection we describe briefly some of the important distributions and how to work\nwith them in R\n\n### Triangle Distributions\n\nThe example distribution in the previous section is usually referred to as a\ntriangle distribution (or triangular distribution) because of the shape of its\npdf.  There are, of course, many triangle distributions.  A triangle\ndistribution is specified with three numbers: $a$, the minimum; $b$, the\nmaximum, and $c$, the location of the peak. A triangle distribution is symmetric\nif the peak is halfway between the minimum and maximum ($c = \\frac{a+b}{2}$).\n\nWhen $X$ is a random variable with a triangle distribution, we will write\n$X \\sim\\Tri(a,b,c)$.  For many of the most common\ndistributions, R has several functions that facilitate computation with\nthose distributions.  The triangle distributions are not in the base R \ndistribution, but they can be added by requiring the **`triangle`** package.\n\nFor each distribution, there are four functions in R that always start\nwith a single letter followed by a name for the distribution.  In the case \nof the triangle distributions, these functions are \n\n\n| Function                 | What it does                                                                                   |\t\n|--------------------------|------------------------------------------------------------------------------------------------|\n| `dtriangle(x, a, b, c)`  | Computes value of the pdf at `x`.                                                              |\n| `ptriangle(q, a, b, c)`  | Computes value of the cdf at `x`, i.e., $\\Prob(X \\le \\texttt{q})$.                             |\n|\t`qtriangle(p, a, b, c)`  | Computes quantiles, that is a value $q$ so that $\\Prob(X \\le \\texttt{q}) = \\texttt{p}$.        |\n|\t`rtriangle(n, a, b, c)`  | Randomly samples `n` values from the $\\Tri(\\texttt{a},\\texttt{b},\\texttt{c})$ distribution.    |\n\n: Triangle distributions in R {#tbl-triangle-in-R}\n\n\n::: {.example #exm-triangle-in-R}\n\n**Q.** Let $X \\sim\\Tri0,4,1)$.  Use R to answer the following questions.\n\n:::: {.enumerate}\na. Plot the pdf for $X$.\na. What is $\\Prob(X \\le 1)$?\na. What is $\\Prob(X \\le 2)$?\na. What is the median of $X$?\na. What is the mean of $X$?\n:::: \n<!-- end enumerate -->\n\n\t\n**A.** The `gf_dist()` function in the **ggformula** package allows us to \ngraph the pdf for any function R knows how to work with in the standard way.\nFor example, here is a plot of the pdf of a $\\Tri(0, 4, 1)$-distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(triangle)      # a package that knows about triangle distributions\ngf_dist(\"triangle\", a = 0, b = 4, c = 1)\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-14-1.png){width=432}\n:::\n:::\n\n\nHere is the R code to answer the remaining questions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nptriangle(1, 0, 4, 1)   # P(X <= 4); notice that his is NOT 1/2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.25\n```\n:::\n\n```{.r .cell-code}\nptriangle(2, 0, 4, 1)   # P(X <= 4); also NOT 1/2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6666667\n```\n:::\n\n```{.r .cell-code}\nqtriangle(0.5, 0, 4, 1) # median is the 0.5-quantile\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.55051\n```\n:::\n\n```{.r .cell-code}\nT <- antiD( x * dtriangle(x, 0,4,1) ~ x, lower.bound = 0)\nT(4)                    # mean of X\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.666667\n```\n:::\n\n```{.r .cell-code}\nintegrate( makeFun( x * dtriangle(x, 0,4,1) ~ x) , 0, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.666667 with absolute error < 1.9e-14\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n\n::: {.problem #exr-triangle-with-geometry}\n\nRepeat parts (2) -- (4) of @exm-triangle-in-R using geometry \nrather than R\n\n::: \n<!-- end problem -->\n\n\n\n::: {.solution}\n\n:::: {.itemize}\n\n* $\\Prob(X \\le 1) = \\frac 12 \\cdot 1 \\cdot \\frac12 = \\frac 14$\n* $\\Prob(X \\le 2) = 1 - \\Prob(X \\ge 2) =  1 - \\frac12 \\cdot 2 \\cdot \\frac13  = 1 - \\frac 13 = \\frac23$.\n* The median $m$ is a number between 1 and 2 and satisfies\n\t\t\t$\\frac 12 = \\Prob(X \\ge m) = \\frac12 (4-m) \\frac{4-m}{6}$.  \n\t\t\tSolving for $m$ we get $m = 4 - \\sqrt{6} = 1.5505103$.\n:::: \n<!-- end itemize -->\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-pdf-from-kernel}\n\nLet \n$$\n\\displaystyle k(x) = (1 - x^2) \\cdot\\boolval{ x \\in[-1,1] } = \n\\begin{cases}\n\t\t1-x^2 & x \\in[-1,1] \\\\ \n\t\t0 & \\mbox{otherwise} \n\\end{cases}\n$$ \n\nbe the kernel of a continuous distribution.\n\n:::: {.enumerate}\n\na. \t\t\tDetermine the pdf for this distribution.\na. \t\t\tCompute the mean and variance for this distribution\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\tThe integrals are easy enough to do by hand, but here is the R code \n\tto compute them.\n\t\n<!-- TODO: originally had area as an argument, but changes to mosaicCalc seem to break this -->\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- makeFun( 1 - x^2 ~ x )\nK <- antiD( k(x) ~ x )\narea <- K(1) - K(-1); area \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.333333\n```\n:::\n\n```{.r .cell-code}\nf <- makeFun( (1 - x^2)/area ~ x)\nF <- antiD(f(x) ~ x)\nF(1) - F(-1)    # this should be 1 if we have done things right\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nG <- antiD( x * f(x) ~ x )\nH <- antiD( x^2 * f(x) ~ x )\nm <- G(1) - G(-1); m               # E(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nH(1) - H(-1)                       # E(X^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2\n```\n:::\n\n```{.r .cell-code}\nH(1) - H(-1) - m^2                 # Var(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-mean-median-triangle}\n\nLet $Y \\sim\\Tri(0,10,4)$.  Compute $\\E(Y)$ and the median of $Y$.\n::: \n<!-- end problem -->\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mean:\nm <- antiD( x * dtriangle(x,0,10,4) ~ x, lower.bound = 0)(10)\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.666665\n```\n:::\n\n```{.r .cell-code}\n# variance:\nantiD( x^2 * dtriangle(x,0,10,4) ~ x, lower.bound = 0)(10) - m^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.222229\n```\n:::\n\n```{.r .cell-code}\n# median\nqtriangle( 0.5, 0, 10, 4 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.522774\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n### Uniform Distributions\n\nA uniform distribution is a described by a constant function over some interval.  Its \nshape is a rectangle.  This makes it particularly easy to calculate probabilities \nfor a uniform distribution.  Despite its simplicity, the family of uniform distributions\nhas many applications.\n\nWe will let $X \\sim \\Unif(a,b)$ denote that $X$ is a uniform random variable on \nthe interval from $a$ to $b$. \nIn R, the parameters $a$ and $b$ are given more meaningful names: `min` and `max`.\nWe can use the following code to graph the $\\Unif (1,4)$ distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dist(\"unif\", min = 1, max = 4, xlim = c(-1, 6)) \n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-18-1.png){width=432}\n:::\n:::\n\n\nNotice that the width of the non-zero portion of the pdf is 3, so the height must be $1/3$.\n\nProbabilities involving uniform distributions are easily calculated using simple geometry,\nbut R also provides several functions for working with uniform probability distributions.\n\n| Function             | What it does                                                                              |\n|----------------------|-------------------------------------------------------------------------------------------|\n| `dunif(x, min, max)` | Computes value of the pdf at `x`.                                                         |\n| `punif(q, min, max)` | Computes value of the cdf at `x`, i.e., $\\Prob(X \\le \\texttt{q})$.                        |\n| `qunif(p, min, max)` | Computes quantiles, that is a value $q$ so that  $\\Prob(X \\le \\texttt{q}) = \\texttt{p}$.  |\n| `runif(n, min, max)` | Randomly samples `n` values from the $\\Unif(\\texttt{min},\\texttt{max})$ distribution.     |\n\n: Uniform distributions in R {#tbl-unif}\n\n\nNotice the pattern to these names.  They start with the same letters as \nthe functions for the triangle distributions, but replace `triangle`\nwith `unif`. \n*There are similar functions for all of the distributions in this chapter.*\n\n::: {.example #exm-unif-in-R}\n\n**Q.** \tLet $X \\sim \\Unif(1,4)$.  Use R to calculate the following values and \n\tcheck the values using geometry:\n\t\n:::: {.enumerate}\n\na. $\\Prob(X \\le 2)$\nb. the 80th percentile of the distribution\n:::: \n<!-- end enumerate -->\n\n\n**A.** \n\n::: {.cell}\n\n```{.r .cell-code}\npunif(2,1,4)   # P(X <= 2 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3333333\n```\n:::\n\n```{.r .cell-code}\n(2-1) * 1/3    # P(X <= 2 ) using area\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3333333\n```\n:::\n\n```{.r .cell-code}\nqunif(.8, 1,4) # 80th percentile\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.4\n```\n:::\n:::\n\n\nWe could also get the 80th percentile by solving the equation $\\frac{3}{(x-1)} = 0.8$\nFrom this we get $\\frac{x}{3} = 0.8 + 1/3$, so $x = 3 ( 0.8 + 1/3) = 2.4 + 1 = 3.4$.\n::: \n<!-- end example -->\n\n\n::: {.problem #exr-exp-var-unif}\nLet $W \\sim \\Unif(0,10)$.  Compute $\\E(W)$ and $\\Var(W)$.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mean:\nm <- antiD( x * dunif(x,0,10) ~ x, lower.bound = 0)(10)\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5\n```\n:::\n\n```{.r .cell-code}\n# variance:\nantiD( x^2 * dunif(x,0,10) ~ x, lower.bound = 0)(10) - m^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.333333\n```\n:::\n:::\n\n::: \n<!-- end solution -->\n\n\n\n### Exponential Distributions\n\nThe exponential distributions are useful for modeling the time until some \"event\" \noccurs.  The model is based on the assumptions that \n\n:::: {.enumerate}\n\na. The probability of an event occurring in any small interval of time \n\t is proportional to the length of the time interval.  The constant\n\t of proportionality is the rate parameter, usually denoted by $\\lambda$.\n\t \na. The probabilities of events occurring in two small non-overlapping intervals\n\t are independent.\n:::: \n<!-- end enumerate -->\n\n\n::: {.example #exm-exponential-dist}\n\nHere are some situations that might be well modeled by an exponential distribution:\n\n:::: {.enumerate}\na. The time until the next radioactive decay event is detected on a Geiger counter \n\na. The time until a space satellite is struck by a meteor (or some other space junk) \n\t and disabled.\n\n\t The model would be good if (over some time span of interest) the chances of getting\n\t struck are always the same.  It would not be such a good model if the satellite moves\n\t through time periods of relatively higher and then relatively lower chances of being\n\t struck (perhaps because we pass through regions of more or less space debris at \n\t different times of the year.)\n\n#. The lifetime of some manufactured device.\n\n\t This is a pretty simple model (we'll learn better ones later) and most often\n\t is *too* simple to describe the interesting features of the lifetime of a\n\t device.  In this model, failure is due to some external thing \"happening to\"\n\t the device; the device itself does not wear (or improve) over time.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end example -->\n\n\nWe will let $X \\sim \\Exp(\\lambda)$ denote that $X$ has an exponential distribution\nwith rate parameter $\\lambda$. The kernel of such a distribution is \n$$\nk(x; \\lambda) = e^{-\\lambda x} \\; \\boolval{x \\ge 0}\n$$\nNotice that the function describing this distribution is defined only for x-values that are real numbers greater than or equal to zero (in mathematical notation, the interval $[0, \\infty)$.) This interval is sometimes called the ``support\" of the distribution.  When using probability distributions to model data, it's important to think about whether the support of the distribution matches well with the range of possible values observed in the data.\n\nThe exponential distribution function is a pretty easy function to integrate, but R provides the now familiar\nfunctions to make things even easier.\n\n\n| Function               | What it does                                               \t                                |\n|------------------------|----------------------------------------------------------------------------------------------|\n| `dexp(x, rate)`        | Computes value of the pdf at `x`.                                                            |\n| `pexp(q, rate)`        | Computes value of the cdf at `x`, i.e., $\\Prob(X \\le \\texttt{q})$.                           |\n|\t`qexp(p, rate)`        | Computes quantiles, that is a value $q$ so that $\\Prob(X \\le \\texttt{q}) = \\texttt{p}$.      | \n|\t`rexp(n, rate)`        | Randomly samples `n` values from the $\\Exp(\\lambda)$ distribution where $\\lambda$ = `rate`.  |\n\n: Exponential distributions in R {#tbl-exp}\n\n<!-- \\iffalse -->\n<!-- \\begin{boxedText} -->\n<!-- \tWe will write $X \\sim \\Exp(\\lambda)$ to indicate that the random variable $X$ has  -->\n<!-- \tan exponential distribution with rate parameter $\\lambda$. -->\n<!-- \t\\begin{center} -->\n<!-- \t\t\\begin{tabular}{lll} -->\n<!-- \t\t\t\\hline  -->\n<!-- \t\t\tparameter & $\\lambda$ & \\texttt{rate} -->\n<!-- \t\t\t\\\\ -->\n<!-- \t\t\tkernel & $e^{-\\lambda x}$ -->\n<!-- \t\t\t\\\\ -->\n<!-- \t\t\tpdf & $f(x; \\lambda) = \\lambda e^{-\\lambda x} \\boolval{x\\ge 0}$  -->\n<!-- \t\t\t& \\texttt{dexp(x,rate)} -->\n<!-- \t\t\t\\\\ -->\n<!-- %\t\t\tmean & $\\frac{1}{\\lambda}$ -->\n<!-- %\t\t\t\\\\ -->\n<!-- %\t\t\tvariance & $\\frac{1}{\\lambda^2}$ -->\n<!-- %\t\t\t\\\\  -->\n<!-- %\t\t\t\\hline -->\n<!-- \t\t\\end{tabular} -->\n<!-- \t\\end{center} -->\n<!-- \\end{boxedText} -->\n<!-- \\fi -->\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dist(\"exp\", rate = 4)\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/gf-dist-eponential-1.png){width=432}\n:::\n:::\n\n\n\n::: {.problem #exr-exp-of-exponential}\n\n:::: {.enumerate}\n\na. Let $X \\sim \\Exp(4)$.  Use R to compute $\\E(X)$.\na. Let $X \\sim \\Exp(10)$.  Use R to compute $\\E(X)$.\na. Let $X \\sim \\Exp(1/5)$.  Use R to compute $\\E(X)$.\na. What pattern do you notice.  Explain in terms of the definition of the exponential \ndistribution why this makes sense.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nantiD( x * dexp(x, 4) ~ x, lower.bound = 0)(Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.25\n```\n:::\n\n```{.r .cell-code}\nantiD( x * dexp(x, 10) ~ x, lower.bound = 0)(Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1\n```\n:::\n\n```{.r .cell-code}\nantiD( x * dexp(x, 1/5) ~ x, lower.bound = 0)(Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5\n```\n:::\n:::\n\n\nIt appears that the mean of an $\\Exp(\\lambda)$-distribution is $1/\\lambda$.  This\nmakes sense.  If events have at a rate of $30$ per hour, we would expect to wait\n$1/30$ of an hour (on average) for the first event to happen.\n::: \n<!-- end solution -->\n\n\n### Gamma and Weibull Distributions\n\nThe Gamma and Weibull familities of distributions are generalizations of the exponential\ndistribution.  Each family has two parameters, a rate parameter as in \nthe exponential distribution, and an additional parameter called the shape \nparameter (denoted by $\\alpha$ below).  The reciprocal of the rate parameter\nis called the scale parameter.  For the Gamma distribution, R lets us use\neither rate or scale (and the default is rate).  For the Weibull, we must use the \nscale.\n\n+----------------------------------------+----------------------------------------------------------+\n+ distribution                           + kernel                                                   +\n+========================================+==========================================================+\n| $\\Gamm(\\alpha, \\lambda)$               | $ k(x) = x^{\\alpha} e^{-\\lambda x}$ for $x \\ge 0$        |\n+----------------------------------------+----------------------------------------------------------+\n| $\\Weibull(\\alpha, \\lambda)$            | $ k(x) = x^{\\alpha} e^{-\\lambda x^{\\alpha}}$ for $x > 0$ |\n+----------------------------------------+----------------------------------------------------------+\n\n: Kernels for Gamma and Weibull distributions.\n\n\nBoth families of distributions are supported on the interval $[0, \\infty$.)\nFor the most part, we won't use these formulas in calculations, \npreferring to let R do the work for us. However, notice that each\nof these distributions has a pdf that allows for relatively simple integration.  For the \nGamma distributions, we need to use integration by parts ($\\alpha- 1$ times).  For \nthe Weibull distributions we can use a substitution: $u = x^{\\alpha}$.\nIn each case, when $\\alpha= 1$ we get an exponential distribution.\n\nThe now familiar functions are available for each of these distributions.\n\n| Function                                      | What it does                                                                            |\t\n|------------------------|----------------------------------------------------------------------------------------------------------------|\n| `dgamma(x, shape, rate = 1, scale = 1/rate)`  | Computes value of the pdf at `x`.                                                       |\n| `pgmma(x, shape, rate = 1, scale = 1/rate)`   | Computes value of the cdf at `x`, i.e., $\\Prob(X \\le \\texttt{q})$.                      |\n|\t`qgamma(x, shape, rate = 1, scale = 1/rate)`  | Computes quantiles, that is a value $q$ so that $\\Prob(X \\le \\texttt{q}) = \\texttt{p}$. |\n|\t`rgamma(n, shape, rate, scale = 1/rate)`      | Randomly samples `n` values from the Gamma distribution.                                |\n\n: Gamma distributions in R.\n\n| Function                                      | What it does                                                                                |\t\n|-----------------------------------------------|---------------------------------------------------------------------------------------------|\n| `dweibull(x, shape, scale = 1)`               | Computes value of the pdf at `x`.                                                           |\n| `pweibull(x, shape, scale = 1)`               | Computes value of the cdf at `x`, i.e., $\\Prob(X \\le \\texttt{q})$.                          | \n| `qweibull(x, shape, scale = 1)`               | Computes quantiles, that is a value $q$ so that $\\Prob(X \\le \\texttt{q}) = \\texttt{p}$.     |\n|\t`rweibull(n, shape, scale = 1)`               | Randomly samples `n` values from the Weibull distribution.                                  |\n\n: Weibull distributions in R.\n\n<!-- $\\Gamm(\\alpha, \\lambda)$ or                               | -->\n<!-- |                                               | $\\Weibull(\\alpha, \\beta)$ distribution where.             | -->\n<!-- |                                               |   $\\alpha$ = `shape`, $\\lambda$ = `rate`,                 | -->\n<!-- |                                               |   and $\\beta$ = `scale`                                   | -->\n<!-- +-----------------------------------------------+-----------------------------------------------------------+ -->\n\nLike the exponential distributions, these distributions are skewed and only take \non positive values.  \nThese distributions arise in many applications, including as more general models \nfor lifetime.  As the pictures below indicate, the shape and scale parameters\nare aptly named.\n\n\n::: {#fig-gamma-dists .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ngf_dist(\"gamma\", params = list(shape = 2, rate = 1), title = \"Gamma(2,1)\")\ngf_dist(\"gamma\", params = list(shape = 5, rate = 1), title = \"Gamma(5,1)\")\ngf_dist(\"gamma\", params = list(shape = 2, scale = 10), title = \"Gamma(5,10)\")\ngf_dist(\"gamma\", params = list(shape = 5, scale = 10), title = \"Gamma(5,10)\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-gamma-dists-1.png){#fig-gamma-dists-1 width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-gamma-dists-2.png){#fig-gamma-dists-2 width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-gamma-dists-3.png){#fig-gamma-dists-3 width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-gamma-dists-4.png){#fig-gamma-dists-4 width=432}\n:::\n\nSome example gamma distributions.\n:::\n\n::: {#fig-weibull-dists .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ngf_dist(\"weibull\", params = list(shape = 2, scale = 1),title = \"Weibull(2,1)\")\ngf_dist(\"weibull\", params = list(shape = 5, scale = 1),title = \"Weibull(5,1)\")\ngf_dist(\"weibull\", params = list(shape = 2, scale = 10),title = \"Weibull(2,10)\")\ngf_dist(\"weibull\", params = list(shape = 5, scale = 10),title = \"Weibull(5,10)\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-weibull-dists-1.png){#fig-weibull-dists-1 width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-weibull-dists-2.png){#fig-weibull-dists-2 width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-weibull-dists-3.png){#fig-weibull-dists-3 width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-weibull-dists-4.png){#fig-weibull-dists-4 width=432}\n:::\n\nSome example Weibull distributions.\n:::\n\n\n\n### Normal Distributions\n\nWe come now to the most famous family of distributions -- the normal\ndistributions (also called Gaussian distributions).  These symmetric \ndistributions have the famous ``bell shape'' and are described by two parameters, \nthe mean $\\mu$ and the standard deviation $\\sigma$.  The pdf for a $\\Norm(\\mu, \\sigma)$\ndistribution is \n\n$$ \nf(x) = \\frac{ 1}{\\sigma\\sqrt{2\\pi}} e^{-(x-\\mu)^2/2\\sigma^2}\n$$ {#eq-normal-pdf}\n\n<!-- +---------------------------------------+--------------------------------------------------------+ -->\n<!-- | distribution                          | pdf                                                    | -->\n<!-- +=======================================+========================================================+ -->\n<!-- |\t$\\displaystyle \\Norm(\\mu, \\sigma)$    | $\\displaystyle f(x) =$                                 | -->\n<!-- |                                       | $\\frac{ 1}{\\sigma\\sqrt{2\\pi}} e^{-(x-\\mu)^2/2\\sigma^2}$| -->\n<!-- +---------------------------------------+--------------------------------------------------------+ -->\n\n<!-- : The normal pdf. {#tbl-normal-pdf} -->\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nThe inflection points \nof the normal distributions are always at $\\mu -\\sigma$ and $\\mu+\\sigma$.\n\nAmong the normal distributions is one special distribution -- the **standard normal\ndistribution** -- which has mean 0 and standard deviation 1. All other normal \ndistributions are simply linear transformations of the standard normal distribution.\nThat is,\nIf $Z \\sim \\Norm(0,1)$ and $Y = a + b X$ , then $Y \\sim \\Norm(a, b)$. \nConversely, if $Y \\sim \\Norm(\\mu,\\sigma)$, then \n$Z = \\frac{Y - \\mu}{\\sigma} \\sim \\Norm(0,1)$.\n\nAs with the other distributions we have encountered, we have four functions\nthat allow us to work with normal distributions in R\n\n| Function                     | What it does                                                                                                       |\t\n|------------------------------|--------------------------------------------------------------------------------------------------------------------|\n| `dnorm(x, mean = 0, sd = 1)` | Computes value of the pdf at `x`.                                                                                  |\n| `pnorm(q, mean = 0, sd = 1)` | Computes value of the cdf at `x`, i.e., $\\Prob(X \\le \\texttt{q})$.                                                 |\n|\t`qnorm(p, mean = 0, sd = 1)` | Computes quantiles, that is a value $q$ so that $\\Prob(X \\le \\texttt{q}) = \\texttt{p}$.                            |\n|\t`rnorm(n, mean = 0, sd = 1)` | Randomly samples `n` values from the   $\\Norm(\\mu, \\sigma)$ distribution where $\\mu$ = `mean` and $\\sigma$ = `sd`. | \n\n: Normal distributions in R. {#tbl-normal-in-R}\n\n#### The 68-95-99.7 Rule\n\nAlso known as the \"Empirical Rule\", the 68-95-99.7 Rule provides a set of probability\nbenchmarks for the normal distributions because for any normal distribution: \n \n* $\\approx 68$% of the normal distribution is between  $\\mu - \\sigma$ and $\\mu + \\sigma$.\n* $\\approx 95$% of the normal distribution is between  $\\mu - 2\\sigma$ and $\\mu + 2\\sigma$.\n* $\\approx 99.7$% of the normal distribution is between $\\mu - 3\\sigma$ and $\\mu + 3\\sigma$.\n \n\n::: {.example #exm-SAT}\n\n**Q.** \tBefore they were rescaled, SAT scores used to be approximately normally\ndistributed with a mean of 500 and a standard deviation of 100.\n\n1. Approximately what percent of test takers scored between 400 and 600?\n2. Approximately what percent of test takers scored above 600?\n3. Approximately what percent of test takers scored below 300?\n4. Approximately what percent of test takers scored between 400 and 700?\n\n\n**A.**\n\n1. 68\\%\n2. Since 68\\% are bewteen 400 and 600, the other 32\\% must be outside that\n\t\t\trange, half above and half below.  So 16\\% are above 600.\n3. Since 95\\% are between 300 and 700, the other 5\\% must be outside that \n\t\t\trange, half above and half below.  So 2.5\\% are below 300.\n4. 16\\% are below 400 and 2.5\\% are above 700, so the remaining 81.5\\% \n\t\t\tmust be between 400 and 700.\n\nOf course, we can get more accurate results using R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm( 600, 500, 100) - pnorm(400, 500, 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6826895\n```\n:::\n\n```{.r .cell-code}\npnorm( 700, 500, 100) - pnorm(300, 500, 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9544997\n```\n:::\n\n```{.r .cell-code}\npnorm( 300, 500, 100) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02275013\n```\n:::\n\n```{.r .cell-code}\npnorm( 700, 500, 100) - pnorm(400, 500, 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8185946\n```\n:::\n:::\n\n\n\nThe `xpnorm()()` function will additionally draw pictures of the normal \ndistribution with a portion of the distribution shaded in.\n\n::: {.cell}\n\n```{.r .cell-code}\nxpnorm(700,500,100) - xpnorm(400, 500, 100)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIf X ~ N(500, 100), then \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\tP(X <= 700) = P(Z <= 2) = 0.9772\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\tP(X >  700) = P(Z >  2) = 0.02275\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nIf X ~ N(500, 100), then \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\tP(X <= 400) = P(Z <= -1) = 0.1587\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\tP(X >  400) = P(Z >  -1) = 0.8413\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-26-1.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-26-2.png){width=432}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8185946\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n::: {.example #exm-qnorm-SAT}\n\nWe can use `qnorm()()` to compute percentiles.  For example, let's calculate\nthe 75th percentile for SAT distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(.75, 500, 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 567.449\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n### Beta Distributions\n\n\nThe Beta distributions have support on the interval $(0,1)$, so they can provide a model for proportions or other quantities that\nare bounded between 0 and 1.^[A more general version of the Beta\ndistributions can do the same thing for quantities bounded by any two numbers.\nThis more general family of distributions has four parameters.]\nThe Beta distributions have two parameters, imaginatively \ncalled `shape1` and `shape2`.  The kernel of the Beta distributions\nis a product of a power of $x$ and a power of $(1-x)$:\n$$\nk(x; \\alpha, \\beta) = x^{\\alpha-1} (1-x)^{\\beta -1} \\; \\boolval{ x \\in [0,1] }\n$$\nWhen $\\alpha = \\beta$, the distribution is symmetric, and when\n$\\alpha = \\beta =1$, we have the $\\Unif(0,1)$-distribution.\n\nThe two shape parameters provide a wide variety of shapes.\n\n\n::: {#fig-beta-dists .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ngf_dist(\"beta\", params = list(shape1 = 2, shape2 = 2), title = \"Beta(2,2)\")\ngf_dist(\"beta\", params = list(shape1 = 2, shape2 = 0.9), title = \"Beta(2,0.9)\")\ngf_dist(\"beta\", params = list(shape1 = 4, shape2 = 2), title = \"Beta(4,2)\")\ngf_dist(\"beta\", params = list(shape1 = 0.9, shape2 = 0.85), title = \"Beta(0.9,0.85)\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-beta-dists-1.png){#fig-beta-dists-1 width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-beta-dists-2.png){#fig-beta-dists-2 width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-beta-dists-3.png){#fig-beta-dists-3 width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/fig-beta-dists-4.png){#fig-beta-dists-4 width=432}\n:::\n\nSome example gamma distributions.\n:::\n\n\n\n| Function                     | What it does                                                                                                                    |\t\n|------------------------------|---------------------------------------------------------------------------------------------------------------------------------|\n| `dbeta(x, shape1, shape2)`   | Computes value of the pdf at `x`.                                                                                               |\n| `pbeta(q, shape1, shape2)`   | Computes value of the cdf at `x`, i.e., $\\Prob(X \\le \\texttt{q})$.                                                              |\n|\t`qbeta(p, shape1, shape2)`   | Computes quantiles, that is a value $q$ so that $\\Prob(X \\le \\texttt{q}) = \\texttt{p}$.                                         |\n|\t`rbeta(n, shape1, shape2)`   | Randomly samples `n` values from the $\\Beta(\\alpha, \\beta)$ distribution where $\\alpha$ = `shape1`  and $\\beta$ = `shape2`.     |\n\n: Beta distributions in R\n\n\n::: {.problem #exr-plot-pdf-and-compute-mean-var}\n\nUse R to plot the pdf and \ncompute the mean and variance of each of the following distributions.\n\n:::: {.enumerate}\n\na.  $\\Beta(2,3)$\na.  $\\Beta(20,30)$\na.  $\\Gamm(\\texttt{shape} = 2, \\texttt{scale} = 3)$\na.  $\\Weibull(\\texttt{shape} = 2, \\texttt{scale} = 3)$\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Beta(2,3)\nm <- antiD( x * dbeta(x,2,3) ~ x, lower.bound = 0)(1)\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4\n```\n:::\n\n```{.r .cell-code}\nantiD( x^2 * dbeta(x,2,3) ~ x, lower.bound = 0 )(1) - m^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Beta(20,30)\nm <- antiD( x * dbeta(x,20,30) ~ x, lower.bound = 0)(1)\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4\n```\n:::\n\n```{.r .cell-code}\nantiD( x^2 * dbeta(x,20,30) ~ x, lower.bound = 0 )(1) - m^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.004705882\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gamma(2,scale = 3)\nm <- antiD( x * dgamma(x,2,scale = 3) ~ x, lower.bound = 0)(Inf)\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n\n```{.r .cell-code}\nantiD( x^2 * dgamma(x,2,scale = 3) ~ x, lower.bound = 0 )(Inf) - m^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Weibull(2,scale = 3)\nm <- antiD( x * dweibull(x,2,scale = 3) ~ x, lower.bound = 0)(Inf)\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.658681\n```\n:::\n\n```{.r .cell-code}\nantiD( x^2 * dweibull(x,2,scale = 3) ~ x, lower.bound = 0 )(Inf) - m^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.931417\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-proportion-between}\n\nFor each of the following distributions, determine the proportion \nof the distribution that lies between 0.5 and 1.\n\n:::: {.enumerate}\n\na.  $\\Exp(\\texttt{rate} = 2)$\nb.  $\\Beta(\\texttt{shape1} = 3, \\texttt{shape2} = 2)$\nc.  $\\Norm(\\texttt{mean} = 1, \\texttt{sd} = 2)$\nd.  $\\Weibull(\\texttt{shape} = 2, \\texttt{scale} = 1/2)$\ne.  $\\Gamm(\\texttt{shape} = 2, \\texttt{scale} = 1/2)$\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npexp(1, 2) - pexp(0.5, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2325442\n```\n:::\n\n```{.r .cell-code}\npbeta(1, 3, 2) - pbeta(0.5, 3, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6875\n```\n:::\n\n```{.r .cell-code}\npnorm(1, 1, 2) - pnorm(0.5, 1, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.09870633\n```\n:::\n\n```{.r .cell-code}\npweibull(1, 2, scale = 1/2) - pweibull(0.5, 2, scale = 1/2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3495638\n```\n:::\n\n```{.r .cell-code}\npgamma(1, 2, scale = 1/2)   - pgamma(0.5, 2, scale = 1/2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.329753\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n### Binomial Distributions\n\nA binomial distribution is a discrete distribution with two parameters ($n$ and $p$,\nor as R calls them \\verb!size! and \\verb!prob!) describing a situation in which\n\n1.  Our random process consists of $n$ identical sub-processes (called trials).\n2.  Each trial has one of two outcomes (traditionally called success and failure).\n3.  The probability of success is $p$ for each trial.\n4.  The outcome of each trial is independent of the others.\n\nThe binomial random varialbe counts the number of successes.\n\n::: {.example #exm-freethrow-amy}\n\n:::: {.enumerate}\n\na. If we flip a coin 100 times and let $X$ be the number of heads, then\n$X \\sim \\Binom( 100, 0.5)$.\na. Amy is a 92% free throw shooter.  If she attempts 50 free throws and we let  -->\n$Y$ be the number that she makes, then $Y \\sim \\Binom(50, 0.92)$ (assuming that \neach shot is independent of the others.^[Whether an individual shooter's \nshots are independent or exhibit longer runs of \"hot\" and \"cold\"\nstreaks that we would expect under indpendence has been investigated by \nmany people.  The general conclusion seems to be that the independence \nassumption matches reality pretty closely.])\n:::: \n<!-- end enumerate -->\n\n\nHere are some example binomial distributions. The distributions are symmetric\nwhen $p = 0.5$.  For a fixed size $n$, the distributions become\nmore and more skewed as $p$ gets closer to 0 or 1.  For a fixed probability\n$p$, the distributions become more and more symmetric as $n$ gets larger.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ngf_dist(\"binom\", size = 10, prob = 0.5, title = \"Binom(10, 0.5)\")\ngf_dist(\"binom\", size = 10, prob = 0.05, title = \"Binom(10, 0.05)\")\ngf_dist(\"binom\", size = 100, prob = 0.5, title = \"Binom(100, 0.5)\")\ngf_dist(\"binom\", size = 100, prob = 0.05, title = \"Binom(100, 0.05)\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-34-1.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-34-2.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-34-3.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-34-4.png){width=432}\n:::\n\nSome example binomial distributions.\n:::\n\n\n\nThe pmf for a binomial distribution is given by\n$$\nf(x) = \\binom{n}{x} p^x (1-p)^{n-x}\n$$\nwhere $\\binom{n}{x} = \\frac{n!}{x!(n-x)!}$ is the binomial coefficient.\nAs with the continuous distributions, we have our usual functions available.\n\n| Function                     | What it does                                                                                                |\t\n|------------------------------|-------------------------------------------------------------------------------------------------------------|\n| `dbinom(x, size, prob)`      | Computes value of the pmf at `x`.                                                                           |\n| `pbinom(q, size, prob)`      | Computes value of the cdf at `x`, i.e., $\\Prob(X \\le \\texttt{q})$.                                          |\n|\t`qbinom(p, size, prob)`      | Computes quantiles, that is a value $q$ so that $\\Prob(X \\le \\texttt{q}) = \\texttt{p}$.                     |\n|\t`rbinom(n, size, prob)`      | Randomly samples `n` values from the $\\Binom(n, \\pi)$ distribution where $n$ = `size` and $\\pi$ = `prob`.   |\n\n::: \n<!-- end example -->\n\n\n\n### Poisson Distributions\n\nThe Poisson distributions are generally used as models for counting \"events\" that\nhappen in a specified amount of time or space.  If the probability of an \nevent happening at any moment is the same and independent of events happening\nat other moments, then the count of events in a fixed amount of time will be \na Poisson random variable.  The Poisson family has one parameter -- often denoted\n$\\lambda$ and called the *rate parameter* -- which is the average number\nof events that happen over the fixed amount of time or space we are observing.\n\n::: {.example #exm-geiger}\n\n:::: {.enumerate}\n\na.  Let $X$ be the number of clicks of a Gieger counter in a 1 second interval.\nSince each click corresponds to a radioactive decay event which we generally assume\noccur \"at random\" but according to some average rate, a Poisson random variable\nwould be a good model for this.  The rate parameter would be the average number \nof decay events per second.\n\na.  If you stand on along a busy highway and count the number of red cars that go \nby in 30 minutes, a Poisson random variable might be a good model.  Sometimes \nyou will get bunches of red cars or periods of time with few or no red cars,\nthat is just as a Poisson model predicts.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end example -->\n\n\nThe Poisson distributions are skewed right, but become less and less skewed as \nthe rate parameter increases.  Here are a few example plots.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ngf_dist(\"pois\", lambda = 1, title = \"Pois(1)\")\ngf_dist(\"pois\", lambda = 5, title = \"Pois(5)\")\ngf_dist(\"pois\", lambda = 15, title = \"Pois(15)\")\ngf_dist(\"pois\", lambda = 50, title = \"Pois(50)\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-35-1.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-35-2.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-35-3.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-35-4.png){width=432}\n:::\n\nSome example Poisson distributions.\n:::\n\n\n\nThe pmf for a $\\Pois(\\lambda)$ random variable is \n$$\nf(x) = \\frac{ e^{-\\lambda} \\lambda^x}{x!}\n$$\nand the functions in R for working with Poisson distributions are \nthe following:\n\n| Function           | What it does                                                                                   |\n|--------------------|------------------------------------------------------------------------------------------------|\n| `dpois(x, lambda)` | Computes value of the pmf at `x`.                                                              |\n| `ppois(q, lambda)` | Computes value of the cdf at `x`, i.e., $\\Prob(X \\le \\texttt{q})$.                             |\n| `qpois(p, lambda)` | Computes quantiles, that is a value $q$ so that $\\Prob(X \\le \\texttt{q}) = \\texttt{p}$.        |\n| `rpois(n, lambda)` | Randomly samples `n` values from the $\\Pois(\\lambda)$ distribution where $\\lambda$ = `lambda`. |\n\n: Poisson distributions in R. \n\n\n## Fitting Distributions to Data\n\nSuppose we think a family of distributions would make a good model for some \nsituation.  How do we decide which member of the family to use?  The simple answer\nis that we should choose the one that fits \"best.\"  The trick is deciding what it \nmeans to fit well.  In fact there is more than one way to measure how well \na distribution fits a data set.  \n\n\n::: {.example #exm-windspeed}\nWe can use the following code to load a data set that contains three year's worth \nof mean hourly wind speeds (mph) in Twin Falls, ID.  This kind of data is often used\nto estimate how much power could be generated from a windmill placed in a given location.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWind <- \n  read.csv(\"https://rpruim.github.io/Engineering-Statistics/data/stob/TwinfallsWind.csv\")\nhead(Wind, 2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"date\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"time\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"speed\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1/1/2010\",\"2\":\"0:00\",\"3\":\"2.24\",\"_rn_\":\"1\"},{\"1\":\"1/1/2010\",\"2\":\"1:00\",\"3\":\"2.42\",\"_rn_\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ntail(Wind, 2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"date\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"time\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"speed\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"12/31/2012\",\"2\":\"22:00\",\"3\":\"3.88\",\"_rn_\":\"26272\"},{\"1\":\"12/31/2012\",\"2\":\"23:00\",\"3\":\"5.04\",\"_rn_\":\"26273\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ngf_histogram( ~ speed, data = Wind, binwidth = 1 )\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-37-1.png){width=432}\n:::\n:::\n\n\nAs we can see, the distribution is skewed, but it doesn't look like an \nexponential distribution would be a good fit.  Of the distributions we have seen,\nit seems like a Weibull or Gamma distribution would be a potentially good choice.\nA Weibull model has often been used as a model for mean hourly wind speed, and the \nshape of our histogram indicates that this is a reasonable family of distributions.\n\n**Q.** Which Weibull distribution is the best model for our data?\n\n**A.** The `fitdistr()()` in the **`MASS`** package uses the method of \n**maximum likelihood** to fit univariate (one variable) distributions.\n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistr( Wind$speed, \"weibull\" )\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in fitdistr(Wind$speed, \"weibull\"): Weibull values must be > 0\n```\n:::\n:::\n\n\nFor `fitdistr()()` to fit a Weibull distribution, all of the data must be positive,\nbut our data includes some 0's.  \n\n::: {.cell}\n\n```{.r .cell-code}\ntally( ~ (speed == 0), data = Wind)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(speed == 0)\n TRUE FALSE \n   48 26225 \n```\n:::\n:::\n\n\nLet's see how small the smallest non-zero measurements \nare.\n\n::: {.cell}\n\n```{.r .cell-code}\nmin( ~ speed, data = Wind |> filter(speed > 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01\n```\n:::\n:::\n\n\nThis may well be a simple rounding issue, since the wind speeds are recorded to the \nnearest 0.01 and 0.01 is the smallest positive value.\nLet's create a new variable that moves each value of 0 to 0.0025 and try again.\nWhy 0.0025?  If we think that 0.01 represents anything in the range 0.005 to 0.015, which\nwould round to 0.01, then 0 represents anything in the range 0 to 0.005.  \n0.0025 is the middle of that range.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWind <- Wind |> mutate(speed2 = ifelse( speed > 0, speed, 0.0025))\nfitdistr( Wind$speed2, \"weibull\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      shape         scale   \n  1.694422851   6.650586935 \n (0.007957624) (0.025551827)\n```\n:::\n:::\n\n\n\n\nThis says that the best fitting (in the sense of maximum likelihood) Weibull\ndistribution is the $\\Weibull(1.69, 6.65)$-distribution.\n\nThe `gf_histogram()()` function has an option to overlay the distribution\nfit by `fitdistr()()` so we can see how good the fit is graphically.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dhistogram( ~ speed2, data = Wind) |>\n  gf_fitdistr( ~ speed2, data = Wind, dist = \"weibull\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/xhistogram-Wind-1.png){width=432}\n:::\n:::\n\n\n\nThis can be abbreviated a bit:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dhistogram( ~ speed2, data = Wind) |>\n  gf_fitdistr(dist = \"weibull\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/xhistogram-Wind-2-1.png){width=432}\n:::\n:::\n\n\n\n`gf_fitdistr()()` is inheriting the formula and data from `gf_dhistogram()()`.\n::: \n<!-- end example -->\n\n\n::: {.example #exm-windspeed-gamma}\n\nAs an alternative, we could fit a Gamma distribution to the wind speed data.\n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistr(Wind$speed2, \"gamma\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      shape         rate    \n  2.495582854   0.421178362 \n (0.020485581) (0.003828652)\n```\n:::\n\n```{.r .cell-code}\ngf_dhistogram( ~ speed2, data = Wind) |>\n  gf_fitdistr(dist = \"gamma\" , color = ~ \"Gamma\") |>\n  gf_fitdistr(dist = \"weibull\" , color = ~ \"Weibull\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-40-1.png){width=432}\n:::\n:::\n\n\n\nBy eye, it appears that the Gamma distribution fits this data set slightly better, but\nthere may other reasons to prefer the Weibull distribution.  In fact,\nthere has been a good deal of research done regarding which distributions to use \nfor wind speed data fitting.  The answer to the question of which distributions should \nbe used seems to be that it depends on the purpose for your modeling:  \n``The fact that different distributions excel under different applications\nmotivates further research on model selection based upon the engineering\nparameter of interest.\" @Morgan2011:WindSpeed\n::: \n<!-- end example -->\n\n\n::: {.example #exm-jordan}\n\n1986--87 was a good season for Michael Jordan, a famous former NBA basketball player.\nPossible models for the points scored each game that season are normal, Weibull, and \nGamma distributions.\nThe normal distributions might be a good choice if we think that the distributions\nis roughly symetric (very good games are about the same amount above average as\nthe very poor games are below average).  Weibull and Gamma distributions have\nthe built in feature that scores cannot be negative and would allow for a\nskewed distribution.\nThe `fitdistr()()` function in the **`MASS`** package can fit each of these.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastR2)     # the Jordan8687 data set is in this package\nfitdistr(Jordan8687$points, \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mean          sd    \n  37.0853659    9.8639541 \n ( 1.0892915) ( 0.7702454)\n```\n:::\n\n```{.r .cell-code}\nfitdistr(Jordan8687$points, \"weibull\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     shape        scale   \n   4.1227692   40.7746012 \n ( 0.3454908) ( 1.1516943)\n```\n:::\n\n```{.r .cell-code}\nfitdistr(Jordan8687$points, \"gamma\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      shape         rate    \n  12.42843002    0.33513033 \n ( 1.91535288) ( 0.05270279)\n```\n:::\n:::\n\n\nWe can use a histogram with overlaid density curve to see how well these fits compare \nto the data.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dhistogram(~ points, data = Jordan8687, binwidth = 5) |>\n  gf_fitdistr(dist = \"dnorm\", color = ~\"normal\") |>\n  gf_fitdistr(dist = \"dweibull\", color = ~ \"Weibull\") |>\n  gf_fitdistr(dist = \"dgamma\", color = ~ \"Gamma\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/jordan2-1.png){width=432}\n:::\n:::\n\n\nThe three fits are similar, but not identical.  \n\n::: \n<!-- end example -->\n\n### Maximum Likelihood\n\nThe `fitdistr()()` function uses the maximum likelihood method to estimate\ndistribution parameters.\nThe maximum likelihood method is one of the most commonly used estimation methods\nin all of statistics because (1) it can be used in a wide range of applications,\nand (2) the resulting estimators have some some desirable properties.  Maximum likelihood estimation tries to choose the parameter values that *maximize* the *likelihood* of the observed data.  \n\nFirst, let's think about the \"likelihood\" of an individual observed data-point.  The likelihood of the data-point is just the probability density function (or probability mass function) for the distribution of interest, evaluated at the value observed in the data. The likelihood gives some indication of how frequently we'd expect to observe this value, but it is *not* a probability (for one thing, likelihoods can exceed 1).  The figure below illustrates that the likelihood of observing a person 80 inches (6 feet, 8 inches) tall, if the person comes from a population whose heights are Normally distributed with a mean of 68 inches and a standard deviation of 6 inches is about 0.009: \n\n::: {.cell}\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/likelihood-1.png){width=432}\n:::\n:::\n\n\n\n\nGiven a set of specific parameter values, the likelihood of an entire observed data-set can be calculated by obtaining the value of the likelihood of each observed data-point, and summing these over all the observed data points.  Then, we can find the maximum likelihood parameter estimates by trying many candidate parameter values until satisfied that we have found the ones that maximize the likelihood. (The numerical methods used are usually a bit more sophisticated than ``guessing lots of random candidate values\", but we won't get into the details here. In some cases, it is also possible to write down a mathematical expression for the likelihood of the data given the parameters, and maximize it analytically.)\n\nWe'll illustrate the main ideas of maximum likelihood with a simple example.\n\n::: {.example #exm-michael-3-dice}\n\nMichael has three dice in his pocket.  One is a standard die with six sides,\nanother has four sides, and the third has ten sides.  He challenges you to a\ngame.  Without showing you which die he is using, Michael is going to roll a \ndie 10 times and report to you how many times the resulting number is a \n$1$ or a $2$.  Your challenge is to guess which die he is using.\n\n**Q.**  Michael reports that $3$ of the $10$ rolls resulted in a $1$ or a\n$2$.  Which die do you think he was using?\n\n**A.** \nThe probability of obtaining a $1$ or a $2$ is one of $\\frac12$,\n$\\frac13$, or $\\frac15$, depending on which die is being used.  Our data\nare possible with any of the three dice, but let's see how likely they are\nin each case.\n\n::: {.itemize}\n\n* If $\\evProb{roll 1 or 2}  = \\frac15$, \nthen the probability of obtaining exactly Michael's data is \n$$\n\\left(\\frac15\\right)^3 \\left(\\frac45\\right)^7  =  0.0599323\n\\;.\n$$\n\n(Whatever the order, there will be 3 events with probability\n$1/5$ and 7 with probability $4/5$.  Since the events are independent,\nwe can multiply all of these probabilities.)\n\n* If $\\evProb{roll 1 or 2}  = \\frac13$, \nthen the probability of obtaining exactly Michael's data is \n$$\n\\left(\\frac13\\right)^3 \\left(\\frac23\\right)^7 \n=  0.0021677\n\\;.\n$$\n\n* If $\\evProb{roll 1 or 2}  = \\frac12$, \nthen the probability of obtaining exactly Michael's data is \n$$\n\\left(\\frac12\\right)^3 \\left(\\frac12\\right)^7 = 0.0016777\n\\;.\n$$\n::: \n<!-- end itemize -->\n\nOf these, the largest likelihood is for the case that \n$\\evProb{roll 1 or 2}  = \\frac13$, i.e.,\nfor the standard, six-sided die.  Our data would be more likely\nto occur with that die than with either of the other two -- it is the \nmaximum likelihood die.\n::: \n<!-- end example -->\n\n\nIn general, maximum likelihood calculations are harder because instead of having\nonly 3 choices, there will be infinitely many choices, and instead of having only\none parameter, there may be multiple parameters.  So techniques from (multi-variable) \ncalculus or numerical approximation methods are often used to maximize the likelihood function.\nThe `fitdistr()` function uses pre-derived formulas for some distributions\nand numerical approximation methods for others.  In some cases, you will get warning\nmessages about attempts to apply a function to values that don't make sense (trying to\ntake logs or square roots of negative numbers, zero in the denominator, etc.) as the \nnumerical approximation algorithm explores options in an attempt to find the best fit.\nThe help documenation for `fitdistr()` explains which distributions it can \nhandle and what method is used for each.\n\n### The method of moments\nAn easy (but sometimes fairly crude) way to estimate the parameters of a distribution\nis the method of moments.  You will often see this method used in engineering textbooks,\nespeically if they do not rely on software that implements others methods (like the \nmaximum likelihood method).\n\nThe basic idea is to set up a system of  equations where we set the mean of the data equal to the mean of the distribution, the variance of the data equal to the variance of the distribution, etc.^[If our distribution has more than 2 parameters, we will need higher moments, which we will not cover here.]  \n\nTo employ this method, we need to know the means and variances of our favorite families of distributions (in terms of the parameters of the distributions).  For all of the distributions we have seen, one can work out formulas for the means and variances in terms of the parameters involved.  These are listed in @tbl-cont-dist\n\n::: {.example #exm-windspeed-2}\n\nLet's return to the wind speeds in @exm-windspeed.\nThe formulas for the mean and variance of a Weibull distribution involve\nthe gamma function $\\Gamma()$, which might be unfamiliar to you.  So let's \nsimplify things.\n\nTheoretical properties and observations of wind speeds at other locations \nsuggest that using a shape parameter of $\\alpha = 2$ is often a good choice (but \nshape does differ from location to location depending on how consistent or \nvariable the wind speeds are).\nThe Weibull distributions with $\\alpha = 2$ have a special name, they are \ncalled the **Rayleigh** distributions.  \nSo $\\Rayleigh(\\beta) = \\Weibull(\\alpha = 2, \\beta)$.\nIn this case, from @tbl-cont-dist, we see that to calculate \nthe mean we need the value of $\\Gamma(1 + \\frac{1}{2}) = \\Gamma(1.5) = \\sqrt{\\pi}/2$.\n\n::: {.cell}\n\n```{.r .cell-code}\ngamma(1.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8862269\n```\n:::\n\n```{.r .cell-code}\nsqrt(pi)/2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8862269\n```\n:::\n:::\n\n\nFrom @tbl-cont-dist we see that the mean of a \n$\\Rayleigh(\\beta)$-distribution is \n$$\n\\E(X) = \\beta \\frac{\\sqrt{\\pi}}{2}\n$$\n\nNow we can choose our estimate $\\hat \\beta$ for $\\beta$ so that \n$$\n\t\\hat \\beta \\frac{\\sqrt{\\pi}}{2} = \\overline x  ;.\n$$\nThat is,\n$$\n\t\\hat\\beta = \\frac{2 \\overline x }{\\sqrt{\\pi}}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx.bar <- mean(~speed, data = Wind) \nx.bar\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.925238\n```\n:::\n\n```{.r .cell-code}\nbeta.hat <- x.bar * 2 / sqrt(pi)\nbeta.hat \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.685915\n```\n:::\n:::\n\n\nSo our method of moments fit for the data is a \n$\\Rayleigh(6.69) = \\Weibull(2, 6.69)$\n\nAlthough the Rayleigh distributions are not as flexible as the Weibull or Gamma \ndistributions, and although maximum likelihood is generally preferred over the\nmethod of moments, the method of moments fit of a Rayleigh distribution does have\none advantage: it can be computed even if all you know is the mean of some sample data.\nSometimes, that is all you can easily get your hands on (because the people who collected\nthe raw data only report numerical summaries).  You can find average wind speeds of for \nmany locations online, for example here:\n<http://www.wrcc.dri.edu/htmlfiles/westwind.final.html>\n::: \n<!-- end example -->\n\n\n\n::: {.example #exm-mom-2-params}\n\nFor distributions with two parameters, we solve a system of two equations with two unknowns.\nFor the normal distributions this is particularly easy since the parameters are the mean\nand standard deviation, so we get\n\n$$\n\\begin{aligned}\n\\hat\\mu &= \\mean x\\\\\n\\hat\\sigma^2 &= s_x^2\\\\\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx.bar <- mean(~speed, data = Wind); x.bar\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.925238\n```\n:::\n\n```{.r .cell-code}\nv <- var(~speed, data = Wind); v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 13.34635\n```\n:::\n\n```{.r .cell-code}\nsqrt(v)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.653265\n```\n:::\n:::\n\n\nSo the method of moments suggests a $\\Norm(5.93, 3.65)$\ndistribution.  In this case, the method of moments and maximum likelihood methods\ngive the same results. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistr(Wind$speed, \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mean          sd    \n  5.92523770   3.65319577 \n (0.02253814) (0.01593687)\n```\n:::\n:::\n\n\nBut this doesn't mean that the fit is particularly good.  Indeed, a normal distribution is \nnot a good choice for this data.  We know that wind speeds can't be negative and we \nhave other distributions (exponential, Weibull, and Gamma, for example) that are also\nnever negative.  So choosing one of those seems like a better idea.\nThe following plot shows, as we expected, that the normal distribution is not a particularly\ngood fit.\n\n::: {.cell}\n\n```{.r .cell-code}\nhistogram(~speed, data = Wind, fit = \"normal\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-45-1.png){width=432}\n:::\n:::\n\n\nIt is important to remember that the best fit using a poor choice for the family\nof distriubtions might not be a useful fit.  \nThe choice of distributions is made based on a combination of theoretical \nconsiderations, experience from previous data sets, and the quality of \nthe fit for the data set at hand.\n::: \n<!-- end example -->\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+\n\n: Some (families of) continuous distributions. {#tbl-cont-dist}\n\n<!-- \\begin{table} -->\n<!-- \\begin{center} -->\n<!-- \\small -->\n<!-- \\begin{tabular}{|p{5mm}p{40mm}p{25mm}p{45mm}|} -->\n<!-- \\hline -->\n<!-- \\multicolumn{2}{|l}{\\textbf{\\sf distribution} } -->\n<!-- && -->\n<!-- \\\\ -->\n<!--   & \\textbf{\\sf pdf or pmf}  -->\n<!--   & \\textbf{\\sf mean}  -->\n<!--   & \\textbf{\\sf variance}  -->\n<!-- \\\\[.5mm] \\hline -->\n<!-- &&& \\\\[-2mm] -->\n<!-- \\multicolumn{2}{|l}{  Triangle: $\\Tri(a,b,c)$} && \\\\ -->\n<!--   &  -->\n<!--   $\\displaystyle  -->\n<!--  \t \\begin{cases} -->\n<!-- \t\t \\frac{2(x-a)}{(b-a)(c-a)}  &  \\mbox{if $x \\in [a,c]$} \\;,  \\\\ -->\n<!-- \t\t \\frac{2(b-x)}{(b-a)(b-c)}  &  \\mbox{if $x \\in [c,b]$} \\;,  \\\\ -->\n<!-- \t\t0 & \\mbox{otherwise} \\\\ -->\n<!-- \t  \\end{cases}$ -->\n<!-- \t  & $\\displaystyle \\frac{a + b + c}{3}$  -->\n<!-- \t  & $\\displaystyle \\frac{a^2 + b^2 + c^2 - ab -ac -bc}{18}$ -->\n<!-- \t  \\\\[5.2mm] -->\n<!-- \t  \\multicolumn{2}{|l}{  Uniform: $\\Unif(a,b)$} && \\\\ -->\n<!--   &  -->\n<!--   $\\displaystyle  -->\n<!--  \t \\begin{cases} -->\n<!-- \t\t\\frac{1}{b-a}  &  \\mbox{if $x \\in [a,b]$} \\;,  \\\\ -->\n<!-- \t\t0 & \\mbox{otherwise} \\\\ -->\n<!-- \t  \\end{cases}$ -->\n<!-- \t  & $\\displaystyle \\frac{b+a}{2}$  -->\n<!-- \t  & $\\displaystyle \\frac{(b-a)^2}{12}$ -->\n<!-- \t  \\\\[5.2mm] -->\n<!-- \\multicolumn{2}{|l}{  Standard normal: $\\Norm(0,1)$ } && \\\\ -->\n<!--   &  -->\n<!--   $\\displaystyle \\frac{1}{\\sqrt{2\\pi}} {e^{-\\frac12 z^2}}$ -->\n<!--   \t& $0$ & $1$ -->\n<!-- \\\\[4.2mm] -->\n<!-- \\multicolumn{2}{|l}{  Normal: $\\Norm(\\mu,\\sigma)$ } && \\\\ -->\n<!--   & $ -->\n<!-- \t \\displaystyle \\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot  -->\n<!--      \t\te^{-\\frac12 (\\frac{x-\\mu} -->\n<!-- \t\t\t{\\sigma})^2}$ -->\n<!--   \t& $\\mu$ & $\\sigma^2$ -->\n<!-- \\\\[5.2mm] -->\n<!-- \\multicolumn{2}{|l}{  Exponential: $\\Exp(\\lambda)$ }  && \\\\ -->\n<!-- %  \t& $\\lambda$ -->\n<!-- \t& $\\lambda e^{-\\lambda x}$ -->\n<!--   \t& $1/\\lambda$ & $1/\\lambda^2$ -->\n<!-- \\\\[4.2mm] -->\n<!-- \\multicolumn{2}{|l}{  Gamma: $\\Gamm(\\alpha, \\lambda = \\frac{1}{\\beta})$ } && \\\\ -->\n<!-- %  \t& $\\lambda$ -->\n<!-- \t& $\\displaystyle \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\lambda x}$ -->\n<!--   \t& $\\alpha/\\lambda = \\alpha \\beta$  -->\n<!-- \t& $\\alpha/\\lambda^2 = \\alpha \\beta^2$ -->\n<!-- \\\\[4.2mm] -->\n<!-- \\multicolumn{2}{|l}{Weibull: $\\Weibull(\\alpha,\\beta = \\frac{1}{\\lambda})$} && \\\\ -->\n<!-- & -->\n<!-- \t  $\\displaystyle \\frac{\\alpha}{\\beta^\\alpha} x^{\\alpha-1} e^{-(x/\\beta)^\\alpha}$ -->\n<!-- \t&  -->\n<!-- \t  $\\beta \\Gamma(1 + \\frac1{\\alpha})$ -->\n<!-- \t& -->\n<!-- \t  $\\beta^2 \\left[ \\Gamma(1 + \\frac{2}{\\alpha})  -->\n<!-- \t  - \\left[ \\Gamma(1 + \\frac{1}{\\alpha}) \\right]^2 \\right] -->\n<!-- \t  $  -->\n<!-- \\\\[4.0mm] -->\n<!-- \\multicolumn{2}{|l}{  Beta: $\\Beta(\\alpha, \\beta)$ } && \\\\ -->\n<!-- \t&  -->\n<!--   $\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}  -->\n<!-- \t\tx^{\\alpha-1}(1-x)^{\\beta-1}$ -->\n<!--   \t&  -->\n<!-- \t  $\\displaystyle \\frac{\\alpha}{\\alpha + \\beta}$ -->\n<!-- \t  & -->\n<!-- \t  $\\displaystyle \\frac{\\alpha \\beta }{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}$ -->\n<!-- \\\\[4mm] -->\n<!-- \\hline -->\n<!-- \\multicolumn{2}{|l}{  Binomial: $\\Binom(n, p)$ } && \\\\ -->\n<!-- \t&  -->\n<!-- \t$\\displaystyle \\binom{n}{x} p^x (1-p)^{n-x}$ -->\n<!-- \t& -->\n<!-- \t$np$ -->\n<!-- \t& -->\n<!-- \t$np(1-p)$ -->\n<!-- \\\\[4mm] -->\n<!-- \\multicolumn{2}{|l}{  Poisson: $\\Pois(\\lambda)$ } && \\\\ -->\n<!-- \t&  -->\n<!-- \t$\\displaystyle \\frac{e^{-\\lambda} \\lambda^x} {x!}$ -->\n<!-- \t& -->\n<!-- \t$\\lambda$ -->\n<!-- \t& -->\n<!-- \t$\\lambda$ -->\n<!-- \\\\[4mm] -->\n<!-- \\hline -->\n<!-- \\end{tabular} -->\n<!-- \\end{center} -->\n<!-- \\caption{Some common continuous distributions. -->\n<!-- Standard names for parameters that appear in several distributions -->\n<!-- include \\texttt{rate} ($\\lambda$), \\texttt{shape} ($\\alpha$), and \\texttt{scale} ($\\beta$). -->\n<!-- In the normal distributions, $\\mu$ and $\\sigma$ are called \\texttt{mean} and \\texttt{sd} -->\n<!-- in \\R, and in the uniform distirbutions, $a$ and $b$ are called \\texttt{min} and \\texttt{max}. -->\n<!-- The function $\\Gamma(x)$ that appears in the formulas for the Weibull and Beta -->\n<!-- distributions is a kind of continuous extrapolation from the factorial function. -->\n<!-- The \\texttt{gamma()} function will calculate these values.} -->\n<!-- \\label{tbl-cont-dist} -->\n<!-- \\end{table} -->\n\n\n## Quantile-Quantile Plots\n\nTo this point we have looked at how well a distribution fits the data by overlaying a density\ncurve on a histogram.  While this is instructive, it is not the easiest way to make a \ngraphical comparison between a data set and a theoretical distribution.   Our eyes\nare much better and judging whether something is linear than they are at judging whether \nshapes have a particular kind of curve.  Furthermore, certain optical misperceptions\ntend to cause people to exaggerate some kinds of differences and underestimate others.\n\nQuantile-quantile plots offer an alternative approach.  As the name suggests, the idea is \nto compare the quantiles of our data to the quantiles of a theoretical distribution.  These\nare then plotted as a scatter plot.  Let's go through those steps with a small data\nset so we can see all the moving parts, then we'll learn how to automate the whole\nprocess using `gf_qq()()`.\n\n### Normal-Quantile Plots\n\nThe normal distributions are especially important for statistics, so normal-quantile\nplots will be our most important example of quantile-quantiles plots.  Also, special\nproperties of the normal distributions make normal-quantile plots especially easy\nand useful.  We will illustrate the construction of these plots using a data set\ncontaining Michael Jordan's game by game scoring output from the 1986--87 basketball\nseason.\n\n::: {.example #exm-jordan-qq}\n\nLet's begin by forming a randomly selected sample of 10 basketball games.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)              # so you can get the same sample if you like.\nSmallJordan <- sample(Jordan8687, 10)\nSmallJordan\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"game\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"points\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"orig.id\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"31\",\"2\":\"27\",\"3\":\"31\",\"_rn_\":\"31\"},{\"1\":\"79\",\"2\":\"53\",\"3\":\"79\",\"_rn_\":\"79\"},{\"1\":\"51\",\"2\":\"43\",\"3\":\"51\",\"_rn_\":\"51\"},{\"1\":\"14\",\"2\":\"40\",\"3\":\"14\",\"_rn_\":\"14\"},{\"1\":\"67\",\"2\":\"40\",\"3\":\"67\",\"_rn_\":\"67\"},{\"1\":\"42\",\"2\":\"49\",\"3\":\"42\",\"_rn_\":\"42\"},{\"1\":\"50\",\"2\":\"33\",\"3\":\"50\",\"_rn_\":\"50\"},{\"1\":\"43\",\"2\":\"38\",\"3\":\"43\",\"_rn_\":\"43\"},{\"1\":\"81\",\"2\":\"61\",\"3\":\"81\",\"_rn_\":\"81\"},{\"1\":\"25\",\"2\":\"43\",\"3\":\"25\",\"_rn_\":\"25\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprobs <- seq(0.05, 0.95, by = 0.10)\nprobs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95\n```\n:::\n\n```{.r .cell-code}\nobserved <- sort(SmallJordan$points)                                    # sorted observations\ntheoretical <- qnorm( probs, mean = mean(observed), sd = sd(observed) ) # theoretical quantiles\n\nQQData <- data.frame(observed = observed, theoretical = theoretical)\nQQData\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"observed\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"theoretical\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"27\",\"2\":\"26.63891\"},{\"1\":\"33\",\"2\":\"32.57980\"},{\"1\":\"38\",\"2\":\"36.11398\"},{\"1\":\"40\",\"2\":\"38.93756\"},{\"1\":\"40\",\"2\":\"41.47299\"},{\"1\":\"43\",\"2\":\"43.92701\"},{\"1\":\"43\",\"2\":\"46.46244\"},{\"1\":\"49\",\"2\":\"49.28602\"},{\"1\":\"53\",\"2\":\"52.82020\"},{\"1\":\"61\",\"2\":\"58.76109\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nIf the observed data matched the theoretical quantiles perfectly, a scatter plot\nwould place all the points on the line with slope 1 passing through the origin.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point( observed ~ theoretical, data = QQData, title = \"Hand made QQ-plot\" ) |>\n  gf_fun( x ~ x, alpha = 0.6, color = \"blue\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/hand-qqplot-1.png){width=432}\n:::\n:::\n\n\n\nEven better, we don't need to know the mean and standard deviation in advance, because\nall normal distributions are linear transformations of the $\\Norm(0,1)$-distribution.\nSo our standard practice will be to compare our data to the $\\Norm(0,1)$-distribution.\nIf $X \\sim \\Norm(\\mu,\\sigma)$, then $X = \\mu + \\sigma Z$ where $Z \\sim \\Norm(0,1)$, so \na plot of $X$ vs. $Z$ will have slope $\\sigma$ and intercept $\\mu$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheoretical2 <- qnorm( probs, mean = 0, sd = 1 ) # theoretical quantiles from Norm(0,1)\nQQData2 <- data.frame(observed = observed, theoretical = theoretical2)\ngf_point(observed ~ theoretical, data = QQData2, title = \"Hand made QQ-plot\", xlab = \"theoretical (z)\" ) |>\n  gf_abline(intercept = ~ mean(SmallJordan$points), slope = ~ sd(SmallJordan$points), \n            alpha = 0.5, color = \"navy\", data = NA) |>\n  gf_hline(yintercept = ~ mean(SmallJordan$points), alpha = 0.5) |>\n  gf_vline(xintercept = ~ 0, alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-48-1.png){width=432}\n:::\n:::\n\n\n\nThis whole process is automated by the `gf_qq()()` function.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_qq( ~ points, data = SmallJordan, title = \"Sub-sample\" )\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-49-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ points, data = Jordan8687, title = \"Full data set\" )\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-49-2.png){width=432}\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n### Other distributions\n\nWorking with other distributions is similar, but most families of distributions\ndon't have a single \"master example\" to which we can make all comparisons, so we\nneed to pick a particular member of the family (either by fitting or for some \ntheoretical reason).^[There are a few other families of distributions that\nhave a prototypical member such that all other members are a linear transformation\nof the prototype.  The exponential family is one such family.]\n\n::: {.example #exm-windspeed-qq}\n\n\tLet's build a quantile-quantile plot for our wind speed data comparing \n\tto normal, gamma and Weibull distributions.\n<!-- \\iffalse -->\n<!-- \tAs in the previous section, we'll begin by creating it manually using  -->\n<!-- \ta small subset of the data and then show how to automate the process  -->\n<!-- \tusing \\function{gf_qq()}. -->\n<!-- <<>>= -->\n<!-- SmallWind <- sample(Wind, 10) -->\n<!-- SmallWind -->\n<!-- @ -->\n<!-- Now we need to compute the quantiles of our data and the quantiles of our  -->\n<!-- theoretical distribution.  We start by selecting 10 evenly spaces  -->\n<!-- proportions. -->\n<!-- <<>>= -->\n<!-- probs <- seq(0.05, 0.95, by = 0.10) -->\n<!-- probs -->\n<!-- @ -->\n<!-- Now we compute the quantiles for these probabilities using the parameters -->\n<!-- we fit previously. -->\n<!-- <<tidy = FALSE>>= -->\n<!-- y <- qdata( probs, SmallWind$speed2)                     # quantiles from data -->\n<!-- x.gamma <- qgamma( probs, shape = 2.496, rate = 0.421 )      # quantiles from Gamma dist -->\n<!-- x.weibull <- qweibull( probs, shape = 1.694, scale = 6.651 ) # quantiles from Weibull dist -->\n<!-- x.normal <- qnorm( probs, mean = 5.925 , sd = 3.653  )       # quantiles from Normal dist -->\n<!-- @ -->\n<!-- Finally, we create the scatter plot -->\n<!-- <<>>= -->\n<!-- gf_point(y ~ x.gamma) -->\n<!-- gf_point(y ~ x.weibull) -->\n<!-- gf_point(y ~ x.normal) -->\n<!-- @ -->\n<!-- \\fi -->\nWe can automate this, but we need to tell `gf_qq()` how to calculate\nthe quantiles.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_qq( ~ speed2, data = Wind)  # normal-quantile plot; normal is not a good model\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-50-1.png){width=432}\n:::\n:::\n\n\n\nThe normal model does not fit well, but both Gamma and Weibull are reasonable models:\n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistr(Wind$speed2, \"gamma\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      shape         rate    \n  2.495582854   0.421178362 \n (0.020485581) (0.003828652)\n```\n:::\n\n```{.r .cell-code}\nfitdistr(Wind$speed2, \"Weibull\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      shape         scale   \n  1.694422851   6.650586935 \n (0.007957624) (0.025551827)\n```\n:::\n\n```{.r .cell-code}\nfittedqgamma <- makeFun( qgamma(p, shape = 2.496, rate = 0.421 ) ~ p )\nfittedqweibull <- makeFun( qweibull(p, shape = 1.694, scale = 6.651) ~ p ) \ngf_qq( ~speed2, data = Wind, distribution = fittedqgamma )\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-51-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq( ~speed2, data = Wind, distribution = fittedqweibull )\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-51-2.png){width=432}\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n\n\n{{< pagebreak >}}\n\n\n\n## Exercises\n\n::: {.problem #exr-twin-falls}\n\n:::: {.enumerate}\n\na.  Using @tbl-cont-dist and the method of moments,\nfit an exponential distribution to the Twin Falls wind speed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWind <- \n  read.csv(\"https://rpruim.github.io/Engineering-Statistics/data/stob/TwinfallsWind.csv\")\n```\n:::\n\n\n\nWhat is the estimated value of the rate parameter?\n\nb.  Now use `fitdistr()()` to fit an exponential\n\tdistribution using maximum likelihood.\n\t\nc.  How do the two estimates for the rate parameter compare?\n\nd.  How well does an exponential distribution fit this data?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n:::: {.enumerate}\n\na.  The method of moments fit for $\\lambda$ comes from solving \n\t\t\t$\\frac{1}{\\lambda} = \\mean x$ for $\\lambda$, so \n\t\t\t$\\hat \\lambda = \\frac{1}{\\mean x}$\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda.hat <- 1/ mean(~speed, data = Wind)\nlambda.hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1687696\n```\n:::\n:::\n\n\nb. \n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistr(Wind$speed, \"exponential\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      rate    \n  0.168769601 \n (0.001041213)\n```\n:::\n:::\n\n\nc.  They are the same in this case.\n\nd.  This fit is not that great.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dhistogram(~speed, data = Wind, fit = \"exponential\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-55-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ speed, data = Wind, distribution = qexp)\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-55-2.png){width=432}\n:::\n:::\n\n\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-gamma-mom}\n\n\tA Gamma distribution can also be fit using the method of moments.\n\tBecause there are two parameters (shape and rate or shape and scale),\n\tyou will need to solve a system of two equations with two unknowns.\n:::: {.enumerate}\n\na. Using @tbl-cont-dist and the method of moments,\nfit a Gamma distribution to the Twin Falls wind speed data.\nWhat are the estimated values of the shape and rate parameters?\nb. How do the method of moments estimates for the parameters compare to the \n\tmaximum likelihood estimates from `fitdistr()()`?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWind <- Wind |> mutate(speed2 = ifelse( speed > 0, speed, 0.0025))\nm <- mean(~speed2, data = Wind); m\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.925242\n```\n:::\n\n```{.r .cell-code}\nv <- var(~speed2, data = Wind); v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 13.34629\n```\n:::\n\n```{.r .cell-code}\nfitdistr(Wind$speed2, \"gamma\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      shape         rate    \n  2.495582854   0.421178362 \n (0.020485581) (0.003828652)\n```\n:::\n:::\n\n\nNow we solve\n\n$$\n\\begin{aligned}\n\\alpha \\beta & = \\mean x  = 5.9252423\n\\\\\n\\alpha \\beta^2 & = s^2 = 13.3462932\n\\end{aligned}\n$$\n\nDividing the second by the first gives $\\hat \\beta = \\sfrac{s^2}{\\mean x}$.\nFrom this we obtain $\\hat \\alpha = \\mean{x} / \\hat \\beta = \\sfrac{ \\mean x^2}{s^2}$.\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta.hat <- v/m; beta.hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.252447\n```\n:::\n\n```{.r .cell-code}\nalpha.hat <- m / beta.hat ; alpha.hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.63058\n```\n:::\n\n```{.r .cell-code}\nlambda.hat <- 1/ beta.hat ; lambda.hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4439616\n```\n:::\n:::\n\n\nThe fitted values are similar to but not identical to the maximum likelihood estimates.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-windspeed-sam}\n\nSam has found some information about wind speed at a location he\nis interested in online.  Unfortunately, the web site only provides\nthe mean and standard deviation of wind speed.  \n\n:::: {.center}\n\n+---------------------+----------+\n| mean:               | 10.2 mph |\n+---------------------+----------+\n| standard deviation: | 5.1 mph  |\n+---------------------+----------+\n:::: \n<!-- end center -->\n\n:::: {.enumerate}\n\na. Use this information and the method of moments to estimate the \n\tshape and rate parameters of a Gamma distribution.\na. \tIn principal, we could do the same for a Weibull distribution, but the \n\tformulas aren't as easy to work with. \n\tFit a Rayleigh distribution instead (i.e., a Weibull\n\tdistribution with shape parameter equal to 2).\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nWe can recycle some work from the previous problem to quickly obtain\nthe method of moments fit for the Gamma distribution:\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- 10.2\nv <- 5.1^2 \nbeta.hat <- v/m; beta.hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.55\n```\n:::\n\n```{.r .cell-code}\nalpha.hat <- m / beta.hat ; alpha.hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n\n```{.r .cell-code}\nlambda.hat <- 1/ beta.hat ; lambda.hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3921569\n```\n:::\n:::\n\n\nFor the Rayleigh distribution we solve \n$\\hat \\beta \\frac{\\sqrt{\\pi}}{2} = \\mean x$   for \n$\\hat \\beta$ and get\n$$\n\t\\hat\\beta = \\frac{2 \\mean x }{\\sqrt{\\pi}}\n\t= \\frac{2 \\cdot 10.2 }{\\sqrt{\\pi}}\n\t= 11.5094675\n$$\t\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-iq}\n\nIn 1964, a study was undertaken to see if IQ at 3 years of age is\n\tassociated with amount of crying at newborn age. In the study, 38 newborns\n\twere made to cry after being tapped on the foot, and the number of distinct\n\tcry vocalizations within 20 seconds was counted.\n\tThe subjects were followed up at 3 years of age and their IQs were measured.\n<!-- \tThe data from this study are in the `Baby` data frame.   -->\nYou can load this data using\n\n::: {.cell}\n\n```{.r .cell-code}\nBaby <- read.csv(\"https://rpruim.github.io/Engineering-Statistics/data/BabyCryIQ.csv\")\nhead(Baby)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"cry.count\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"IQ\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"10\",\"2\":\"87\",\"_rn_\":\"1\"},{\"1\":\"20\",\"2\":\"90\",\"_rn_\":\"2\"},{\"1\":\"17\",\"2\":\"94\",\"_rn_\":\"3\"},{\"1\":\"12\",\"2\":\"94\",\"_rn_\":\"4\"},{\"1\":\"12\",\"2\":\"97\",\"_rn_\":\"5\"},{\"1\":\"15\",\"2\":\"100\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\tThe `cry.count` variable records the number of distinct cry vocalizations \n\twithin 20 seconds.  Choose a family of distributions to fit to this data\n\tand do the fit using `fitdistr()()`. Also include a plot showing \n\ta histogram and your fitted density curve.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\tThe distribution is skewed and non-negative, so a gamma or Weibull seems like \n\ta good thing to try.  \n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistr(Baby$cry.count, \"gamma\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     shape         rate   \n  12.6338200    0.7329544 \n ( 2.8608824) ( 0.1693120)\n```\n:::\n\n```{.r .cell-code}\nfitdistr(Baby$cry.count, \"Weibull\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     shape        scale   \n   3.5842275   19.1020465 \n ( 0.4245079) ( 0.9177923)\n```\n:::\n\n```{.r .cell-code}\ngf_dhistogram( ~ cry.count, data = Baby, binwidth = 1) |>\n  gf_fitdistr( ~ cry.count, data = Baby, \"gamma\", title = \"Gamma\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-59-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_dhistogram(~cry.count, data = Baby, title = \"Weibull\") |>\n  gf_fitdistr(~ cry.count, data = Baby, \"dweibull\")\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-59-2.png){width=432}\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n<!-- \\iffalse -->\n<!-- \\begin{problem} -->\n<!-- \tFor each of the following, give a family of distributions -->\n<!-- \tthat might make a reasonable model and say breifly why you chose that family -->\n<!-- \t\\begin{enumerate} -->\n<!-- \t\t\\item -->\n<!-- \t\t\tBatting averages of MIAA base ball players.  (A batting average is roughly -->\n<!-- \t\t\tthe proportion of at bats where the batter gets a hit.) -->\n\n<!-- \t\\end{enumerate} -->\n<!-- \\end{problem} -->\n<!-- \\fi -->\n\n::: {.problem #exr-HELPrct-substance-qq}\n\nCreate normal quantile plots for the ages of patients in the `HELPrct`\n\tdata set separated by `substance`. (Getting separate or overlaid plots\n\tusing `gf_qq()()` works just like it does for other **`ggformula`** plots).\n\n\tComment on the plots.\n\n{{< pagebreak >}}\n\n\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_qq( ~ age | substance, data = HELPrct)\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-60-1.png){width=432}\n:::\n:::\n\n\nThe qq-plot for the alchol group looks good.  The other two (especially cocaine) show\nsigns of skew -- indicated by the curve to the qq plot.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dens( ~ age | substance, data = HELPrct)\n```\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/unnamed-chunk-61-1.png){width=432}\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-qq-matching}\n\nMatch the normal-quantile plots to the histograms.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/compareplots1-1.png){width=432}\n:::\n\n::: {.cell-output-display}\n![](04-random-variables_files/figure-html/compareplots1-2.png){width=432}\n:::\n:::\n\n\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\na) Y  b) V c) Z d) W  e) X f) U\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-var-shortcut}\n\nShow that $\\Var(X) = \\E(X^2) - \\E(X)^2$ by showing that \n$$\n\t\\int_{-\\infty}^{\\infty} (x - \\mu_X) ^2 f(x) \\; dx\n\t=\n\t\\int_{-\\infty}^{\\infty} x^2 f(x) \\; dx  -  \\mu_X^2\n$$\nwhenever $f$ is a pdf and all the integrals involved converge.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nSome algebra and properties of integrals are all we need:\n\n$$\n\\begin{aligned}\n\\int_{-\\infty}^{\\infty} (x - \\mu_X) ^2 f(x) \\; dx\n\t&=\n\t\\int_{-\\infty}^{\\infty} (x^2 - 2\\mu_X x + \\mu_X^2) f(x) \\; dx\n\t\\\\\n\t&=\n\t\\int_{-\\infty}^{\\infty} x^2 f(x) \\; dx\n\t- 2 \\mu_X \\int_{-\\infty}^{\\infty} x f(x) \\; dx\n\t+ \\mu_X^2 \\int_{-\\infty}^{\\infty} f(x) \\; dx\n\t\\\\\n\t&=\n\t\\Var(X) - 2 \\mu_X^2 + \\mu_X^2\n\t\\\\\n\t&=\n\t\\Var(X) - \\E(X)^2 \n\\end{aligned}\n$$\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-heights}\n\nThe heights of 18--22 year olds in the US follow approximately normal distributions\nwithin each sex.  Estimated means and standard deviations appear in the table below.\n\t\n\n::::{.center}\t\n+------------+----------------+-------------------------+\n+            + mean           + standard deviation.     +\n+============+================+=========================+\n+ women      + 64.3 in        + 2.6 in                  +\n+------------+----------------+-------------------------+\n+ men        + 70.0 in        + 2.8 in                  +\n+------------+----------------+-------------------------+\n::::\n\nAnswer the following questions without using a computer or calculator (except for basic\narithmetic).\n\t\n:::: {.enumerate}\n\na.  If a woman is 68 inches tall, what is her z-score?\nb.  If a man is 74 inches tall, what is his z-score?\nc.  What is more unusual, a woman who is at least 68 inches tall\n\t\t\tor a man who is at least 74 inches tall?\nd.  Big Joe has decided to open a club for tall people.  To join his club,\n\t\t\tyou must be in the tallest 2.5% of people of your sex. \n\t\t\tHow tall must a woman be to join Big Joe's club?\ne.  How tall must a man be to join Big Joe's club?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n:::: {.enumerate}\n\na. \n\n::: {.cell}\n\n```{.r .cell-code}\n(68 - 64.3)/2.6\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.423077\n```\n:::\n:::\n\n\n\nb. \n\n::: {.cell}\n\n```{.r .cell-code}\n(74 - 70) / 2.8\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.428571\n```\n:::\n:::\n\n\nc. \nIt's pretty close, but the z-score for the man is slightly\nlarger, so it is slightly more unusual for a man to be that tall.\n\nd. \n\n::: {.cell}\n\n```{.r .cell-code}\n64.3 + 2 * 2.6\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 69.5\n```\n:::\n:::\n\n\n\ne. \n\n::: {.cell}\n\n```{.r .cell-code}\n70 + 2 * 2.8\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 75.6\n```\n:::\n:::\n\n\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-heights-more}\n\nUse the information from @exr-heights to answer the following questions.\n\n:::: {.enumerate}\n\na.  What proportion of women are 5'10\" or taller?\nb.  What proportion of men are 6'4\" or taller?\nc.  If a man is in the 75th percentile for height, how tall is he?\nd.  If a woman is in the 30th percentile for height, how tall is she?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n:::: {.enumerate}\n\n#. \n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(70, mean = 64.3, sd = 2.6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01417865\n```\n:::\n:::\n\n\n\n#. \n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(76, mean = 70, sd = 2.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01606229\n```\n:::\n:::\n\n\n\n#. \n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(.75, mean = 70, sd = 2.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 71.88857\n```\n:::\n:::\n\n\n\n#. \n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(.30, mean = 64.3, sd = 2.6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 62.93656\n```\n:::\n:::\n\n\n\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n",
    "supporting": [
      "04-random-variables_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}