{
  "hash": "c8c472b00330a09f19913b755792f64a",
  "result": {
    "markdown": "\n# Propagation of Uncertainty {#sec-propagation}\n\n\n\n\n\n\n\n\nYou have probably been told (in physics labs, for example) to report all measurements\nalong with an **uncertainty**.  The reporting often uses the notation:\n\n$$\n\\mbox{measurement} \\pm \\mbox{uncertainty} \\;. \n$$\nBut what is uncertainty and how is it calculated?  These are topics for this chapter.\n\n## Error and Uncertainty\nAlthough many people mistakenly conflate the terms **error** and **uncertainty**, \nthese are two different, but related concepts.  \nThe word \"error\" in the context of scientific measurement has a rather\ndifferent meaning from its use in everyday English. It does not mean blunder or\ngoof (although a blunder or goof could increase the amount of experimental error). \nInstead, error refers to the unavoidable fact that\nthe measurements scientists record are not exactly correct.\n\n\nError is easily defined:\n\n::: {.boxedText #def-error}\n\n$$ \n\\mbox{error} = \\mbox{estimate} - \\mbox{estimand} \n$$\nor, equivalently,\n$$ \n\\mbox{error} = \\mbox{measurement} - \\mbox{measurand} \n$$\n::: \n<!-- end boxedText -->\n\nThat is, error is the difference between the number we have measured or calculated and the \nnumber that calculation is attempting to estimate.  In most applications, we do not know \nthe error exactly, because we do not know the estimand.  This is where uncertainty comes in.\n\n**Uncertainty** is a numerical measure summarizing how large the error *might be*.  There are several different types, or definitions, or uncertainty.  The different definitions of uncertainty have in common that they are all trying to describe a statistical distribution of errors.  \n<!--  Typically, we arrange things so that the mean -->\n<!--  of this distribution is 0 (so that on average our estimates will be correct).   -->\nKnowing something about this distribution\nof the errors can tell us how close our estimates tend to be to the estimand.\n\nWe will use **standard uncertainty** unless we say otherwise.\n::: {.boxedText #def-standard-uncertainty}\n\n\t**Standard uncertainty** is the (estimated) standard deviation of \n\tthe distribution of errors.\n::: \n<!-- end boxedText -->\n\nYou may wonder how we can know the distribution of errors when we cannot know the error.\nThat is a good question, which we will begin to address via an example.\n\n## An Example: Estimating the number of dimes in a sack of dimes\n\n\n\n\n\n\n\nSuppose you want to estimate the number of dimes in a large sack of dimes.  Here is one method\nyou could use:\n\n::: {.enumerate}\n\n#.  Measure the weight of all the dimes in the bag by placing them (without the bag)\n\t\ton an appropriately sized scale.  (Call this $\\hat B$, our estimate for $B$,\n\t\tthe actual weight of the dimes in the bag.)\n#.  Measure the weight of 30 individual dimes and use those measurements to\n\t\testimate the mean weight of dimes. (Call this $\\hat D$.)\n#.  Combine these two estimates to compute an estimated number of dimes in the bag.\n\t\t($\\hat N = \\hat B / \\hat D$.)\n::: \n<!-- end enumerate -->\n\n\nSuppose that the dimes in our our bag together weigh 10.2 kg and the mean weight of our 30 \nmeasured dimes is 2.2582333. \nThen we would estimate the number of dimes to be \n$$\n10200 / 2.2582333 = 4516.8051722 \\;.\n$$\nBut how good is this estimate?  Do we expect to be within a small handful of dimes?  Might we be \noff by 100 or 500?  Standard uncertainty provides a way to quantify this.  But first, \nwe need to calculate uncertainty for the two ingredients in this recipe: \nthe total weight of all the dimes in the bag ($\\hat B$), and the mean weight of the \n30 measured dimes ($\\hat D$).\n\n### Calculating a Standard Uncertainty without Using Data\n\nWe only measure the bag of dimes once (and might expect that the value\nobserved on the digital read out would be the same if we measured it repeatedly\nanyway), so the distribution involved in our uncertainty calculation will be \nbased on some assumptions about the workings of our scale.  For example, we could\nmodel a reading of 10.2 kg with\na $\\Unif(10.15, 10.25)$-distribution.^[Other models are possible, and the choice of \nmodel matters for the uncertainty calculation that will result.]  \nThis model reflects the assumption that if the actual weight is anywhere between 10.15 and 10.25,\nthe reading will be 10.2 and that the actual weight is equally likely to be anywhere within\nthat range.\n\nIf we interpret the 10.2 reading in this way, then the uncertainty can be\ncalculated as the standard deviation of a $\\Unif(10.15, 10.25)$-distribution:  \n\n$$\nu_{\\hat B} = \\frac{b-a}{\\sqrt{12}} \n= \\frac{10.25-10.15}{\\sqrt{12}} \n= \\frac{0.1}{\\sqrt{12}} \n= 0.0288675 kg\n= 28.8675135 g\n$$\n\n### Calculating a Standard Uncertainty Using Data\n\nThe situation for our estimated mean weight of a dime is a little different.  We weighed 30 dimes, \nand calculated the mean mass of one dime from those data.  But if we repeated the measurements many times -- \ntaking another 30 dimes, calculating the average...taking *another* 30 dimes, calculating \n*another* average...and so on many times...how much variability would there be in the \n*calculated averages*? \nWe will learn how to estimate this quantity ourselves soon; for now, \nwe will take it as a given that it is $\\frac{s}{\\sqrt{n}}$, \nwhere $s$ is the standard deviation of one sample (the masses of 30 dimes), and $n$ is the sample \nsize (here, 30). So the uncertainty in our mean dime weight is about:\n<!--  First, note that we measured the masses of 30 individual dimes, so -->\n<!--  we have data that can tell us something about the variability in the weights of dimes.   -->\n<!--  This would not be the case if we simply measured all 30 dimes at once and -->\n<!--  divided by 30 to get the average.  Second, we are interested in how accurately we can -->\n<!--  estimate the mean weight of the dimes in the sack,  -->\n<!--  not in how accurately we can estimate the weight of any particular dime.   -->\n<!--  df_stats( ~ mass, data = Dimes) -->\n<!--  -->\n\n$$\nu_{\\hat D} = \\frac{s}{\\sqrt{n}} \n= \\frac{0.0220699}{\\sqrt{30}} \n= 0.0040294\\;.\n$$\nSo we would report our estimate for the mean weight of a dime as \n$$\n2.2582333 \\pm 0.0040294 \\;.\n$$\n\nThis notation looks like a confidence interval,\nand indeed it is a confidence interval.\nSince we are using $t_* = 1$, this is approximately a 68\\% confidence interval: \n\n\n::: {.cell}\n\n```{.r .cell-code}\npt(1, df = 29) - pt(-1, df = 29)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.674418\n```\n:::\n:::\n\n\n\n\n### Propagating Error by the Delta Method\nNow that we have computed the uncertainties for $B$ and $D$, we need to find a way to combine\nthem to determine the uncertainty for $B/D$.  For this we use a linear approximation of the function $f(B,D) = B/D$.\n\nRecall from calculus that \n$$\nf(x,y) \\approx f(a, b)\n+\n\\frac{\\partial f}{\\partial x} (x-a)\n+\n\\frac{\\partial f}{\\partial y} (y-b) \\;,\n$$\nwhere the partial derivatives are evaluated at $(x,y) = (a,b)$.\n\nIf we apply this to our estimators $\\hat B$ and $\\hat D$, we get\n$$\nf(\\hat B,\\hat D) \\approx f( B, D ) \n+\n\\frac{\\partial f}{\\partial \\hat B} (\\hat B- B)\n+\n\\frac{\\partial f}{\\partial \\hat D} (\\hat D- D)\n\\;.\n$$\n<!-- \\iffalse -->\n<!-- $$ -->\n<!-- f(\\hat B,\\hat D) \\approx f( \\mu_{\\hat B}, \\mu_{\\hat D})  -->\n<!-- + -->\n<!-- \\frac{\\partial f}{\\partial \\hat B} (\\hat B-\\mu_{\\hat B}) -->\n<!-- + -->\n<!-- \\frac{\\partial f}{\\partial \\hat D} (\\hat D-\\mu_{\\hat D}) -->\n<!-- \\;. -->\n<!-- $$ -->\n<!-- \\fi -->\n<!-- where the partial derivatives are evaluated at $(\\hat B, \\hat D) = (B,D)$. -->\n\nFrom this it follows that\n$$\n\\begin{aligned}\n\t\\E(f(\\hat B,\\hat D)) &\\approx \n\t\\E\\left(f( B, D)\\right)\n+\n\\E\\left( \\frac{\\partial f}{\\partial {\\hat B}} (\\hat B- B) \\right)\n+\n\\E\\left( \\frac{\\partial f}{\\partial \\hat D} (\\hat D- D) \\right)\n\\\\\n&\\approx f(B,D) + 0 + 0 \n\\\\\n&= f(B,D) \\;.\n\\end{aligned}\n$$\n\n(The 0's come because our estimators are approximately unbiased: $\\E(\\hat B) \\approx B$ and $\\E(\\hat D) \\approx D$.)\nThis says that $\\hat B/ \\hat D$ is a reasonable estimate for $B/D$ -- it is approximately\nunbiased.^[There is a small sleight of hand here.  Technically, we should evaluate\nthe partial derivatives at the unknown values $B$ and $D$.  We will instead plug in our \nparticular estimates (from our data) for $\\hat B$ and $\\hat D$.  To denote all of this \ncompletely rigorously, we would need to have separate notation for $\\hat B$ considered\nas a random variable (that has an expected value and variance) and as a number (the value\ncomputed from our particular data).  We're avoiding this extra layer of notation.]\n\nBut we really want an expression for the uncertainty -- the variance (which we will turn into a standard deviation).  We use similar logic to the expectation calculation above, and we will need to use the finding (not proven here) that for constants $a$ and $b$ and random variable $X$, $Var(aX+b) = a^2 Var(X)$. Assuming $\\hat B$ and $\\hat D$ are independent,^[A more general formula\ncan approximate the propagation of uncertainty in cases where $\\hat B$ and $\\hat D$ \ncannot be assumed to be independent.] a reasonable assumption in this situation,\nwe get\n\n$$\n\\begin{aligned}\n\t\\\\[5mm]\n\\Var(f(\\hat B,\\hat D))\n&\\approx \n\\Var\\left(f( B, D)\\right)\n+\n\\Var\\left( \\frac{\\partial f}{\\partial \\hat B} (\\hat B - B) \\right)\n+\n\\Var\\left( \\frac{\\partial f}{\\partial \\hat D} (\\hat D - D) \\right)\n\\\\\n&=\n0 \n+\n\\left(\\frac{\\partial f}{\\partial \\hat B}\\right)^2 \\Var(\\hat B)\n+\n\\left(\\frac{\\partial f}{\\partial \\hat D}\\right)^2 \\Var(\\hat D)\n\\end{aligned}\n$$\n\nwhere again we evaluate the partial derivatives with $\\hat{B}$ and $\\hat{D}$.\n\nApplying this to $f(\\hat B,\\hat D) = \\hat B/\\hat D$, we get\n$$\n\\begin{aligned}\n\t\\frac{\\partial f}{\\partial \\hat B} &= \\frac{1}{\\hat D}\n\t\\\\\n\t\\frac{\\partial f}{\\partial \\hat D} &= \\frac{-\\hat B}{\\hat D^2}\n\\end{aligned}\n$$\nSo our uncertainty for the estimated number of dimes (remember, we want the\nstandard deviation, which will be the square root of the variance estimate we\njust derived) is\n$$\n\\sqrt{\n\\frac{1}{2.2582333^2} \\cdot 28.8675135^2 \n+ \\left(\\frac{1.02\\times 10^{4}}{2.2582333^2}\\right)^2 \\cdot 0.0040294^2}\n=\n15.1117467 \\;,\n$$\nand we would report our estimated number of dimes as \n$$ \n4517 \\pm 15 \\; \\mbox{dimes} \\;.\n$$\n\nThe method described in this example is generally referred to as the\n**Delta Method**.  Often scientists and engineers who are using this method don't use the hat notation\nto distinguish between estimates/estimators and estimands.  In the box below, we've \ndropped the hats.\n\n::: {.boxedText}\n\n### The Delta Method for independent estimates {-}\n\nLet $X$ and $Y$ be independent estimates with uncertainties $u_{X}$ and $u_{Y}$,  \nand let $W = f(X,Y)$.\nThen the uncertainty in the estimate for $W$ can be estimated as \n$$\n\tu_{W} \\approx\n\t\\sqrt{ \n\\left(\\frac{\\partial f}{\\partial X}\\right)^2 u_X^2\n+\n\\left(\\frac{\\partial f}{\\partial Y}\\right)^2 u_Y^2\n\t}\n$$\nwhere the partial derivatives are evaluated using estimated values of $X$ and $Y$.\n\n\nThe Delta Method can be extended to functions of more (or fewer) than two variables by\nadding (or removing) terms.  Slightly more complicated formulas exist to handle\nsituations where the estimators are not independent (but we will not cover those in this course).\n\t\nBecause this method is based on using a linear approximation to $f$, it works better\nwhen the linear approximation is better.  In particular, when\n$\\frac{\\partial^2 f}{\\partial X^2}$ or\n$\\frac{\\partial^2 f}{\\partial Y^2}$ are large near the estimated \nvalues of $X$ and $Y$, the approximations might not be very good.\n::: \n<!-- end boxedText -->\n\n\n### Estimating Uncertainty via Simulations\n\nWe can also estimate the uncertainty in the estimated number of dimes using simulations.\n\n\n\n::: {.cell hash='Stat241-Propagation_cache/epub/dimes-sim_049f5c0cb15822d95b018bb2fdff45e7'}\n\n```{.r .cell-code}\nB <- runif(10000, 10150, 10250)\nSampleMeans <- do(10000) * mean( ~ mass, data = resample(Dimes) )\nhead(SampleMeans ,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mean\n1 2.254000\n2 2.257867\n3 2.252633\n```\n:::\n\n```{.r .cell-code}\nD <- SampleMeans$mean\nN <- B / D\nhistogram( ~ N)\n```\n\n::: {.cell-output-display}\n![](Stat241-Propagation_files/figure-epub/dimes-sim-1.png)\n:::\n\n```{.r .cell-code}\ngf_qq( ~ N)\n```\n\n::: {.cell-output-display}\n![](Stat241-Propagation_files/figure-epub/dimes-sim-2.png)\n:::\n\n```{.r .cell-code}\nsd(N)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15.05321\n```\n:::\n:::\n\n\n\nA few explanatory notes regarding the computation above.\n::: {.enumerate}\n\n#.  `resample()()` samples from a data frame *with replacement*.  That is,\n\t\tsome of the rows may appear more than once, others not at all.  Resampling\n\t\tis a common way to estimate a sampling distribution from a single sample.\n\t\t`resample()()` can also be used on a \"bare\" vector (that is, a vector of data points not contained within a data frame, as exemplified below).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresample(1:10)    # some items may be chosen more than once\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  7  4  7  1  4  3 10  7  2  3\n```\n:::\n\n```{.r .cell-code}\nsample(1:10)      # no item may be chosen more than once, so this just shuffles\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  6  4 10  5  8  7  1  9  2  3\n```\n:::\n\n```{.r .cell-code}\nshuffle(1:10)     # this also shuffles\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  7  8  4  3  6  9  1  5 10  2\n```\n:::\n:::\n\n\n\n#. \n\tWhen `do()()` doesn't know what to call the result of what it is \"doing\", it\n\tcalls it `result`.  (Sometimes `do()()` can figure out a better name.)\n#. \n\tThe simulated distribution of $N = B/D$ is unimodal, roughly symmetric, and reasonably \n\twell approximated by a normal distribution (but clearly not exactly normal).  \n\tThe Delta Method does not guarantee\n\tthat the distribution will be approximately normal -- it only estimates the \n\tvariance.  Sometimes the distribution will be skewed or have heavier or lighter\n\ttails than a normal distribution.\n#. \n\tThe results of the Delta Method and simulation are very close.  Each is an acceptable method for approximating \n\tthe same thing, so this is not surprising.\n::: \n<!-- end enumerate -->\n\n\n## Reporting Measurements and Estimates\n\n### What to record, What to report\n \nWhen you record the results of a measurement for which there is an *a priori*\nestimate of uncertainty,  the uncertainty should be recorded along with\nthe measurement itself.  Similarly, reports of quantities estimated from data\nshould also include estimated uncertainties.\n\nAs a general guideline, a properly reported scientific estimated quantity includes \nthe following five elements: \n\n::: {.enumerate}\n\n#.  A number (the estimate)\n#.  Units (e.g., m  or kg or seconds) \n#.  A statement about how it was measured or calculated \n#.  A statement about most likely sources of (the largest components of) error\n#.  An estimate of the uncertainty\n::: \n<!-- end enumerate -->\n\n\n::: {.example #exm-pendulum}\n\nIf you measured the length of a pendulum using a meter stick, you\nmight report the measurement this way: \n\n:::: {.itemize}\n\n#. Length $= 0.834 \\pm 0.002$ m \n#. Measured with a meter stick from pivot point to the center of the steel weight. \n\n#. Uncertainty reflects the limited accuracy of measurement with a meter stick.\n:::: \n<!-- end itemize -->\n\n::: \n<!-- end example -->\n\nIn plots, the number is given by the scales of the plot, the units are typically included\nin the axes labels, uncertainties may be represented by \"error bars\", and a statement \ndescribing the method of measurement or calculation should appear in the plot legend.\n \n### How many decimal places?\n \nNumerical values and their uncertainties should be recorded to the proper number of\ndecimal places.  Most software either reports too many significant digits or\nrounds numbers too much.  For \ncorrect professional presentation of your data, follow these guidelines:\n::: {.enumerate}\n\n#. The experimental uncertainty should be rounded to one significant \nfigure unless the leading digit is a 1, in which case, it is generally better to use two digits.\n#. A measurement should be displayed to the same number of decimal places as the \nuncertainty on that measurement.  \n::: \n<!-- end enumerate -->\n\nNote carefully the difference between significant figure and decimal place.  \nThe following examples will help: \n\n::: {.example #exm-uncertainy-reporting-1}\n\nThe timer reports a value of 0.3451 seconds.  The uncertainty on the measurement\nis 0.0038 seconds. By Rule 1, the uncertainty should be reported to one\nsignificant figure, so we round it to 0.004 seconds. By Rule 2, the measurement\nmust also be rounded to the third decimal place.  Thus, the measurement should\nbe reported as $0.345\\pm0.004$ seconds.\n::: \n<!-- end example -->\n\n                                                 \n::: {.example #exm-uncertainy-reporting-2}\n \nThe measured value is $7.92538 \\cdot 10^4$, and its uncertainty is $2.3872 \\cdot 10^2$.\nBy Rule 1, the uncertainty should be rounded to one significant\nfigure, so $2 \\cdot 10^2$. By Rule 2, we report the measurement to the same\ndecimal place as the uncertainty, so $7.93 \\cdot 10^4$. Putting it together, the\nmeasurement should be reported as $(7.93\\pm0.02) 10^4$.\n::: \n<!-- end example -->\n\n \n \n::: {.example #exm-uncertainty}\n \nThe estimated value is $89.231$, and its uncertainty is $0.1472$.    By Rule 1,\nthe uncertainty should be rounded to two significant figures, so $0.15$. By Rule\n2, we report the estimate to the same decimal place as the uncertainty, so\n$89.23  \\pm  0.15$.\n::: \n<!-- end example -->\n\n\n<!-- %For more on reporting uncertainties and rounding, see section \\ref{sec:3F} -->\n \n\n### Reporting numbers in a table\n \nMultiple similar measurements should be reported in a table.  The column\nheadings should clearly and concisely indicate the quantity in each column; the\ncolumn heading must include the units.   Uncertainties should be listed in a\nseparate column, located just to the right of the measurement column.  (Sometimes, uncertainties are listed in parentheses after the estimate instead; just make sure the header and legend of the table makes it clear what values are being reported, and where.) \n\n::: {.example #exm-kinetic-energy-uncertainty}\n\nA lab group calculated these numbers for kinetic energy and its uncertainty: \n\n| Kinetic Energy | uncertainty |\n| 0.8682 | 0.059 |\n| 1.0661 | 0.071 |\n| 1.0536 | 0.070 |\n| 1.3881 | 0.058 |\n| 0.8782 | 0.108 |\n\nThis should be reported with appropriate rounding as\n\n\n| Kinetic Energy | uncertainty   |\n|----------------|---------------|\n| 0.87 | 0.06 |\n| 1.07 | 0.07 |\n| 1.05 | 0.07 |\n| 1.39 | 0.06 |\n| 0.88 | 0.11 |\n::: \n<!-- end example -->\n\n\n\n\n## Additional Propagation of Uncertainty Examples {#sec-propagation-examples}\n\n::: {.example}\n\n**Q.** \n\tThe side of a square is measured and reported as $12.3 \\pm 0.2$ mm.\n\tHow should the area be reported?\n\n**A.** \nOur estimate for the area is $12.3^2 = 151.29$. \nOur transformation is $f(x) = x^2$, so $\\Partial{f}{x} = f'(x) = 2x$.  \nApplying the Delta Method, our uncertainty is\n\n$$\n\t\\sqrt{ (2 (12.3))^2 (0.2)^2 } = 2 (12.3)(0.2) = 4.92\n$$\nand we report the area as\n$151 \\pm 5$.\n\nIt is worth looking at the relative uncertainty of the linear and area measurements.\n\t\n$$\n\\begin{aligned}\n\t\t\\frac{0.2}{12.3} &= 0.0162602\n\t\t\\\\\n\t\t\\frac{5}{151} \n\t\t&= 0.0325\n\\end{aligned}\n$$\n\nSo the relative uncertainty of the area measurement is twice the relative uncertainty\nof the linear measurement.\n::: \n<!-- end example -->\n\n\nThe preceding example demonstrates a simplified version of the Delta Method formula \nwhen we are dealing with only one estimator.\n\n::: {.boxedText}\n\n\t%s{\\textsf{\\bfseries The Delta Method for one estimator}}\n\nLet $X$ be an estimator with uncertainty $u_{X}$ and let $\\hat W = f(\\hat X)$.\nThen the uncertainty in the estimate $W$ can be estimated as \n\n$$\n\tu_{W} \\approx\n\\left|\\frac{df}{dX}\\right| u_X\n$$\nwhere the derivative is evaluated using the estimated value of $X$, $\\hat{X}$.\n::: \n<!-- end boxedText -->\n\n\n\n\n::: {.example #exm-box1}\n\n**Q.** \nThe sides of a rectangle are measured and reported as $12.3 \\pm 0.2$ mm and\n$6.3 \\pm 0.1$ mm.\nHow should the area be reported?\n\n**A.** \nOur estimate for the area is $12.3 \\cdot 6.3 = 77.49$.  \n\nNow we need to estimate the uncertainty.\nOur transformation is $f(x,y) = xy$, so   $\\frac{\\partial f}{\\partial x} = y$ and \n$\\frac{\\partial f}{\\partial y} = x$. \n\n$$\n\\sqrt{ 6.3^2 \\cdot 0.2^2 + 12.3^2 \\cdot 0.1^2 } =\n1.7608237 \n$$\n\nand we should report the area as \n\n$$\n77.5 \\pm \n\t1.8 \n$$\n<!-- or -->\n<!-- $$ -->\n<!-- 77 \\pm 2  -->\n<!-- $$ -->\n::: \n<!-- end example -->\n\n\nSometimes it is more convenient to think about **relative uncertainty**:\n\n::: {.boxedText #def-relative-uncertainty}\n\n### Relative uncertainty {-}\n\n$$\n\\mbox{relative uncertainty} \n= \\frac{ \\mbox{uncertainty of measurement} }{\\mbox{magnitude of measurement}}\n$$\nFor example, if we measure a mass to be $10.2$ g with an uncertainty of $0.3$ g, the \nrelative uncertainty is \n\n$$\n\\frac{0.3}{10.2} \n= 0.0294118\n= 2.9 %\n$$\nOften it is the case that uncertainty grows with the magnitude of the estimate, and relative uncertainty\nis a way of comparing the uncertainty in large measured values with the uncertainty of \nsmall measured values on a more equal basis.  Relative uncertainty is also independent \nof the units used.\n::: \n<!-- end boxedText -->\n\n\nIn the example above, we get a nice formula if we compute relative uncertainty\ninstead of absolute uncertainty.  Let $P = XY$ where $X$ and $Y$ have\nuncertainties $u_X$ and $u_Y$.  Then $\\Partial{P}{X} = Y$ and $\\Partial{P}{Y} =\nX$, so\n\n$$\n\\begin{aligned}\n\\frac{u_P}{P} \n& = \\sqrt{ \\frac{ Y^2 u_X^2 + X^2 u_Y^2}{P^2} }\n\\\\\n& = \\sqrt{ \\frac{ Y^2 u_X^2 + X^2 u_Y^2}{X^2Y^2} }\n\\\\\n& = \\sqrt{ \\frac{ u_X^2}{X^2} + \\frac{u_Y^2}{Y^2} }\n\\\\\n& = \\sqrt{ \\left(\\frac{ u_X}{X}\\right)^2 + \\left(\\frac{u_Y}{Y}\\right)^2 }\n\\end{aligned}\n$$\nwhich gives a Pythagorean identity for the relative uncertainties.\n\nThe computations are the same for any product.  \nSo this Pythagorean identity for relative uncertainties can be \napplied to estimate uncertainties for quantities such as \narea (length $\\times$ width), \nwork (force $\\times$ distance), \ndistance (velocity $\\times$ time), etc.\n\n::: {.example #exm-relative-uncertainty-rectangle}\n\n**Q.** \nUse relative uncertainty to estimate the area of the rectangle \nin \t@exm-box1.\n\n**A.** \nThe relative uncertainties in the length and width are \n$$ \n\t\\frac{0.2}{12.3} = 0.0162602 \\ \\tand \\  \n\t\\frac{0.1}{6.3} = 0.015873 \n\t\\;.\n$$\n\tSo the relative uncertainty in the area estimation is \n$$\n\t\\sqrt{ \n\t(0.0162602)^2 +  (0.015873)^2 }\n\t=\n\t0.0227232\n\t\\;.\n$$\n\tNow we solve \n$$\n\t\\frac{u_A}{77.49} = 0.0227232\n$$\nto get \n$$\n\tu_A = (77.49) (0.0227232) =\n\t1.7608237 \n\t\\;.\n$$\nNotice that this matches the result from @exm-box1.\n\n::: \n<!-- end example -->\n\n\n::: {.example #exm-resistors}\n\n**Q.** \nWhen two resistors with resistances $R_1$ and $R_2$ are connected in parallel, \nthe combined resistance satisfies\n$$\nR = \\frac{R_1 R_2}{R_1 + R_2} \n$$\nSuppose the resistances of the two resistors are reported as \n$20 \\pm 0.7$ ohms and $50 \\pm 1.2$ ohms.  How should you report the combined resistance?\n\n**A.** \nOur estimate is $\\hat R = \\frac{ 20 \\cdot 50}{ 20 + 50} = 14.2857143$.\nTo estimate the uncertainty, we need the partial derivatives \n$\\Partial{R}{R_1}$ and $\\partial{R}{R_2}$.\n\n$$\n\\begin{aligned}\n\t\t\\Partial{R}{R_1} &= \\frac{ (R_1+R_2)R_2 - (R_1 R_2) }{(R_1 + R_2)^2}\n\t\t\\\\\n\t\t&= \\left( \\frac{ R_2}{R_1 + R_2}\\right)^2 \n\t\t\\\\\n\t\t&= \\left( \\frac{ 50}{20 + 50}\\right)^2  = 0.5102041\n\\end{aligned}\n$$\nSimilarly,\n$$\n\\begin{aligned}\n\\Partial{R}{R_2} \n&= \\left( \\frac{ R_1}{R_1 + R_2}\\right)^2\n\\\\\n&= \\left( \\frac{ 20}{20 + 50}\\right)^2 = 0.0816327\n\\end{aligned}\n$$\n\n\n\n\n\n\nSo our estimated uncertainty is given by\n$$\n\tu_R = \\sqrt{ (0.5102041)^2 (0.7)^2 + (0.0816327)^2)(1.2^2) }\n\t= 0.3703337 \\;.\n$$\nSo we report the combined resistance as \n\n$$\n14.3 \\pm 0.4\n$$\n\n::: \n<!-- end example -->\n\n\n\n## Experimental Error and Its Causes \n\nThere are many reasons for experimental error, and it is important to identify\npotential causes for experimental error, to reduce their effects when possible,\nand to handle them appropriately in any case.\n\n\n### Random error: Same procedure, different results\n\nEven if measurements are taken by carefully trained scientists using\nhighly precise instruments, repeated measurements of the \"same thing\"\nmay not always give the same value.\n\nAll data collection is done in the context of variability,\nand statistics allows us to interpret our data in this context.\n\n\n#### The moving target\nOne reason that a measurement may change is that what we are measuring\nmay be changing.  Some quantities depend on environmental factors (like \ntemperature and atmospheric pressure, for example) that may change\nbetween measurements.   This sort of variability can often be reduced\nby attempting to  control factors that might lead to such variability.\nFor example, a delicate experiment might be conducted in a\nclimate controlled chamber.\nAnother solution is to use a model that includes additional variables \nfor these quantities.  Often a combination of these two approaches is used.\n\nIf measurements are made using similar (but not identical) objects, then\ndifferences among those objects may lead to variability in measurements.   If\nmeasurements are made on a sample of living things, the variability from one\nindividual to the next could be quite large.  But even in the physical\nsciences, each run of an experiment may require the use of different\n``consumables\" that have slightly different properties that affect \nour measurements.\n\n<!-- \\iffalse -->\n<!-- \\subsubsection{Measurement error} -->\n\n<!-- Another source of variability is the measuring process itself.  Every -->\n<!-- measurements are made on a sample of living things, the variability from one -->\n<!-- individual to the next could be quite large.  But even in the physical -->\n<!-- sciences, each run of an experiment may require the use of different -->\n<!-- ``consumables'' that have slightly different properties that affect  -->\n<!-- our measurements. -->\n<!-- \\fi -->\n\n\n#### Measurement error\n\nAnother source of error is the measuring process itself.  Every\nmeasuring device has its limits, as do the humans who are using them.\nIf you repeatedly timed how long it takes for a steel ball to fall from a fixed \nheight, it is quite likely \nthat you would not get exactly the same result each time.  The amount of \nvariability would likely increase if several different students were \neach asked to measure the time as different students\nmight employ slightly different methods, or be more or less skillful in \ntheir measuring.\n\nThe variability from one measurement to another that would exist even if there\nwere no moving target effect is called **measurement error**.  In practice,\nit can be difficult or impossible to isolate the moving target effect from\nmeasurement error, so they may be combined into one source of variability which\nwe will call **random error**.\nIn physics, the moving target effect is often small -- at least in carefully\ndesigned experiments -- so that random error is dominated by the \ndifficulties of measuring the quantity under study.  In other disciplines,\nthe relative magnitudes may be reversed.\n\nThe best way to estimate the effects of random error is by making repeated\nmeasurements and comparing them.  The **discrepancies** (differences between\nmeasurements) provide an indication of the amount of random error.  \n\nSometimes\nadditional information can also be used to help us estimate random error.  This\ncan be especially important when our ability to take repeated measurements is\nlimited or when limitations of our measurement apparatus make it impossible\nto directly observe the effects of random error.\n\n#### Invisible measurement error\nAlthough there is no theoretical limit to the precision of a numerical quantity \nlike mass, time, velocity, etc., every measurement device has limited precision.\nBecause of this lack of precision in our measurement device, it may well be \nthat repeated measuremets will all look identical.  \n*But this does not mean that they are exactly correct.*  \n\nFor example,\nif we measure temperature using a digital thermometer that has a display\nshowing tenths of a degree C, then any temperature between 57.15 and 57.25 will\nbe displayed as 57.2.  Any variability within that range will be invisible to\nour thermometer.\nSimilarly, if we meaure with a ruler with a 1/8 inch scale, we can\nuse interpolation to get not only to the nearest 1/8 inch, but likely to the \nnearest 1/16 inch or (with some practice) perhaps to the nearest 1/32 inch.  But\nbeyond that, we really cannot tell.  If more precise measurements are required,\na different measuring tool will need to be used.\n\nIf other sources of variability are small, this kind of measurement error \nmay completely mask them.\n\n### Systematic error: A tendency to over- or under-estimate\nEven if your target were not moving, and even if there were no \nvariability in measurements from time to time, and even if our measurement\napparatus were perfectly precise, there is still\na chance that a measurement might not give the value we are searching \nfor because the procedure used might tend to give results that over-\nor under-estimate the quantity being measured.  This kind of error may be referred to as either **bias** or **systematic error**.\n\n\n::: {.boxedText #def-bias}\n**Bias** or **Systematic error** refers to the tendency \nto either over- or under-estimate a quantity.  It is a tendency to \"be off \nin a particular direction\".\n::: \n<!-- end boxedText -->\n\n\n#### Calibration errors\nPerhaps the easiest type of systematic error to understand is **calibration error**.\nIf a measuring device is not properly calibrated, the resulting measurements may be \ntoo large or too small.\nFor example, if a timing device uses an internal clock that runs a bit slow, it will\ntend to underestimate times.  It might be possible to correct for this bias by\nperforming calibration exercises comparing this timing device to another \n(more accurate) device.  If several similar timing devices systematically\ndisagree, but there is no reference to calibrate with, then we have evidence\nthat at least some of the devices are introducing systematic error, but we may\nnot know which ones or how much.\n\nCalibrating equipment and procedures by using them to measure or compute\nstandard quantities is an important part of quality scientific experimentation\nbecause it helps reduce the effects of systematic bias.  \n\n\n#### Design flaws\n\nPoorly designed experiments can also introduce systematic error.  For example,\nimagine an experiment where a steel ball is dropped from a platform at\ndifferent heights and the time is recorded until the ball hits the ground.  \nTo save time, the researchers set the platform at a specified height and drop the ball\nmultiple times before moving the platform to a new height and repeating.\nUsing this method, any error in measuring the height of the platform will affect all\nthe drops from that height *in the same way*.  This introduces an unknown amount\nof systematic error into the measurements taken at each height.  An alternative design\nin which the heights are done in random order and in which the platform height is \nreset before each drop would likely have somewhat more random error but would avoid this\nsource of systematic error.\n\n#### Dealing with systematic error\nSystematic error is generally more difficult to handle than random error in\npart because there is often no good way to measure how large systematic error\nmight be or even to detect that it is occurring.\n\n### Relative Magnitudes of Errors from Different Sources\n\nWe have identified several potential sources of error.  \n<!-- In the next section we will learn -->\n<!-- how to quantify the uncertainty that results.  But even without quantified methods,  -->\nIt is good to get in the habit of qualitatively determining which sources you expect to contribute\nrelatively larger and smaller amounts of error.  Often we can ignore the sources of relatively\nsmaller potential error and focus our attention on the sources of relatively larger potential\nerror.\n\n\n## Uncertainty -- How Much Error Might There Be?\n\nIn @sec-propagation-examples we did several examples of propagation of \nuncertainty.  But uncertainty cannot be propagated to derived estimates unless we already\nhave estimated uncertainties for the components of the derivation.\nWhere do these uncertainty estimates come from?\n\nAs we have mentioned before, it is not possible to measure error directly.  \n<!--  This makes dealing with error a bit tricky.  In our dimes example we illustrated -->\n<!--  two common ways that uncertainty estimates are obtained.  -->\nIf we knew the amount of an error exactly, we would correct for it and obtain the exact, correct \nestimate of the value we were trying to measure.  \nSince we do not know the error exactly, we have to try to use our data \n(and our prior knowledge about the situation) to try to estimate it. In this section we focus \nour attention on this part of the uncertainty calculation.\n\n### Looking at variability in your data\n\nIf you have repeated measurements, a histogram, boxplot, or density plot of these \nmeasurements can provide a \nvisual representations that shows both what is \"typical\" or \"average\" and how much \nvariability there is in the data.  \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Three graphical representations of the same 25 measurements](Stat241-Propagation_files/figure-epub/fig-variability-1.png){#fig-variability-1 width=.3\\textwidth}\n:::\n\n::: {.cell-output-display}\n![Three graphical representations of the same 25 measurements](Stat241-Propagation_files/figure-epub/fig-variability-2.png){#fig-variability-2 width=.3\\textwidth}\n:::\n\n::: {.cell-output-display}\n![Three graphical representations of the same 25 measurements](Stat241-Propagation_files/figure-epub/fig-variability-3.png){#fig-variability-3 width=.3\\textwidth}\n:::\n:::\n\n\n\n\nIn addition, graphical displays of your data may help isolate outliers -- values\nthat don't seem to fit the pattern of the rest of the data.  Outliers should not be removed\nfrom your data without furhter investigation.  You need to know why these values are \ndifferent from the rest.  Was there a mistake in the measurement?  Was the value recorded\nincorrectly (decimal point in the wrong place, wrong units, transposed digits, typo)?\nOr was there something different going on -- so that the potential outlier is *really* an important, informative data point that is trying to tell you something about the process you are measuring?  An outlier might be the key observation in \nyour data set -- don't throw it away without investigating.\n\n### Estimating uncertainty from your data {#sec-uncertainty-from-data}\n\nWhenever possible, you should make multiple measurements of the same phenomenon.\nThe variability among these measurements allows us to estimate the uncertainty.\nIf we are using the mean of multiple measurements as our estimate, then the uncertainty\nis the \"standard error of the mean\" (which we will derive in the following chapter):\n\n$$ \nSE = \\frac{s}{\\sqrt{n}} \n$$\nThis is how we estimated the uncertainty in our estimate for the mean weight of a dime.\n\nIn other more complicated study designs, some other standard error formula may be used.\n\n### Estimating uncertainty in other ways {sec-uncertainty-without-data}\n\nEstimating uncertainty from data is only possible if \n::: {.itemize}\n* You have multiple measurements from which to estimate the variability, and \n* The measurements actually vary (aren't masked by imprecise measuring devices, \nfor example).\n::: \n<!-- end itemize -->\n\n\nFrequently, these conditions are not met, but we still want to quantify the uncertainty.\n\n<!-- #### Overcoming lack of measurement precision -->\n\n#### The uniform model for measurement (im)precision\n\nThere are many measuring devises that provide limited precision so that the \nbest we may be able to say is that we know the measured value to be in some interval $[a,b]$.\nThis would be the case for measuring devices with digital displays, for example.  \nImagine a device that displays 8.03 on its digital display.  \nPresumably, this means that the actual measurement can lie anywhere between \n8.025 and 8.035.  This value has been rounded for digital display, and we \nhave no way of knowing where within that interval the value might be.\n\nWe don't want to report the uncertainty as $b-a$ or even $(b-a)/2$.  These would be\noverestimates of the \"average\" amount of error.\nHere, we aren't looking for an upper bound on the potential error; we want to estimate something \nlike the average amount of error.\n\nIn this case we can estiamte a standard uncertainty based on the **uniform distribution**.  \nRecall, a uniform\ndistribution is one in which every value in some interval is \"equally likely\".\nA uniform distribution has standard deviation $s = \\frac{b-a}{\\sqrt{12}}$, so we will\nuse this as our estimate for standard uncertainty as well.\nNotice that \n$\\frac{2}{\\sqrt{12}} = \\frac{1}{\\sqrt{3}} = 0.5773503$.\nThis means that adding and subtracting one standard uncertainty from the center \nof the interval\n$$\n\\frac{a+b}{2} \\pm \\frac{b-a}{\\sqrt{12}}\n$$\nwill cover about 58% of the interval.\nThis is quite a bit less than the 68% covered by the central portion of a normal\ndistribution (within 1 standard deviation of the mean).\nFor a \"back of the envelope\" computation, to use an uncertainty with a similar amount of \"coverage\" to the \"coverage\" that the standard deviation has for the normal distribution, we might choose to use the approximation \n$\\frac{b-a}{\\sqrt{12}} \\approx \\frac{b-a}{3}$,\nsince this will slightly over-estimate the uncertainty and lead to a central covering\nprobability of $2/3 \\approx 68$%.\n\nThis same idea can be used when working with analog scales, but in this case, we typically\ncan see that the value is closer to one end than the other or closer to the center than\nto the edges.  This reduces our uncertainty by a factor of 2.  For example, given a ruler\nmarked in mm, if we can tell than a reading is closer to 12.3 mm than it is to 12.4 mm, then we \nare saying we know the value is in the interval $[12.3, 12.35]$, which is only half as \nwide as the scale of the ruler.  We can then use a uniform distribution as above, except that the limits $a$ and $b$ of the distribution are a bit narrower now.  \n(On some analog scales, it may be possible to do even better than this.)\n\n::: {.example #exm-reading-scale}\n\nMeasuring length on a ruler with a 1 mm scale, we could use $\\frac{1/2}{\\sqrt{12}}$\nas the estimated uncertainty of the measurement if the primary source of error is reading\nthe scale.  Of course, it may be that lining up the scale with the object being measured\nintroduces more uncertainty than reading the scale does.  That may lead us to choose a \nlarger value for our estimated uncertainty.\n::: \n<!-- end example -->\n\n\n#### Other models for measurement (im)precision\n\nOther distributions, most notably the triangle and normal distributions, are sometimes\nused instead of the uniform distribution to model errors in measurements made\nwith various devices.  Each of these models produces a somewhat smaller estimate\nfor the uncertainty than you would get using a uniform distribution.  In the case of the normal distribution, typically one considers\nhalf the width of the interval to be spanned by 3 standard deviations of the\nnormal distribution (since that would capture 99.7% of the distribution.  If $a$ and $b$ \nare the lower and upper limits of plausible values corresponding to a measurement,\nthe three uncertainty calculations are as follows:\n\n\n| distribution | uncertainty                            |\n|--------------|----------------------------------------|\n|\tuniform      | $\\displaystyle \\frac{b-a}{2 \\sqrt{3}}$ |\n| triangle     | $\\displaystyle \\frac{b-a}{2 \\sqrt{6}}$ |\n|\tnormal       | $\\displaystyle \\frac{b-a}{2 \\cdot 3}$  |\n\n: Uncertainties based on 3 different distributions. {#tbl-uncertainties-from-distributions}\n\n\nThis makes the uniform distribution the most conservative and the normal distribution the least conservative.\n\n<!-- Because our uncertainty is based on a standard deviation, we will be able to combine -->\n<!-- it with other uncertainties when we learn about propagation of uncertainty. -->\n\n#### Taking advantage of other data\n\nSometimes previous experience with a device or protocol may provide us with a good \nestimate for the uncertainty even before we collect our data.  In such cases, we can \nuse these *a priori* uncertainties both in planning and in analysis, but it is also\ngood to check that the data collected are consistent with the estimated uncertainties.\n\n\n## Exercises\n\n::: {.problem #exr-diabetes-drug}\n\nA clinical trial with 30 patients has been performed in\nwhich the volume of distribution \n(the theoretical volume that would be necessary to contain the total \namount of an administered drug at the same concentration that it is observed \nin the blood plasma) of a new anti-diabetes drug \nwas measured for each patient.  The sample mean was 10.2 L with a \nstandard deviation of 1.9 L.\n:::: {.enumerate}\n\n#. \nCalculate a 95% confidence interval \nfor the the mean volume of distribution.\n#. \nCalculate a 99% confidence interval \nfor the the mean volume of distribution.\n#. \nCalculate the standard uncertainty for the mean volume of distribution.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 95 <!--  CI -->\nt.star <- qt(.975, df = 29); t.star\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.04523\n```\n:::\n\n```{.r .cell-code}\nME <- t.star *  1.9 / sqrt(30); ME\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7094717\n```\n:::\n\n```{.r .cell-code}\n1.2 + c(-1,1) * ME\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4905283 1.9094717\n```\n:::\n\n```{.r .cell-code}\n# 99 <!--  CI -->\nt.star <- qt(.995, df = 29); t.star\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.756386\n```\n:::\n\n```{.r .cell-code}\nME <- t.star *  1.9 / sqrt(30); ME\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9561653\n```\n:::\n\n```{.r .cell-code}\n10.2 + c(-1,1) * ME\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  9.243835 11.156165\n```\n:::\n\n```{.r .cell-code}\n# standard uncertainty = SE = s / sqrt(n)\n1.9 / sqrt(30)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.346891\n```\n:::\n:::\n\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-linear-thermal-expansion}\n\nA handbook gives the value of the coefficient of linear thermal expansion of \npure copper at 20 degrees C, $\\alpha_{20}$(Cu), as \n$16.52 \\times 10^{-6} \\ {}^\\circ C^{-1}$\nand simply states that \"the error in this value should not exceed \n$0.40 \\times 10^{-6} \\ {}^{\\circ} C^{-1}$.\"\n:::: {.enumerate}\n\n#. \nBased on this limited information, and assuming a rectangular distribution,\ncompute the standard uncertainty.\n#. \nBased on this limited information, and assuming a triangular distribution,\ncompute the standard uncertainty.\n#. \nWhy might one prefer one of these over the other?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# rectangualr: sd = width / sqrt(12) = width / (2 * sqrt(3))\n0.4 * 2 / sqrt(12)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2309401\n```\n:::\n\n```{.r .cell-code}\n0.4 / sqrt(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2309401\n```\n:::\n\n```{.r .cell-code}\n# triangular: sd = width / (2 * sqrt(6))\n0.4 * 2 / (2 * sqrt(6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1632993\n```\n:::\n\n```{.r .cell-code}\n0.4 / sqrt(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1632993\n```\n:::\n:::\n\n\n\n\nThe uniform distribution is a more conservative assumption.  The trianlge distribution should only\nbe used in situations where errors are more likely to be smaller than larger \n(with the the specified bounds).\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-standard-solution}\n\nThe following data are given in in the certificate of a standard solution: \nC(HCl) = ($0.10000 \\pm 0.00010$) mol/l. \nNo additional information is given on the type of the uncertainty. \n(The $\\pm$ part here is not the uncertainty but is supposed to indicate\nupper and lower bounds on  the error.)\n:::: {.enumerate}\n\n#. \nConvert the uncertainty to standard uncertainty assuming a\nrectangular distribution.\n#. \nConvert the uncertainty to standard uncertainty assuming a\ntriangular distribution.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# rectangular\n0.0001 / sqrt(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.773503e-05\n```\n:::\n\n```{.r .cell-code}\n# trianglular\n0.0001 / sqrt(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.082483e-05\n```\n:::\n:::\n\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-uncertainty-circle}\n\nThe area of a circle is to be calculated from a measured radius.  The measurement and its\nstandard uncertainty are reported as \n$$\n12.5 \\pm 0.3 \\mbox{m} \\;.\n$$\nWhat should the researchers report as the area?\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nLet $f(R) = \\pi R^2$.  Then $\\Partial{f}{R} = 2\\pi R$, so the uncertainty in the area estimation\nis \n$$\n\\sqrt{ (2 \\pi R)^2 (0.3)^2 } = 2 \\pi R \\cdot 0.3  = 23.5619449 \n\\mathrm{m}^2\n\\;.\n$$\nWe can report the area as \n$490 \\pm 20$.\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-reporting-digits}\n\n\tBelow are some computer-computed estimates and uncertainties.  They are far too precise (there are way too\n\tmany digits reported).  Use standard practice to report each estimate with the correct number of digits.\n\n| estimate   | uncertainty    |\n|------------|----------------|\n|\t5.43210    | 0.024135       |\n|\t1535.68    | 12.7342        |\n|\t576.3415   | 3.453567       |\n|\t0.00148932 | 0.0000278      |\n\t\t\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n| estimate   | uncertainty    |\n|------------|----------------|\n|\t5.43       | 0.02           |\n|\t1536       | 13             |\n|\t576        | 3              |\n|\t0.00149    | 0.00003        |\n\t\t\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-volume-of-tank}\n\n\tA student is calculating the volume of a rectangular tank by measuring \n\tthe length, width, and height.  These measurements are recorded as \n\t $L = 2.65 \\pm 0.02$cm, $W = 3.10 \\pm 0.02$cm, and $H = 4.61\\pm 0.05$ cm.\n\n\t How should the volume be reported?\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\tIn a product the relative uncertainties add, so\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL <- 2.65; W <- 3.10; H <- 4.61\nuL <- 0.02; uW <- 0.02; uH <- 0.05\nV <- L * W * H; V\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 37.87115\n```\n:::\n\n```{.r .cell-code}\nuV <- sqrt( (uL/L)^2 + (uW/W)^2 + (uH/H)^2) * V; uV\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5568714\n```\n:::\n:::\n\n\n\n\tSo we report $37.9 \\pm 0.6$\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-gasoline-consumption}\n\nEstimate (with uncertainty) \nthe amount of gasoline burned by personal cars in a particular\nyear in the US from the following estimates and uncertainties for that\nyear:\n\n\t<!--  note: estimates found online for 2011.  uncertainties made  -->\n\t<!--  up but roughly based on the number of digits reported in online sources. -->\n\t\n\t\n| quantity                                    | estimate | uncertainty |\n|---------------------------------------------|----------|-------------|\n|\tcars per person                             | 0.80     | 0.12        |\n| population (millions of people)             | 311.6    | .2          |\n|\tfleet fuel efficiency (mpg)                 | 23.7     | 1.7         |\n| average distance driven per vehicle (miles) | 12,000   | 2,000       |\n\n\nNote:  Various estimates for these quantities are available online, but \n\tmost do not report uncertainty.  The uncertainties here reflect the \n\tnumber of significant figures used to report these numbers and the \n\tvariability between estimates found at different web sites.\n\tAlso, the methodology for determining these values is not always\n\tclear.  So treat this as an exercise in propagation of error, but \n\tunderstand that better estimates of fuel consumption (and uncertainty)\n\twould be possible with better data.\n::: \n<!-- end problem -->\n\n\n\n::: {.solution}\n\nThe total fuel consumed is given by\n$$\nF = \\frac{kPd}{E}\n$$\nwhere $k$ is the cars per person, $P$ is total population, $d$ is average distance \ndriven per car, and $E$ is the average efficiency.\n\n$$\n\\begin{aligned}\n\\Partial{F}{k} &= \\frac{Pd}{E} \n\\\\\n\\Partial{F}{P} &= \\frac{kd}{E}\n\\\\\n\\Partial{F}{d} &= \\frac{kP}{E}\n\\\\\n\\Partial{F}{E} &= \\frac{-kPd}{E^2}\n\\end{aligned}\n$$\n\nSo the estimated amount of fuel consumed (in millions of gallons) is \n$$\n\\frac{kPd}{E} = \\frac{0.8 \\cdot 311.6 \\cdot 12000 }{23.7}\n= 1.2621772\\times 10^{5}\n$$\nand the uncertainty can be computed as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 0.8; u_k <- 0.12\nP <- 311.6; u_P <- 0.2\nd <- 12000; u_d <- 2000\nE <- 23.7; u_E <- 1.7\nvariances <- c(\n            (P*d/E)^2 * u_k^2 , \n            (k*d/E)^2 * u_P^2 , \n            (k*P/E)^2 * u_d^2 , \n            (k * P * d/E^2)^2 * u_E^2\n)\nvariances\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.584455e+08 6.563051e+03 4.425254e+08 8.196753e+07\n```\n:::\n\n```{.r .cell-code}\nu_F <- \n  sqrt( \n            (P*d/E)^2 * u_k^2 + \n            (k*d/E)^2 * u_P^2 + \n            (k*P/E)^2 * u_d^2 + \n    (k * P * d/E^2)^2 * u_E^2\n    )\nu_F\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 29714.39\n```\n:::\n\n```{.r .cell-code}\nsqrt(sum(variances))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 29714.39\n```\n:::\n:::\n\n\n\n\nSo we might report fuel consumption as \n1.3\\times 10^{5} $\\pm$ \n3\\times 10^{4} millions of gallons or \n130 $\\pm$ \n30 billions of gallons.\n\nNote: This problem can also be done in stages.  First we estimate the number \nof (millions of) cars using\n$$\nC = k P \\;.\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC <- k * P; C\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 249.28\n```\n:::\n\n```{.r .cell-code}\nu_C <- sqrt( P^2 * u_k^2 + k^2 * u_P^2); u_C\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 37.39234\n```\n:::\n:::\n\n\n\n\nThe number of (millions of) miles driven is estimated by $M = C d$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM <- C * d; M\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2991360\n```\n:::\n\n```{.r .cell-code}\nu_M <- sqrt( d^2 * u_C^2 + C^2 * u_d^2); u_M\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 670746.6\n```\n:::\n:::\n\n\n\n\nFinally, the fuel used is $F = M / E$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nF = M / E\nu_F = sqrt( (1/E)^2 * u_M^2 + (M/E^2)^2 * u_E^2); u_F\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 29714.39\n```\n:::\n:::\n\n\n\n\nThis gives the same result (again in millions of gallons of gasoline).\n\nIt would also be possible to relative uncertainty to do this problem, since \nwe have nice formulas for the relative uncertainty in products and quotients.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-speed-propagation}\n\nA physics student is calculating the speed of a falling object by measuring the time\n\tit takes for the object to move between two timing sensors.\n\tIf she records the time as $0.43 \\pm 0.02$ seconds and the distance as \n\t$1.637 \\pm 0.006$ m, how should she report the speed in $m/s$?\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\tSee the next problem for the answer.\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-relative-uncertainty-quotient}\n\n:::: {.enumerate}\n\n#. Work out a formula for the relative uncertainty of $Q = X/Y$ given\n\t\t\trelative uncertainties for $X$ and $Y$.\n#. Redo @exr-speed-propagation using your new formula.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\tLet $Q = X Y^{-1}$.  So \n\t$\\Partial{Q}{X} = Y^{-1}$ and \n\t$\\Partial{Q}{Y} = -X Y^{-2}$.  From this we get\n$$\n\\begin{aligned}\n\\frac{u_Q}{Q} \n& = \\sqrt{ \\frac{ Y^{-2} u_X^2 + X^2 Y^{-4} u_Y^2}{Q^2} }\n\\\\\n& = \\sqrt{ \\frac{ Y^{-2} u_X^2 + X^2 Y^{-4} u_Y^2}{X^2Y^{-2}} }\n\\\\\n& = \\sqrt{ \\frac{ u_X^2}{X^2} + \\frac{u_Y^2}{Y^2} }\n\\\\\n& = \\sqrt{ \\left(\\frac{ u_X}{X}\\right)^2 + \\left(\\frac{u_Y}{Y}\\right)^2 }\n\\end{aligned}\n$$\n\nwhich gives a Pythagorean identity for the relative uncertainties just as it \ndid for a product.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nV <- 1.637 / 0.43; V\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.806977\n```\n:::\n\n```{.r .cell-code}\nuV <- sqrt( (0.02/.43)^2 + (0.006/1.637)^2 ) * V; uV\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1776176\n```\n:::\n:::\n\n\n\nSo we report\n$3.81 \\pm 0.18$.\n::: \n<!-- end solution -->\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}