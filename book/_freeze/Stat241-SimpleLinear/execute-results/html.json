{
  "hash": "f1f8a29103edac953bcca9e88f92ab50",
  "result": {
    "markdown": "\n# Linear Models\n\n::: {.hidden}\n$$\n\\def\\reals{\\mathbb{R}}\n\\def\\card#1{| #1 |}\n\\def\\mean{\\overline}\n\\def\\evProb#1{\\operatorname{Prob}(\\mbox{#1})}\n\\def\\Prob{\\operatorname{Prob}}\n\\def\\tand{\\operatorname{and}}\n\\def\\tor{\\operatorname{or}}\n\\def\\tnot{\\operatorname{not}}\n\\def\\E{{\\operatorname E}}\n\\def\\Var{\\operatorname{Var}}\n\\def\\SD{\\operatorname{SD}}\n\\def\\union{\\cup}\n\\def\\intersect{\\cap}\n\\def\\Unif{{\\sf Unif}}\n\\def\\Rayliegh{{\\sf Rayliegh}}\n\\def\\Exp{{\\sf Exp}}\n\\def\\Norm{{\\sf Norm}}\n\\def\\Tri{{\\sf Tri}}\n\\def\\Gamm{{\\sf Gamma}}\n\\def\\Weibull{{\\sf Weibull}}\n\\def\\boolval#1{[\\![ #1 ]\\!]}\n$$\n:::\n\n\n\n\n\nIn @sec-propagation we learned how to estimate one quantity based\non its (known) relationship to other quantities.  For example, we estimated the\nnumber of dimes in a sack of dimes from our estimates of the weight of the\ndimes and the average weight of a dime.  \n\nIn this chapter we will explore how to use data to determine the relationship among\ntwo or more variables when this relationship is not known in advance.  The general\nframework we will use is \n\n$$\nY = f(x_1, x_2, \\dots, x_k) + \\varepsilon\n$$\n\n::: {.itemize}\n\n*  $Y$ is the **response** variable that we are trying to estimate\n\t\tfrom $k$ **explanatory** or **predictor** variables $x_1, x_2, \\dots, x_k$.\n*  The relationship between the explanatory variables and the response \n\t\tvariables is described by a function $f$.\n*  The relationship described by $f$ need not be a perfect fit.  The **error**\nterm in the model, $\\varepsilon$, describes how individual responses\ndiffer from the value given by $f$.  \n\t\t\n    We will model $\\varepsilon$ with a \n\t\tdistribution -- typically a distribution with a mean of 0 -- \n\t\tso another way to think about this model is the for a given \n\t\tvalues of the predictors, the values of $Y$ have a distribution.  The mean\n\t\tof this distribution is specified by $f$ and the shape by $\\varepsilon$.\n::: \n<!-- end itemize -->\n\n\n\n## The Simple Linear Regression Model\n\n$$\nY = \\beta_0 + \\beta_1 x + \\varepsilon  \\qquad \\mbox{where $\\varepsilon \\sim \\Norm(0,\\sigma)$.}\n$$\n\nIn other words:\n\n::: {.enumerate}\n\n1.  The mean response for a given predictor value $x$ is given by a linear formula\n$$\n\\mbox{mean response} = \\beta_0 + \\beta_1 x\n$$\n\n    This can also be written as \n$$\n\\E(Y \\mid X = x) = \\beta_0 + \\beta_1 x\n$$\n\n2. The distribution of all responses for a given predictor value $x$ is normal.\n\n3. The standard deviation of the responses is the same for each predictor value.\n::: \n<!-- end enumerate -->\n\nFurthermore, in this model the values of $\\varepsilon$ are independent.\n\nThere are many different things we might want to do with a linear model, for example:\n\n::: {.itemize}\n\n*  Estimate the coefficients $\\beta_0$ and $\\beta_1$.\n*  Estimate the value $Y$ associated with a particular value of $x$.\n*  Say something about how well a line fits the data.\n::: \n<!-- end itemize -->\n\n\n## Fitting the Simple Linear Model\n\n### The Least Squares Method\n\nWe want to determine the best fitting line to the data.  The usual method is \nthe method of least squares^[In this case, it turns out that the least \nsquares and maximum likelihood methods produce exactly the same results.],\nwhich chooses the line that has the \n* smallest possible sum of squares of residuals*, where residuals are defined by\n\n$$\n\\mbox{residual} = \\mbox{observed} - \\mbox{predicted}\n$$\n\n::: {.example}\n\nConsider the following small data set.\n\n:::{.columns}\n\n::::{.columnn width=\"50<!-- \"} -->\n\n::: {.cell}\n\n```{.r .cell-code}\nSomeData <- tibble(\n  x = c(1,2,4,5,6),\n  y = c(1,4,3,6,5)\n)\nSomeData\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"x\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"y\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\"},{\"1\":\"2\",\"2\":\"4\"},{\"1\":\"4\",\"2\":\"3\"},{\"1\":\"5\",\"2\":\"6\"},{\"1\":\"6\",\"2\":\"5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n::::\n::::{.columnn width=\"50<!-- \"} -->\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point( y ~ x, data = SomeData) |>\n  gf_lims(y = c(0, 7), x = c(0, 7))\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-3-1.png){width=432}\n:::\n:::\n\n\n::::\n:::\n\n\n\n\n\n\n::: {.enumerate}\n\n#.  Add a line to the plot that \"fits the data well\".  Don't do any calculations,\n\t\tjust add the line.\n#.  Now estimate the residuals for each point relative to your line\n#.  Compute the sum of the squared residuals, $SSE$.\n#.  Estimate the slope and intercept of your line.\n::: \n<!-- end enumerate -->\n\n\n\n{{< pagebreak >}}\n\n\n\nFor example, suppose we we select a line that passes through $(1,2)$ and $(6,6)$. \nthe equation for this line is $y = 1.2 + 0.8 x$, and it looks like a pretty good fit:\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun( 1.2 + 0.8 * x ~ x)\ngf_point(y ~ x, data = SomeData) |> \n  gf_lims(x = c(0, 7), y = c(0, 7)) |>\n  gf_fun( f(x) ~ x, col = \"gray50\" )\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-5-1.png){width=432}\n:::\n:::\n\n\nThe residuals for this function are \n\n::: {.cell}\n\n```{.r .cell-code}\nresids <- with(SomeData, y - f(x)) ; resids \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -1.0  1.2 -1.4  0.8 -1.0\n```\n:::\n:::\n\n\nand $SSE$ is \n\n::: {.cell}\n\n```{.r .cell-code}\nsum(resids^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.04\n```\n:::\n:::\n\n\nThe following plot provides a way to visualize the sum of the squared residuals (SSE).\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-8-1.png){width=432}\n:::\n:::\n\n\n\nIf your line is a good fit, then $SSE$ will be small.  \nThe best fitting line will have the smallest possible $SSE$.   \nThe `lm()()` function will find this best fitting line for us.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- lm( y ~ x, data = SomeData ); model1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = SomeData)\n\nCoefficients:\n(Intercept)            x  \n     1.1628       0.7326  \n```\n:::\n:::\n\n\nThis says that the equation of the best fit line is \n$$\n\\hat y = 1.1627907 + 0.7325581 x\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(y ~ x, data = SomeData) |>\n  gf_lm()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\nâ„¹ Please use the `linewidth` aesthetic instead.\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-10-1.png){width=432}\n:::\n:::\n\n\nWe can compute $SSE$ using the `resid()()` function.\n\n::: {.cell}\n\n```{.r .cell-code}\nSSE <- sum (resid(model1)^2); SSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.569767\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-12-1.png){width=432}\n:::\n:::\n\n\nAs we see, this is a better fit than our first attempt -- \nat least according to the least squares criterion.\nIt will better than *any* other attempt -- it is the least \nsquares regression line.\n::: \n<!-- end example -->\n\n\n\\subsection{Properties of the Least Squares Regression Line}\nFor a line with equation $y = \\hat\\beta_0 + \\hat\\beta_1 x$, the residuals are \n$$\ne_i = y_i - (\\hat\\beta_0 + \\hat\\beta_1 x) \n$$\nand the sum of the squares of the residuals is \n$$\nSSE = \\sum e_i^2  = \\sum (y_i - (\\hat\\beta_0 + \\hat\\beta_1 x) )^2\n$$\nSimple calculus (which we won't do here) allows us to compute the \nbest $\\hat\\beta_0$ and $\\hat\\beta_1$ possible.  \nThese best values define the least squares regression line.\nWe always compute these values using software, but it is good to note that \nthe least squares line satisfies two very nice properties.\n\n::: {.enumerate}\n\n#. \nThe point $(\\mean x, \\mean y)$ is on the line. \n\nThis means that $\\mean y = \\hat\\beta_0 + \\hat\\beta_1 \\mean x$  (and $\\hat\\beta_0 = \\mean y - \\hat\\beta_1 \\mean x$)\n#. \nThe slope of the line is $\\displaystyle b = r \\frac{s_y}{s_x}$ where $r$ is the \n**correlation coefficient**:\n\\myindex{correlation coefficient}<!--  -->\n$$\nr = \\frac{1}{n-1} \\sum \\frac{ x_i - \\mean x }{s_x} \\cdot \\frac{ y_i - \\mean y }{s_y}\n$$\n::: \n<!-- end enumerate -->\n\nSince we have a point and the slope, it is easy to compute the equation for the line\nif we know $\\mean x$, $s_x$, $\\mean y$, $s_y$, and $r$.\n\n### An Example: Estimating OSA\n\n::: {.example}\n\nIn a study of eye strain caused by visual display terminals, researchers wanted\nto be able to estimate ocular surface area (OSA) from palpebral fissure (the\nhorizontal width of the eye opening in cm) because palpebral fissue is easier\nto measure than OSA.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEyes <- \n  read.table(\n    \"https://rpruim.github.io/Engineering-Statistics/data/PalpebralFissure.txt\", \n    header = TRUE)\nhead(Eyes, 3) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"palpebral\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"OSA\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.40\",\"2\":\"1.02\",\"_rn_\":\"1\"},{\"1\":\"0.42\",\"2\":\"1.21\",\"_rn_\":\"2\"},{\"1\":\"0.48\",\"2\":\"0.88\",\"_rn_\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nx.bar <- mean( ~ palpebral, data = Eyes)  \ny.bar <- mean( ~ OSA, data = Eyes)\ns_x <- sd( ~ palpebral, data = Eyes)\ns_y <- sd( ~ OSA, data = Eyes)\nr <- cor( palpebral ~ OSA, data = Eyes)\nc( x.bar = x.bar, y.bar = y.bar, s_x = s_x, s_y = s_y, r = r )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    x.bar     y.bar       s_x       s_y         r \n1.0513333 2.8403333 0.3798160 1.2083374 0.9681245 \n```\n:::\n\n```{.r .cell-code}\nslope <- r * s_y / s_x\nintercept <- y.bar - slope * x.bar\nc(intercept = intercept, slope = slope)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n intercept      slope \n-0.3977389  3.0799672 \n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\nFortunately, statistical software packages do all this work for us, so the\ncalculations of the preceding example don't need to be done in practice.\n\n::: {.example}\n\nIn a study of eye strain caused by visual display terminals, researchers wanted\nto be able to estimate ocular surface area (OSA) from palpebral fissure (the\nhorizontal width of the eye opening in cm) because palpebral fissue is easier\nto measure than OSA.\n\n::: {.cell}\n\n```{.r .cell-code}\nosa.model <- lm(OSA ~ palpebral, data = Eyes) \nosa.model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = OSA ~ palpebral, data = Eyes)\n\nCoefficients:\n(Intercept)    palpebral  \n    -0.3977       3.0800  \n```\n:::\n:::\n\n\n`lm()()` stands for linear model.  The default output includes the estimates\nof the coefficients ($\\hat\\beta_0$ and $\\hat \\beta_1$) based on the data.  If that is the \nonly information we want, then we can use \n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(osa.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)   palpebral \n -0.3977389   3.0799672 \n```\n:::\n:::\n\n\n\nThis means that the equation of the least squares regression line is \n$$\n\\hat y = -0.398 + 3.08 x\n$$\n\nWe use $\\hat y$ to indicate that this is not an observed value of the response variable\nbut an estimated value (based on the linear equation given).\n\nR\\ can add a regression line to our scatter plot if we ask it to.\n\n::: {.center}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point( OSA ~ palpebral, data = Eyes) |> \n  gf_lm()\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/osa-scatter-1.png){width=432}\n:::\n:::\n\n\n::: \n<!-- end center -->\n\n\nWe see that the line does run roughly \"through the middle\" of the data but that\nthere is some variability above and below the line.\n::: \n<!-- end example -->\n\n\n### Explanatory and Response Variables Matter\nIt is important that \nthe explanatory variable be the \"`x`\" variable\nand the response variable be the \"`y`\" variable\nwhen doing regression.  If we reverse the roles of `OSA` and \n`palpebral`\nwe do not get the same model.  This is because the residuals are measured vertically \n(in the $y$ direction).  \n\n\n## Estimating the Response\n\nWe can use our least squares regression line to estimate the value of the response\nvariable from the value of the explanatory variable.\n\n::: {.example}\n\nIf the palpebral width is 1.2 cm, then we would estimate OSA to be \n\n$$\n\\hat{`osa`} = \n-0.398 + 3.08 \\cdot 1.2 \n= 3.298 \n$$\n\nR\\ can automate this for us too.  The `makeFun()()` function will\ncreate a function from our model.  \nIf we input a palpebral measurement into this function, the function\nwill return the estimated OSA.\n\n::: {.cell}\n\n```{.r .cell-code}\nestimated.osa <- makeFun(osa.model)\nestimated.osa(1.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n3.298222 \n```\n:::\n:::\n\n\n\nAs it turns out, the 17th measurement in our data set had a\n`palpebral` measurement of 1.2 cm.\n\n::: {.cell}\n\n```{.r .cell-code}\nEyes[17,]\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"palpebral\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"OSA\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1.2\",\"2\":\"3.76\",\"_rn_\":\"17\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThe corresponding OSA of 3.76 means that the residual for this observation is \n$$\n\\mbox{observed} - \\mbox{predicted} = 3.76 - 3.2982218 = \n0.4617782  \n$$\n::: \n<!-- end example -->\n\n\n### Cautionary Note: Don't Extrapolate\\!\nWhile it often makes sense to generate model predictions corresponding to x-values *within* the range of values measured in the dataset, it is dangerous to *extrapolate* and make predictions for values *outside* the range included in the dataset.  To assume that the linear relationship observed in the dataset holds for explanatory variable values outside the observed range, we would need a convincing, valid justification, which is usually not available.  If we extrapolate anyway, we risk generating erroneous or even nonsense predictions.  The problem generally gets worse as we stray further from the observed range of explanatory-variable values.\n\n## Parameter Estimates\n### Interpreting the Coefficients\nThe coefficients of the linear model tell us how to construct the linear function\nthat we use to estimate response values, but they can be interesting in their own\nright as well.\n\nThe intercept $\\beta_0$ is the mean response value when the \nexplanatory variable is 0.  This may or may not be interesting.\nOften $\\beta_0$ is not interesting because we are not interested\nin the value of the response variable when the predictor is 0.  (That might not \neven be a possible value for the predictor.)  Furthermore, \nif we do not collect data with values of the explanatory variable near 0, then\nwe will be extrapolating from our data when we talk about the intercept. (Extrapolating is dangerous because we can't really be sure that the relationships we've uncovered with our model really hold for variable values outside the range we measured.)\n\nThe estimate for $\\beta_1$, on the other hand, is nearly always of interest.\nThe slope coefficient $\\beta_1$ tells us how quickly the response variable changes \nper unit change in the predictor.  This is an interesting value in many more situations.\nFurthermore, when $\\beta_1 = 0$, then our model does not depend on the predictor at all.\nSo if we construct a confidence interval for $\\beta_1$, and it contains 0, then we do *not* have sufficient evidence to be convinced that our predictor is of any use in predicting the response.\n\n### Estimating $\\sigma$\n\nThere is one more parameter in our model that we have been mostly ignoring so far: $\\sigma$ (or \nequivalently $\\sigma^2$).  This is the parameter that describes how tightly things should \ncluster around the regression line.  We can estimate $\\sigma^2$ from our residuals:\n\n$$\n\\begin{aligned}\n\\hat\\sigma^2 & = MSE = \\frac{ \\sum_i e_i^2 }{ n -2 }\n\\\\\n\\hat\\sigma & = RMSE = \\sqrt{MSE} = \\sqrt{\\frac{ \\sum_i e_i^2 }{ n -2 } }\n\\end{aligned}\n$$\n\nThe acronyms $MSE$ and $RMSE$ stand for **Mean Squared Error** and \n**Root Mean Squared Error**.\nThe numerator in these expressions is the sum of the squares of the residuals\n$$\nSSE = \\sum_i e_i^2 \\;.\n$$\nThis is precisely the quantity that we were minimizing to get our least squares fit.\n$$\nMSE = \\frac{SSE}{DFE} \n$$\nwhere $DFE = n-2$ is the **degrees of freedom** associated with the\nestimation of $\\sigma^2$ in a simple linear model.  We lose two degrees of\nfreedom when we estimate $\\beta_0$ and $\\beta_1$, just like we lost 1 degree of\nfreedom when we had to estimate $\\mu$ in order to compute a sample variance.\n\n$RMSE = \\sqrt{MSE}$ is listed in the summary output for the linear model as the\n**residual standard error** because it is the estimated standard deviation of \nthe error terms in the model.\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(osa.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = OSA ~ palpebral, data = Eyes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60942 -0.19875 -0.01902  0.21727  0.66378 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.3977     0.1680  -2.367   0.0251 *  \npalpebral     3.0800     0.1506  20.453   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.308 on 28 degrees of freedom\nMultiple R-squared:  0.9373,\tAdjusted R-squared:  0.935 \nF-statistic: 418.3 on 1 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nWe will learn about other parts of this summary output shortly.\nMuch is known about the estimator $\\sigma^2$, including \n\n::: {.itemize}\n\n*  $\\hat \\sigma^2$ is unbiased (on average it is $\\sigma^2$), and \n*  the sampling distribution is related to a Chi-Squared distribution with\n\t\t$n-2$ degrees of freedom.  \n\t\t(Chi-Squared distributions are a special case of Gamma distributions.)\n\t\tMore specifically,\n$$\n\t\t\\frac{SSE}{\\sigma^2} = \\frac{(n - 2) \\hat \\sigma^2}{\\sigma^2} \\sim \\Chisq(n-2) \\;.\n$$\n::: \n<!-- end itemize -->\n\n\n\n## Checking Assumptions\n### What have we assumed?\nIn fitting a linear regression model, we have assumed:\n\n::: {.itemize}\n\n*  A linear relationship between the explanatory and response variables\n*  The errors ($\\epsilon$) are Normally distributed\n*  Independence of the errors (in particular, no correlation over time between successive errors for data points collected over time)\n*  Homoscedasticity of the errors -- this means that the variance (spread) of the errors is constant over time, and over the full range of explanatory and predictor variables\n::: \n<!-- end itemize -->\n\n\n### Don't Fit a Line If a Line Doesn't Fit\n\nThe least squares method can be used to fit a line to any data -- even if a line\nis not a useful representation of the relationship between the variables.\nWhen doing regression we should always look at the data to see if a line \nis a good fit.  If it is not, then the simple linear model is not a good choice and \nwe should look for some other model that does a better job of describing the \nrelationship between our two variables.  \n\n### Checking the Residuals\nWe look at the residuals (not just the data scatter plot) because some of our assumptions refer specifically to them.  Also, often, it is easier to assess the linear fit by looking at a plot of the \nresiduals than by looking at the natural scatter plot, because on the scale \nof the residuals, violations of our assumptions are easier to see.\n\nSo, to verify that our linear regression assumptions are sensible, we can examine the model residuals. Residuals should be checked to see that their distribution looks approximately\nnormal and that thier standard deviation (the spread of the residuals) remains consistent across the range of our data (and across time).\n\nIn addition, especially if the data were collected over time (measurements made in order during an experiment; data points collected at a series of time points), it is important to verify that the residuals are *independent* of one another over time.  To look for this problem, we can look at a scatter plot of the residuals as a function of time, and suspect a problem if we see series of very large, or very small, residuals all in a row.  Another plot that can help us look for non-independence in the residuals is a plot of the autocorrelation function (ACF), obtained using the `acf()` function in R.  This function computes and plots the correlation coefficient R for the residuals at various \"lags\".  For example, the correlation coefficient for lag 1 is the correlation coefficient between each residual (corresponding to the *i*th datapoint) and the preceding one (the *i-1*th data point).  Lag 2 is between the *i*th and *i-2*th data point, and so on.  If the residuals are not independent, then these coefficients will have large absolute values.  (Note: the \"lag 0\" coefficient measures the correlation of the *i*th residual with itself, so it is always 1.  This does NOT indicate any problem with the linear regression model.)\n\nIn general, we might want to check the following plots of the residuals:\n\n::: {.itemize}\n\n*  Residuals as a function of \"fitted values\", or model predictions for the x-values obseved in the actual data set.\n*  Residuals as a function of the observed values of the explanatory variable (from the actual data set).\n*  Normal quantile-quantile plot of the residuals (note: in Chapter 4, we made these by hand for any distribution; you may also use the shortcut function qqmath() as illustrated below, to make them for the Normal distribution.)\n*  Residuals as a function of time (if you know the order in which they were collected, or if the explanatory variable is a time-related one).\n*  Residual autocorrelation function plot (if the data points were collected over time, or if the explanatory variable is a time-related one).\n::: \n<!-- end itemize -->\n\n\nFor all the scatter plots, we want to make sure the residuals \"look random\" -- the extent of the spread of the residuals should not vary with time or x or y (``trumpet\"-shaped plot).  If there is a pattern, it suggests a problem with the homoscedasticity assumption.  There should not be long runs of similar residuals, especially over time; if there are, it suggests non-independence of the residuals.  There should also be no apparent trends in the plot, linear or non-linear; if there are, it suggests that the relationship between the predictor and response variables was not linear.  In an autocorrelation plot, the correlation coefficients (except for lag 0) should not be too large, far exceeding the dotted guide-lines on the plot; if they are, there is probably a problem with the independence assumption.\n\n::: {.example}\n\nReturning to our OSA data, we can obtain the residuals using the \n`resid()()` function and plot them.\n\n::: {.cell}\n\n```{.r .cell-code}\nosa.hat <- makeFun(osa.model)\npreds <- osa.hat(Eyes$palpebral)\ngf_point( resid(osa.model) ~ preds, data = Eyes,\n        title =\"Residuals versus fitted values\")\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-20-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ resid(osa.model),  data = Eyes, title = \"Normal QQ plot\")\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-20-2.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(resid(osa.model) ~ palpebral, data = Eyes,\n        title = \"Residuals versus explanatory variable\")\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-20-3.png){width=432}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nacf(resid(osa.model), ylab = \"Residual ACF\", main = \"\")\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/acfplot-1.png){width=576}\n:::\n:::\n\n\n\n::: \n<!-- end example -->\n\nIf the assumptions of the model are correct, there should be no distinct patterns to these scatter\nplots of the residuals, and the normal-quantile plot should be roughly linear\n(since the model says that differences between observed responses and the true linear fit\nshould be random noise following a normal distribution with constant standard deviation).\n\nIn this case things look pretty good. \n\nWe can save ourselves a little typing if we create these plots using \n`plot()()` or `mplot()()`.\n\n::: {.cell}\n\n```{.r .cell-code}\nmplot(osa.model, w = 1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-21-1.png){width=432}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[2]]\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-21-2.png){width=432}\n:::\n:::\n\n\n\nThe \"standardized\" residuals are the residuals that have been adjusted to\nhave an expected variance (and hence also standard deviation) of 1.  Roughly,\nthey are the residuals divided by $\\hat \\sigma$, but there is an additional \nadjustment that is made as well.  This gives us a kind of unitless version\nof the residual and the normal-quantile plot using standardized residuals does a \nbetter job of indicating whether the normality assumption is compatibly with\nthe data.  (Remember, the normality assumption is about the *errors* not \nthe residuals.  The standardized residuals should behave roughly like a normal\ndistribution if the errors are normally distributed.)  Typically the shape of\nthe normal-quantile plot made with raw residuals and the one made with standarized\nresiduals will be very similar.\n\n### Outliers in Regression\n\nOutliers can be very influential in regression, especially in small data sets,\nand especially if they occur for extreme values of the explanatory variable.\nOutliers cannot be removed just because we don't like them, but they should be\nexplored to see what is going on (data entry error? special case? etc.)\n\nSome researchers will do \"leave-one-out\" analysis, or \"leave some out\" analysis\nwhere they refit the regression with each data point left out once.  If the regression summary changes very little when we do this, this means that the regression line\nis summarizing information that is shared among all the points relatively equally.\nBut if removing one or a small number of values makes a dramatic change, then\nwe know that that point is exerting a lot of influence over the resulting\nanalysis (a cause for caution).   \n\nThis kind of analysis can be very helpful, especially if you have one or several large potential outliers in your data set, but in this class, we will not generally do it as a matter of course (it's not a required part of model assessment for coursework).\n\n## How Good Are Our Estimates?\nAssuming our diagnostics indicate that fitting a linear model is reasonable for our data,\nour next question is *How good are our estimates?*\nNotice that there are several things we have estimated:\n\n::: {.itemize}\n\n*  The intercept coefficient $\\beta_0$  [estimate: $\\hat \\beta_0$]\n*  The slope coefficient $\\beta_1$  [estimate: $\\hat \\beta_1$]\n*  Values of $y$ for given values of $x$.  [estimate: $\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x$]\n::: \n<!-- end itemize -->\n\n\nWe would like to be able to compute uncertainties and confidence intervals for these.\nFortunately, R\\ makes this straightforward.\n\n### Estimating the $\\beta$s\n\n::: {.example}\n\n**Q.** \nReturning to the OSA data, compute standard uncertainties and 95% confidence intervals\nfor $\\beta_0$ and $\\beta_1$.\n\n**A.** \nThe `summary()()` function provides additional information about the \nmodel:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(osa.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = OSA ~ palpebral, data = Eyes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60942 -0.19875 -0.01902  0.21727  0.66378 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.3977     0.1680  -2.367   0.0251 *  \npalpebral     3.0800     0.1506  20.453   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.308 on 28 degrees of freedom\nMultiple R-squared:  0.9373,\tAdjusted R-squared:  0.935 \nF-statistic: 418.3 on 1 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\tWe don't know what to do with all of the information displayed here, but we can see\n\tsome familiar things in the coefficient table.  \nIf we only want the coefficients part of the summary output we can get that using\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(summary(osa.model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -0.3977389  0.1680090 -2.367367 2.506081e-02\npalpebral    3.0799672  0.1505882 20.452918 2.252825e-18\n```\n:::\n:::\n\n\nFrom this we see the estimates ($\\hat \\beta$'s)\ndisplayed again.  Next to each of those is a standard error.  That is the standard\nuncertainty for these estimates.  So we could report our estimated coefficients as \n$$\n\t\\beta_0: <!-- .2f \\pm  -->\n\t\t0.17\n\t\\qquad\n\t\\beta_1: <!-- .2f \\pm  -->\n\t\t0.15\n$$\n\nA confidence interval can be computed using\n$$\n\t\\hat\\beta_i \\pm t_* SE_{\\beta_i}\n$$\nbecause \n\n::: {.itemize}\n* the sampling distribution for $\\hat \\beta_i$ is normal, \n* the sampling distribution for $\\hat \\beta_i$ is unbiased (the mean is $\\beta_i$), and\n* the standard deviation of the sampling distribution depends on $\\sigma$ (and some other things), but\n*  we don't know $\\sigma$, so we have to estimate it using $RMSE = \\sqrt{MSE}$.\n::: \n<!-- end itemize -->\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.star <- qt(.975, df = 28); t.star    # n-2 degrees of freedom for simple linear regression\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.048407\n```\n:::\n\n```{.r .cell-code}\nt.star * 0.151\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3093095\n```\n:::\n:::\n\n\n\tSo a 95% confidence interval for $\\beta_1$ is \n$$\n\t3.08 \\pm 0.31\n$$\n\tThe degrees of freedom used are $DFE = n-2$, the same as used in the estimate of $\\sigma^2$.\n\t(We are using a t-distribution instead of a normal distribution because we don't know \n\t$\\sigma$.  The degrees of freedom are those associated with using $RMSE = \\sqrt{MSE}$\n\tas our estimate for $\\sigma$.)\n\n\tR\\ can compute confidence intervals for both parameters using the function\n\t`confint()()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(osa.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %      97.5 %\n(Intercept) -0.7418897 -0.05358811\npalpebral    2.7715014  3.38843310\n```\n:::\n:::\n\n\nA 68% confidence interval should have a margin of error of approximately 1 standard\nuncertainty:\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(osa.model, level = 0.68, \"palpebral\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              16 %     84 %\npalpebral 2.927507 3.232428\n```\n:::\n\n```{.r .cell-code}\n(3.2325 - 2.9275) / 2   # margin of error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1525\n```\n:::\n\n```{.r .cell-code}\ncoef(summary(osa.model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -0.3977389  0.1680090 -2.367367 2.506081e-02\npalpebral    3.0799672  0.1505882 20.452918 2.252825e-18\n```\n:::\n:::\n\n\n\n::: \n<!-- end example -->\n\n\n### Confidence and Prediction Intervals for the Response Value\nWe can also create interval estimates for the response.    R\\ will compute\nthis if we simply ask:\n\n::: {.cell}\n\n```{.r .cell-code}\nestimated.osa <- makeFun(osa.model)\nestimated.osa(1.2, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 3.298222 3.174238 3.422206\n```\n:::\n\n```{.r .cell-code}\nestimated.osa(0.8, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 2.066235 1.927384 2.205086\n```\n:::\n:::\n\n\nThese intervals are confidence intervals for the *mean* response.  Sometimes it\nis desirable to create an interval that will have a 95% chance of containing a new \n*observation* -- that is, including the anticipated error as well as the mean response.  These intervals are called **prediction intervals** to distinguish\nthem from the usual confidence interval.\n\n::: {.cell}\n\n```{.r .cell-code}\nestimated.osa <- makeFun(osa.model)\nestimated.osa(1.2, interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 3.298222 2.655228 3.941216\n```\n:::\n\n```{.r .cell-code}\nestimated.osa(0.8, interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr     upr\n1 2.066235 1.420209 2.71226\n```\n:::\n:::\n\n\nPrediction intervals are typically much wider than confidence intervals.  \nWe have to \"cast a wider net\"  to create an interval that is highly likely to contain a new \nobservation (which might be quite a bit above or below the mean).\n\nThe widths of both types of intervals depend on the value(s) of the explanatory\nvariable(s) from which we are making the estimate.  Estimates are more\nprecise near the mean of the predictor variable and become less precise as we move\naway from there.  Extrapolation beyond the observed data range is both less precise, and risky,\nbecause we don't have data to know whether the linear pattern seen in the data\nextends into that region.\n\nThe plot below illustrates both confidence (dotted) and prediction (dashed) \nintervals.\nNotice how most of the dots are within the prediction bands, but not within the \nconfidence bands.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(OSA ~ palpebral, data = Eyes) |>\n  gf_lm(interval = \"confidence\") |>\n  gf_lm(interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\nâ„¹ Please use the `linewidth` aesthetic instead.\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-29-1.png){width=.9\\textwidth}\n:::\n:::\n\n\n\n#### A Caution Regarding Prediction Intervals\nPrediction intervals are much more sensitive to the normality assumption\nthan confidence intervals are because the Central Limit Theorem does not \nhelp when we are thinking about individual observations (essentially samples of \nsize 1).  So if the true distribution of errors is not really normal, then the prediction intervals we compute using the normality assumption will not be accurate.\n\n::: {.problem #exr-rainfall}\n**Rainfall**\n\nUse the output below to answer some questions about rainfall volume  and \nrunoff volume (both in $m^3$) for a particular stretch of a Texas highway.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = runoff ~ rainfall, data = TexasHighway)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.279 -4.424  1.205  3.145  8.261 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.12830    2.36778  -0.477    0.642    \nrainfall     0.82697    0.03652  22.642  7.9e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.24 on 13 degrees of freedom\nMultiple R-squared:  0.9753,\tAdjusted R-squared:  0.9734 \nF-statistic: 512.7 on 1 and 13 DF,  p-value: 7.896e-12\n```\n:::\n:::\n\n\n::: {.enumerate}\n\n#.  How many times were rainfall and runoff recorded?\n#.  What is the equation for the least squares regression line?\n#.  Report the slope together with its standard uncertainty.\n#.  Give a 95% confidence interval for the slope of this line.\n#.  What does this slope tell you about runoff on this\n\t\t\tstretch of highway?\n#.  What is $\\hat\\sigma$?\n::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n::: {.enumerate}\n\n#.  The residual degrees of freedom is $13$, so there were $13 + 2 = 15$\n\t\t\tobservations.\n#.  $`runoff` = -1  + 0.83 \\cdot `rainfall`$\n#.  $0.83 \\pm 0.04$\n#. \n\n::: {.cell}\n\n```{.r .cell-code}\nconfint( rain.model, \"rainfall\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             2.5 %    97.5 %\nrainfall 0.7480677 0.9058786\n```\n:::\n:::\n\n\n\tWe can compute this from the information displayed:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.star <- qt( .975, df = 13 ) # 13 df listed for residual standard error\nt.star\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.160369\n```\n:::\n\n```{.r .cell-code}\nSE <- 0.0365\nME <- t.star * SE; SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0365\n```\n:::\n\n```{.r .cell-code}\n0.8270 + c(-1,1) * ME  # CI as an interval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7481465 0.9058535\n```\n:::\n:::\n\n\nWe should round this using our rounding rules (treating the margin of error\nlike an uncertainty).\n#. \n\tThe slope tells us how much additional run-off there is per additional\n\tamount of rain that falls.  Since both are in the same units ($m^3$) and\n\tsince the intercept is essentially 0, we can interpret this slope as a\n\tproportion.  Roughly 83% of the rain water is being measured as runoff. \n#. \n\t$5.24$\n::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-kids-feet}\n**Kids' feet**\n\nThe `KidsFeet` data set contains variables giving the widths and lengths\nof feet of some grade school kids.\n::: {.enumerate}\n\n#.  Perform our usual diagnostics to see whether there are any reasons\n\t\t\tto be concerned about using a simple linear model in this situation.\n#.  Based on this data, what estimate would you give for the width of a \n\t\t\tBilly's foot if Billy's foot is 24 cm long?  \n\t\t\t(Use a 95% confidence level.)\n#.  Based on this data, what estimate would you give for the average width of a \n\t\t\tkids' feet that 24 cm long?\n\t\t\t(Use a 95% confidence level.)\n::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoot.model <- lm(width ~ length, data = KidsFeet)\nmplot(foot.model, w = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-33-1.png){width=432}\n:::\n\n```{.r .cell-code}\nmplot(foot.model, w = 2)\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-33-2.png){width=432}\n:::\n:::\n\n\nOur diagnostics look pretty good.  The residuals look randomly distributed with\nsimilar amounts of variability throughout the plot.  The normal-quantile plot\nis nearly linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun(foot.model)\nf(24, interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 8.813022 7.996604 9.629441\n```\n:::\n\n```{.r .cell-code}\nf(24, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 8.813022 8.665894 8.960151\n```\n:::\n:::\n\n\nWe can't estimate Billy's foot width very accurately (between 8.0 and 9.6 cm),\nbut we can estimate the average foot width for all kids with a foot length of\n24 cm more accurately (between 8.67 and 8.96 cm).\n::: \n<!-- end solution -->\n\n\n::: {.problem exr-bike-space}\n**Space for bicycles**\n\nSome traffic engineers were interested to study interactions between bicycle and \nautomobile traffic.  One part of the study involved comparing the amount of \n\"available space\" for a bicyclist \n(distance in feet from bicycle to centerline of the roadway) and \n\"separation distance\" \n(the average distance between cyclists and passing car, also measured in feet, \ndetermined by averaging based on photography over an extend period of time).\nData were collected at 10 different sites with bicycle lanes.\nThe data are available in the `ex12.21` data set in \nthe **`Devore7`** package.\n\n::: {.enumerate}\n\n#.  Write out an equation for the least squares regression line for \n\t\t\tpredicting separation distance from available space.\n#.  Give an estimate (with uncertainty) for the slope and interpret it.\n#.  A new bicycle lane is planned for a street that has 15 feet of available\n\t\t\tspace.  Give an interval estimate for the separation distance \n\t\t\ton this new street.  Should you use a confidence interval or a prediction\n\t\t\tinterval?  Why?\n#.  Give a scenario in which you would use the other kind of interval.\n::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike.model <- lm( distance ~ space, data = Devore7::ex12.21 )\ncoef(summary(bike.model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -2.1824715 1.05668813 -2.065388 7.274847e-02\nspace        0.6603419 0.06747931  9.785841 9.974851e-06\n```\n:::\n\n```{.r .cell-code}\nf <- makeFun(bike.model)\nf( 15, interval = \"prediction\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 7.722656 6.313278 9.132035\n```\n:::\n:::\n\n\nWe would use a confidence interval to estimate the average separation distance \nfor all streets with 15 feet of available space.\n\n::: {.cell}\n\n```{.r .cell-code}\nf( 15, interval = \"confidence\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 7.722656 7.293168 8.152145\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-biometrics}\n**Biometrics**\n\nSelect only the non-diabetic men from the `pheno` data set using\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastR2)\nMen <- Pheno |> filter(sex==\"M\" & t2d==\"control\")  # note the double == and quotes here\nhead(Men, 3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"t2d\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"bmi\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sex\"],\"name\":[4],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"age\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"smoker\"],\"name\":[6],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"chol\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"waist\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"weight\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"height\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"whr\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sbp\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"dbp\"],\"name\":[13],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1012\",\"2\":\"control\",\"3\":\"30.47048\",\"4\":\"M\",\"5\":\"53.86161\",\"6\":\"former\",\"7\":\"5.02\",\"8\":\"104\",\"9\":\"94.6\",\"10\":\"176.2\",\"11\":\"0.9327354\",\"12\":\"143\",\"13\":\"89\",\"_rn_\":\"1\"},{\"1\":\"1110\",\"2\":\"control\",\"3\":\"26.75386\",\"4\":\"M\",\"5\":\"68.07944\",\"6\":\"never\",\"7\":\"5.63\",\"8\":\"99\",\"9\":\"81.0\",\"10\":\"174.0\",\"11\":\"0.9252336\",\"12\":\"162\",\"13\":\"91\",\"_rn_\":\"2\"},{\"1\":\"1146\",\"2\":\"control\",\"3\":\"NA\",\"4\":\"M\",\"5\":\"62.14521\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"_rn_\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThis data set contains some phenotype information for subjects in\na large genetics study.  You can find out more about the data set with\n\n::: {.cell}\n\n```{.r .cell-code}\n?pheno\n```\n:::\n\n\n::: {.enumerate}\n\n#.  Using this data, fit a linear model that can be used \n\t\tto predict weight from height.  What is the equation \n\t\tof the least squares regression line?\n#.  Give a 95% confidence interval for the slope of this regression\n\t\tand interpret it in context.  (Hint: what are the units?)\n#.  Give a 95% confidence interval for the mean weight of all \n\t\tnon-diabetic men who are 6 feet tall.  \n\t\t\n\t\tNote the heights are in cm and the weights are in kg, so you will need to convert \n\t\tunits to use inches and pounds.  (2.54 cm per inch, 2.2 pounds per kg)\n#.  Perform regression diagnostics.  Is there any reason to be concerned\n\t\tabout this analysis?\n::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n::: {.solution}\n\n::: {.enumerate}\n\n#. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm( weight ~ height, data = Men )\ncoef(summary(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) -69.6988584 12.14450332 -5.739128 1.551023e-08\nheight        0.8706793  0.06994051 12.448856 1.277243e-31\n```\n:::\n:::\n\n\n<!--  So the equation is  -->\n<!-- %$$ -->\n<!-- <!--  \t\t\t`weight` =  --> -->\n<!-- % \t\t\t-70 +  -->\n<!-- <!--  \t\t\t0.87 \\cdot `height` --> -->\n<!-- %$$ -->\n\n#. \n\n::: {.cell}\n\n```{.r .cell-code}\n# we can ask for just the parameter we want, if we like\nconfint(model, parm = \"height\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           2.5 %   97.5 %\nheight 0.7333052 1.008053\n```\n:::\n:::\n\n\nThe slope tells us how much the average weight (in kg) increases per \ncm of height.\n\n#. \n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun(model)\n# in kg\nf(6 * 12 * 2.54, interval = \"confidence\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 89.53098 87.97557 91.08639\n```\n:::\n\n```{.r .cell-code}\n# in pounds\nf(6 * 12 * 2.54, interval = \"confidence\") * 2.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 196.9682 193.5463 200.3901\n```\n:::\n:::\n\n\n#. \n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(resid(model) ~ fitted(model))\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-42-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ resid(model))\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-42-2.png){width=432}\n:::\n:::\n\n\nWe could also have used\n\n::: {.cell}\n\n```{.r .cell-code}\nmplot(model, which = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-43-1.png){width=432}\n:::\n\n```{.r .cell-code}\nmplot(model, which = 2)\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-43-2.png){width=432}\n:::\n:::\n\n\nThe residual plot looks fine.  There is bit of a bend to the normal-quantile plot, indicating\nthat the distribution of residuals is a bit skewed (to the right -- the heaviest men are farther above\nthe mean weight for their height than the lightest men are below).\n\nIn this particular case, a log transformation of the weights improves the\nresidual distribution.  There is still one man whose weight is quite high for\nhis height, but otherwise things look quite good.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm( log(weight) ~ height, data = Men)\ncoef(summary(model2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Estimate   Std. Error  t value     Pr(>|t|)\n(Intercept) 2.5134829 0.1448876368 17.34781 2.145773e-54\nheight      0.0108067 0.0008344117 12.95128 8.626095e-34\n```\n:::\n\n```{.r .cell-code}\ngf_point(resid(model2) ~ fitted(model2))\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-44-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ resid(model2))\n```\n\n::: {.cell-output-display}\n![](Stat241-SimpleLinear_files/figure-html/unnamed-chunk-44-2.png){width=432}\n:::\n:::\n\n\nThis model says that\n$$\n\\log( `weight` ) \n\t= 2.51 + 0.0108 \\cdot `height`\n$$\nSo\n$$\n`weight`  \n= 12.3 \n\t\t\\cdot (1.011)^{`height`}\n$$\n::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-anscombe}\n**Anscobe's data**\n\nThe `anscombe` data set contains four pairs of explanatory (`x1`, `x2`, `x3`, and `x4`)\nand response (`y1`, `y2`, `y3`, and `y4`) variables. \nThese data were constructed by Anscombe (@Anscombe:1973:Graphs).\n:::: {.enumerate}\n\n#. For each of the four pairs, us R\\ to fit a linear model and compare the results.  Use, for example,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- lm(y1 ~ x1, data = anscombe); summary(model1)\n```\n:::\n\n\n\t\t\tBriefly describe what you notice looking at this output.  (You do not have\n\t\t\tto submit the output itself -- let's save some paper.)\n#.  For each model, create a scatterplot that includes the regression line.\n\t\t\t(Make the plots fairly small and submit them.\n\t\t\tUse `fig.width` and `fig.height` or \"output options\" (the little gear icon)\n\t\t\tto control the size of the plots in RMarkdown.)\n#.  Comment on the results.  Why do you think Anscombe invented these data?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n  Anscombe's data show that it is not sufficient to look only at the \n  numerical summaries produced by regression software.  His four data\n  sets produce nearly identical output of \\verb!lm()! and \\verb!anova()!\n  yet show very different fits.  An inspection of the residuals (or even\n  simple scatterplots) quickly reveals the various difficulties.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-article}\n**Read and article**\n\nFind an article from the engineering or science literature that uses \na simple linear model and report the following information:\n:::: {.enumerate}\n\na.  Print the first page of the article (with title and abstract) and write\na full citation for the article on it.  Staple this at the end of your\nassigment.\n#.  If the article is available online, provide a URL where it can be found.\n(You can write that on the printout of the first page of the article, too.)\n#.  How large was the data set used to fit the linear model?  How do you know?  (How \ndid the authors communicate this information?)\n#.  What are the explanatory and response variables?\n#.  Did the paper give an equation for the least squares regression line \n(or the coefficients, from which you can determine the regression equation)?\nIf so, report the equation\n#.  Did the paper show a scatter plot of the data?  Was the regression line \nshown on the plot?\n#.  Did the paper provide confidence intervals or uncertainties for the \ncoefficients in the model?\n#.  Did the paper show any diagnostic plots (normal-quantile, residuals plots, etc.)?\nIf not, did the authors say anything in the text about checking that \na linear model is appropriate in their situation?\n#.  What was the main conclusion of the analysis of the linear model?\n#.  If there is an indication that the data are available online,\nlet me know where in case we want to use these data for an example.\n:::: \n<!-- end enumerate -->\n\nGoogle scholar might be a useful tool for locating an article, or a service like\nJSTOR (available through many academic libraries) also has a large number of\nscientific articles.  Or you might ask an engineering or physics professor for\nan appropriate engineering journal to page through in the library.  Since the\nchances are small that two students will find the same article if working\nindependently, I expect to see lots of different articles used for this problem.\n\n<!-- If your article looks particularly interesting or contains statistical  -->\n<!-- things that you don't understand but would like to understand, let me know, -->\n<!-- and perhaps we can do something later in the semester with your article. -->\n<!-- It's easiest to do this if you can give me a URL for locating the paper online. -->\n::: \n<!-- end problem -->\n\n\n\n::: {.problem #exr-human-waste-in-japan}\n**Human waste in Japan**\n\nHigh population density in Japan leads to many resource usage problems, including\nhuman waste removal.  In a study of a new (in the 1990's) copression machine for \nprocessing sewage sludge, researchers measures the moisture content of the \ncompressed pellets (%) and the machines filtration rate (kg-DS/m/hr).\nYou can load the data using\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(xmp12.06, package = \"Devore7\")\n```\n:::\n\n\n\n:::: {.enumerate}\n\na.  What are the least squares estimates for the intercept and slope of a line\n\t\tthat can be used to estimate the moisture content from the filtration rate?\n\t\t(Give them in our usual manner with both estimate and uncertainty.)\nb.  The first row of the data is\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(xmp12.06, 1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"moistcon\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"filtrate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"77.9\",\"2\":\"125.3\",\"_rn_\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nCompute the residual for this observation.\n\nc.  What is $\\hat \\sigma$, the estimated value of $\\sigma$?\nd.  Give a 95% confidence interval for the slope.\ne.  Give a 95% confidence interval for the mean moisture content\n\t\twhen the filtration rate is 170 kg-DS/m/hr.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(xmp12.06, package = \"Devore7\")\nsewage.model <- lm(moistcon ~ filtrate, data = xmp12.06)\n# a)\ncoef(summary(sewage.model))  # this includes standard errors; could also use summary() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) 72.95854697 0.697528488 104.595795 1.615724e-26\nfiltrate     0.04103377 0.004836781   8.483693 1.051720e-07\n```\n:::\n\n```{.r .cell-code}\n# b)\nresid(sewage.model)[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1 \n-0.2000784 \n```\n:::\n\n```{.r .cell-code}\n# c)\nsigma(sewage.model)  # can also be read off of the summary() output\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6653314\n```\n:::\n\n```{.r .cell-code}\n# d)\nconfint(sewage.model)[2, , drop = FALSE]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              2.5 %     97.5 %\nfiltrate 0.03087207 0.05119547\n```\n:::\n\n```{.r .cell-code}\n# e) \nestimated.moisture <- makeFun(sewage.model)\nestimated.moisture(170, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr     upr\n1 79.93429 79.50398 80.3646\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n",
    "supporting": [
      "Stat241-SimpleLinear_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}