
# Random Variables

```{r}
#| include: false
library(mosaic)
library(mosaicCalc)
library(alr4)
library(triangle)
library(fastR2)
theme_set(theme_bw())
onLine <- TRUE
```


::: {.description}
* **random variable** a random process that results in a number
::: 
<!-- end description -->


We have already seen notation for and a few examples of random variables.
In this chapter we will learn a bit more about random variables.  We will
focus our attention on two types of random variables: discrete random variables
and continuous random variables.^[There are important examples
of random variables that are neither discrete nor continuous.]

## Discrete Random Variables

A discrete random variable takes on values from a discrete set of possibilities,
typically either a finite set or a subset of the integers.  Here are some examples.

::: {.enumerate}

#. If we roll a die and record the number, there are six possible values,
so the random variable is discrete.
#. If we keep flipping a coin until we get a head and record the number of coin
tosses, then the possible values of the random variable are $1, 2, 3, \dots
This is also a discrete random variable.
::: 
<!-- end enumerate -->



For each of the possible values of a discrete random variable, there is some
probability of that value occuring.  So to specify a discrete random variable,
we need to specify those probabilities.  When there are only a small number 
of possible value, we can do this with a table.

:::{.center}
+-------------------+-----+-----+------+
| value of $X$      |  0  |  1  |   2  |
+-------------------+-----+-----+------+
| probability       | 0.2 | 0.5 | 0.3  |
+-------------------+-----+-----+------+
:::

This is really just one way of describing a function, called the 
**probability mass function** (or pmf).  The pmf satisfies
$$
f(x) = \Prob(X = x)
$$ {#eq-pmf}

Sometimes instead of providing a table, we will be able to specify
the pmf using a formula.  For example, we could define a pmf $g$ by

$$ 
g(y) = (2 - |1-y|) / 4 \mbox{ for $y \in \{0, 1, 2\}$} 
$$
which is the same as specifying $g$ with the following table.


:::{.center}
+-------------------+------+------+-------+
| value of $Y$      |  0   |  1   |  2    |
+-------------------+------+------+-------+
| probability       | 0.25 | 0.5  | 0.25  |
+-------------------+------+------+-------+
:::

Here's one more example.  Let $W$ be a random varaible that can take on any integer
value and has pmf given by

$$
h(w) = \left(\frac{1}{2}\right)^{w+1} \mbox {for $w = 0, 1, 2, 3, \dots$}
$$

So, for example, $\Prob(W = 2) = h(2) = \left(\frac12 \right)^3 = \frac18$.

Probabilities can be obtained from the pmf by adding:


* $\Prob( X > 0 ) = 0.5 + 0.3 - 0.8$
* $\Prob( Y > 0) = 0.5 + 0.25$
* $\Prob(W > 0) = \frac{1}{4} + \frac{1}{8} + \frac{1}{16} + \cdots = \frac{1}{2}$
* $\Prob(W > 0) = 1 - \Prob(W = 0) = 1 - \frac{1}{2} = \frac{1}{2}$

The only restrictions on a pmf are that 

::: {.enumerate}
#. the values must all be non-negative, and 
#. the sum (over all possible values of the random variable) must be 1.
::: 
<!-- end enumerate -->

That way the probabilities will behave the way probabilities should.


::: {.example #exm-freethrow-amy}

**Q.** Amy is a 92% free throw shooter. We watch her take shots until she misses and let 
$X$ be the number of shots.  What is the pmf for $X$?

**A.** This time our table would be infinite (since there is no limit to how many consecutive
free throws Amy might make), so we won't be able to write the whole table down.
But we can work out the first few probabilities:

::: {.itemize}

* $f(0) = \Prob(X = 0) = 0.08$.  (She has to miss the first shot.)
* $f(1) = \Prob(X = 1) = (0.92)(0.08)$.  (She has to make the first and miss the second.)
* $f(2) = \Prob(X = 2) = (0.92)^2(0.08)$.  (She has to make the first two, then miss the third.)
::: 
<!-- end itemize -->

At this point, we see there is a general pattern that allows us to write down an 
algebraic form for the pmf:

$$
f(x) = \Prob(X = x) = (0.92)^{x-1}(0.08)
$$
where $x$ is a positive integer.  (If $x$ is not an integer, or $x < 1$, then $f(x) = 0$, since
those values are not possible.)
::: 
<!-- end example -->



## Continuous Random Variables

### Density histograms, density plots, density functions
A histogram is a simple picture describing the "density" of data. 
Histogram bars are tall in regions where there is more data -- i.e., where the data are 
more "dense".  
```{r histograms-count-density}
#| label: fig-oldfaith-histograms
#| layout-ncol: 2
#| fig-cap: Two histograms of the Old Faithful eruption data.
#| fig-subcap:
#|   - A frequency histogram.
#|   - A density histogram.
library(alr4)
gf_histogram(  ~ Duration, data = oldfaith)
gf_dhistogram( ~ Duration, data = oldfaith)
```


The density scale is the same scale that is used by `gf_dens()` and `gf_density()`, 
and it is the default scale for histograms created using `gf_dhistogram()`.

```{r histogram-density}
#| label: fig-histogram-and-density
#| fig-cap: A histogram with an overlaid density curve.
library(alr4)
gf_dhistogram( ~ Duration, data = oldfaith, binwidth = 20, center = 110) |>
gf_dens( ~ Duration, data = oldfaith)
```


The density scale is chosen so that the area of each rectangular bar (width times height)
is equal to the proportion of the data set represented by the rectangle. 

::: {.example #exm-old-faithful}

**Q.** Use the histogram of Old Faithful eruption times to estimate
	the proportion of eruptions that last between 100 and 120 seconds.

**A.** In our histogram of Old Faithful eruption durations, the bar corresponding to the 
bin from 100--120 appears to have a height of about 0.09.  That gives an area of 
0.18 and indicates that approximately 18% of the eruptions last between 100 and 120 
seconds.
```{r }
tally( ~ ( 100 < Duration & Duration <= 120), data = oldfaith, format = "prop" )
```

::: 
<!-- end example -->


The key idea behind the density scale can be expressed as 

::: {.boxedText}
:::: {.center}
Probability $=$ area
:::: 
:::

<!-- end center -->

This association of area with probability means that
the total area of all the bars will always be equal to 1 if we use the density
scale.  

It also provides us with a way to describe a distribution with a mathematical function.


::: {.boxedText #def-pdf}

Let $f: \reals \to \reals$ be a function such that 

#. $f(x) \ge  0$ for all $x$,
#. $\displaystyle  \int_{-\infty}^{\infty} f(x) \; dx = 1$.

Then $f$ is called a **density function** 
(or probability density function, abbreviated pdf) 
and describes a continuous random variable $X$ such that

$$
   \Prob(a \le X \le b) = \int_a^b f(x) \; dx \;.
$$ { #eq-integral-for-probability }
::: 

<!-- end boxedText -->


:::{.example #exm-triangle}

**Q.** Let $f$ be defined by
$$
	f(x) = \begin{cases}  
		1 - |x| & x \in [-1,1] \\
		0 & \mbox{otherwise}\\
	\end{cases}
$$
Show that $f$ is a density function.  Let $X$ be the associated random 
variable, and compute the following probabilities:

1. $\Prob(X\le0)$
2. $\Prob(X \le 1)$
3. $\Prob(X \le \frac12)$
4. $\Prob(-\frac12 X \le \frac12)$

**A.**
While we could set up integrals for these, it is easier to solve them using 
geometry.^[R cleverly turns TRUE and FALSE into 1 and 0 when you use them in arithmetic
expressions.  The definition of `f()` makes use of this conversion to simplify specifying
the cases.]

```{r }
f <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x )
gf_fun( f(x) ~ x, xlim = c(-1.5, 1.5) )
```


The entire area under the curve can be found as the area of a triangle with base 2 and height 1.
$$
\int_{-\infty}^{\infty} f(x) \; dx 
=
\int_{-1}^1 f(x) \; dx
=
\frac 12 \cdot 2 \cdot 1 = 1
$$
This implies that $f$ is a density function.

1. $\Prob(X \le 1)  = \int_{-\infty}^{1} f(x) \; dx  =
\int_{-1}^{1} f(x) \; dx = 1$.

2. $\Prob(X \le \frac12) = \int_{-\infty}^{1/2} f(x) \; dx  =
\int_{-1}^{1/2} f(x) \; dx = 1 - \frac12 \cdot \frac12 \cdot \frac12 = \frac78$.

3. $\Prob( -\frac12 \le X \le \frac 12 )  = \int_{-1/2}^{1/2} f(x) \; dx =
	1 - \frac{2}{8} = \frac{3}{4}$.

We can also let R do (numerical) integration for us.  There are two ways to do this.
The first method uses the `integrate()` function.
```{r }
integrate( f, -Inf, 1 )
# this will be more accurate since we aren't asking R to approximate
# something that we already know is exactly 0
integrate( f, -1, 1)
integrate( f, -.5, .5 )
# if you just want the value without the text saying how accurate the approximation is
# here are two equivalent ways to get it
value(integrate( f, -.5, .5 ))        # extract the value using val()
integrate( f, -.5, .5 ) |> value()   # |> is the "then" operator
```


An alternative approach uses `antiD()` from the **`mosaicCalc`** package.
```{r tidy = FALSE}
library(mosaicCalc)
F <- antiD( f(x) ~ x)
F(1) - F(-1)        # total probability -- better be 1
F(.5) - F(-1)       # P( -1 <= X <= 0.5 )
F(.5) - F(-.5)      # P( -.5 <= X <= .5 )
```

::: 
<!-- end example -->


### Kernels

The **kernel** of a continuous random variable is a function that is a 
constant multiple of the pdf.  The reason that these are interesting
is that any kernel can be converted into a pdf by dividing by this constant.
In particular, if 

$$ 
\int_{-\infty}^{\infty} k(x) \; dx = A \;, 
$$
then $k$ is the kernel of a random variable with pdf
$$ 
f(x) = \frac{k(x)}{A} \;.
$$

:::{.example #exm-kernel}

**Q.** The kernel of a random variable is given by 

$$
k(x) = x^2 \; \boolval{ x \in [0,2] } \; .
$$
Determine the pdf.

**A.** First we determine the value of the integral
$$
\int_{-\infty}^{\infty} k(x) \; dx \;. 
$$
```{r }
k <- makeFun( x^2 * ( 0 <= x & x <= 2) ~ x )
plotFun(k(x) ~ x, xlim = c(-1,3))
integrate( k, 0, 2)
K <- antiD(k(x) ~ x, lower.bound = 0)
K(2)
```

Since the total area is $8/3$, if $\frac{k(x)}{8/3}$ is the pdf.
:::

### Cumulative distribution functions


<!-- % The \textbf{cumulative distribution function} (cdf) returns the probability of being -->
<!-- % less than or equal to some value.  We generally use a capital letter ($F$, $G$, etc.) -->
<!-- % to denote a cdf. -->
<!-- % $$ -->
<!-- % F_X(x) = \Prob( X \le x)  \; . -->
<!-- % $$ -->

<!-- % If we help R choose the anti-derivative, we get a useful function called  -->
<!-- % the \term{cumulative distribution function}, abbreviated cdf. -->


::: {.boxedText}

If $X$ is a random variable, then the **cumulative distribution function** (cdf) for 
$X$, often denoted $F_X$, is the function defined by
$$
F_X(x)  = \Prob(X \le  x)
$$
That is, the output of the cdf reports the probability of being below a particular value.
::: 
<!-- end boxedText -->


For a continuous random variable, the cdf is a particular anti-derivative
of the pdf.  The derivative of the cdf is the pdf.

::: {.example #exm-kernel-continued}

Continuing with our previous example, if we choose -1 as our lower endpoint,
then the anti-derivative will be the cdf.
```{r tidy = FALSE}
f <- makeFun((1 - abs(x)) * (abs(x) <= 1) ~ x)
F <- antiD( f(x) ~ x, lower.bound = -1)   # We can use -1 instead of -Inf here.
F(-1)               # this should be 0 since we chose -1 as the lower bound.
F(1)                # P(X <= 1); should be 1
F(.5)               # P(X <= 0.5)
F(.5) - F(-.5)      # P( -0.5 <= X <= 0.5 )
```

::: 
<!-- end example -->


<!-- ## Working with Probability Density Funcitons -->

We have already seen that we can use a pdf $f$ to calculate probabilities via integration, 
and that there is a special anti-derivative of $f$ called the cdf such that the cdf $F$
satisfies
$$
F(x) = \Prob(X \le x)
$$
This function can also be used to compute probabilities, since
$$
\Prob(a \le X \le b) = \int_a^b f(x) \; dx = F(b) - F(a)
$$
Indeed, once we learn how to get the cdf function in R this will 
be our primary way to calculate probabilities in applications.

## Mean and Variance

### The mean of a random variable

The definition for the mean of a random variable will be motivated 
by the calculation of a mean of some data.

::: {.example #exm-gpa}

**Q.**   Suppose a student has taken $10$ courses
  and received $5$ A's, $4$ B's, and $1$ C.  Using the traditional numerical scale where 
  an A is worth $4$, a B is worth $3$, and a C is worth $2$, what is this student's 
  GPA (grade point average)?

**A.** The first thing to notice is that $\frac{4 + 3 + 2}{3} = 3$ is 
*not* correct. We cannot simply add up the values and divide by the number of values.  Clearly
this student should have a GPA that is higher than $3.0$, since there were more A's than
C's.

Consider now a correct way to do this calculation:

$$
\begin{aligned}
	\mbox{GPA} &= \frac{4 + 4 + 4 + 4 + 4 + 3 + 3 + 3 + 3 + 2}{10}
	\\[2mm]
	& = \frac{5\cdot 4 + 4\cdot 3 + 1 \cdot 2}{10} \\[1mm]
	& = \frac{5}{10} \cdot 4 + \frac{4}{10}\cdot 3 + \frac{1}{10} \cdot 2 \\
	& = 4 \cdot \frac{5}{10} + 3 \cdot \frac{4}{10} + 2 \cdot \frac{1}{10} \\
	& =  3.4 \;.
\end{aligned}
$$

:::

The key idea here is that the mean is a **sum of values times probabilities**.  

$$
\mbox{mean} = \sum \mbox{value} \cdot \mbox{probability}
$$
For a discrete random variable this translates to
$$
\E(X) = \sum x f(x)
$$
where the sum is taken over all possible values of $X$.

The mean of a random variable also goes by another name: **expected value**.  We 
can denote the mean of $X$ by either $\mu_X$ or $\E(X)$.

::: {.example #exm-mean-exp-value-1}

Let $X$ be discrete random variable with probablilities given in the table 
below.

:::: {.center}

+--------------------+-----+-----+-----+
+ value of $X$       |  0  |  1  | 2   | 
+--------------------+-----+-----+-----+
+ probability        | 0.2 | 0.5 | 0.3 |
+--------------------+-----+-----+-----+
::::

<!-- end center -->


**Q.** What is the mean (expected value) of $X$?

**A.** $E(X) = 0 \cdot 0.2 + 1 \cdot 0.5 + 2 \cdot 0.3 = 0.5 + 0.6 = 1.1$
This value reflects the fact that the random variable is larger than
1 a bit more often than it is less than 1.
::: 
<!-- end example -->



::: {.example #exm-raffle}

A local charity is holding a raffle.  They are selling $1000$ raffle tickets 
for \$$5$ each.  The owners of five of the raffle tickets will win a prize.  The 
five prizes are valued at \$$25$, \$$50$, \$$100$, \$$1000$, and \$$2000$.  Let $X$ 
be the value of the prize associated with a random raffle ticket ($0$ for 
non-winning tickets). 
Then:

::: {.itemize}

* $\evProb{the ticket wins a prize} = \Prob(X > 0) = 5/1000$.
* $\evProb{the ticket wins the grand prize} = \Prob(X = 2000) = 1/1000$.
* $\evProb{the ticket wins a prize worth more than \$75}  = \Prob(X > 75) = 3/1000$.
::: 
<!-- end itemize -->

The expected value of a ticket is
  
$$
  0 \cdot\frac{995}{1000} 
  + 25 \cdot\frac{1}{1000}
  + 50 \cdot\frac{1}{1000}
  + 100 \cdot\frac{1}{1000}
  + 1000 \cdot\frac{1}{1000}
  + 2000 \cdot\frac{1}{1000}
$$

```{r }
25 * .001 + 50 * 0.001 + 100 * 0.001 + 1000 * 0.001 + 2000 * 0.001
# R can help us set up this sum:
sum(c(25, 50, 100, 1000, 2000) * 0.001)
```

::: 
<!-- end example -->

When working with a continuous random variable, 
we replace the sum with an integral and replace the 
probabilities with our density function to get the following definition:

$$
\E(X) = \mu_X = \int_{-\infty}^{\infty} x f(x) \; dx
$$

If you recall doing center of mass problems you may recognize this integral as
the first moment. (For pdfs, we don't need to divide by the ``mass'' because the
total ``mass'' is the area under the curve, which will always be 1 for a random
variable).

Note: It is possible that the integral used to define the mean will fail to converge.
In that case, we say that the random variable has no mean or that the mean fails to 
exist.^[Actually, we will require that $\int_{\infty}^{\infty} |x| f(x) \; dx$ converges.  
If this integral fails to converge, we will also say that the distribution has no mean.]

::: {.example #exm-mean-triangle}

**Q.** Compute the mean of our triangle distribution from @exm-triangle-in-R.

**A.** We simply compute the integral from the definition.

$$
\begin{aligned}
\E(X) & = \int_{-1}^{1} x f(x) \; dx 
\\
	& = \int_{-1}^{0} x (x-1) \; dx + \int_{0}^1 x ( 1-x ) \; dx 
	\\
	& = \int_{-1}^{0} x^2-x) \; dx + \int_{0}^1 x-x^2 ) \; dx  
	\\
	& = \left. \frac{x^3}{3} - \frac{x^2}{2} \right|_{-1}^0
	+ \left. \frac{x^2}{2} - \frac{x^3}{3} \right|_{0}^1
	\\
	& = \frac13 - \frac12 + \frac12 - \frac13 = 0
\end{aligned}
$$

This isn't surprising, by symmetry we would expect this result.

We could also calculate this numerically in R:

```{r }
f <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x)
xf <- makeFun( x * f(x) ~ x )
integrate(xf, -1, 1)
F <- antiD( x * f(x) ~ x, lower.bound = -1)
F(-1)  # should be 0
F(1)
```

::: 
<!-- end example -->


### Variance

Arguing similarly, we can compute the variance of a discrete or continuous random
variable using 

::: {.itemize}

* discrete: $\Var(X) = \sigma^2_X = \sum x (x-\mu_X)^2$

* continuous: $\Var(X) = \sigma^2_X = \int_{-\infty}^{\infty} (x-\mu_X)^2 f(x) \; dx$
::: 
<!-- end itemize -->

These can be combined into a single definition by writing
$$
\Var(X) = \E((X - \mu_X)^2) \;.
$$

Note: It is possible that the sum or integral used to define the mean (or the variance) will fail 
to converge. In that case, we say that the random variable has no mean (or variance) or that the 
mean (or variance)  fails to exist.^[Actually, we will require that 
$\int_{\infty}^{\infty} |x| f(x) \; dx$ converges and   
$\int_{\infty}^{\infty} |x|^2 f(x) \; dx$ converges.  
If these integrals (or the corresponding sums for discrete random variables) fail to converge, 
we will say that the distribution has no mean (or variance).]

::: {.example #exm-var-triangle}

**Q.** Compute the variance of the triangle random variable from the @exm-triangle.
	
**A.** 

```{r }
f <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x)
xxf <- makeFun( (x-0)^2 * f(x) ~ x )
integrate(xxf, -1, 1)
G <- antiD( (x-0)^2 * f(x) ~ x)
G(1) - G(-1)
```

::: 
<!-- end example -->


Some simple algebraic manipulations of the sum or integral above shows that

$$
\begin{aligned}
\Var(X) &= \E(X^2) - \E(X)^2
\end{aligned}
$$ {#eq-var-shortcut}


::: {.problem #exr-show-pdf}
 
Let $f(x) = 5/4 - x^3$  on $[0,1]$.  

1. Show that $f$ is a pdf.
2. Calculate $\Prob(X \le\frac12)$.
3. Calculate $\Prob(X \ge\frac12)$.
4. Calculate $\Prob(X = \frac12)$.


::: 
<!-- end problem -->


::: {.solution}

```{r }
f <- makeFun( (5/4 - x^3) * ( abs(x-.5) <= .5 ) ~ x )
plotFun(f(x) ~ x, x.lim = c(-1,2))  # quick plot to make sure things look correct.
F <- antiD(f(x) ~x)  
# part a:  f(x) >=0, so we just need to check that the total area is 1
F(1) - F(0) == 1  
F(1/2) - F(0)     # part b
F(1) - F(1/2)     # part c
F(1/2) - F(1/2)   # part d
```

::: 
<!-- end solution -->




::: {.example #exm-compute-mean-var}

**Q.** Compute the mean and variance of the random variable with pdf given by
	
$$ 
g(x) = \frac{3x^2}{8} \boolval{x \in[0,2]} \;. 
$$

This is the pdf computed in @exm-kernel.
	
**A.** 
```{r }
g <- makeFun( (3 * x^2/8 ) * (0 <= x & x <= 2) ~ x )
m <- antiD( x * g(x) ~ x, lower.bound = 0)(2)  # all in one step instead of defining F or G
m
v <- antiD( (x - m)^2 * g(x) ~ x, m = m, lower.bound = 0)(2)
v
# here's the alternate computation
antiD( x^2 * g(x) ~ x, lower.bound = 0)(2) - m^2
```

::: 
<!-- end example -->


As with data, the standard deviation is the square root of the variance.

### Quantiles

Quantiles solve equations of the form

$$
\int_{-\infty}^x f(t) \; dt = F(x) = \Prob(X \le x) = q
$$

where $q$ is known and $x$ is unknown.  So the 50th percentile (which is the 0.5-quantile
or the median) is the number such that 

$$
\Prob(X \le x) = 0.5 \;.
$$

::: {.example #exm-triangle-quantiles}

**Q.** What is the 25th percentile of the triangle distribution in @exm-triangle?

**A.** 	We need to solve for $x$ in the following equation:

$$
0.25 = \Prob(X \le x)  \; .
$$
We can do this by working out the integral involved:

$$
\begin{aligned}
0.25 &= \int_{-1}^{x} 1 - |t| \; dt 
	\\
	 &= \int_{-1}^{x} 1 + t \; dt 
	 \\
	 &=  \left( t + t^2/2 \right) |_{-1}^{x}
	 \\
	 &=   x + x^2/2 + 1 - 1^2/2  
	 \\
	 &=  x + x^2/2 + 1/2
	 \\
	 0 & =  x^2/2 + x + 1/4
	 \\
	 0 & =  2x^2 + 4x + 1
\end{aligned}
$$

So by the quadratic formula, $x = \frac{1}{2} \sqrt{2} - 1 = `r 1/2*sqrt(2) - 1`$.

We can check this by evaluating the cdf.
```{r }
x <- 1/2*sqrt(2) - 1
F(x)
```


This could also be done geometrically by solving $\frac{1}{2} y^2 = \frac14$ and letting
$x = -1 + y$.
::: 
<!-- end example -->



## Some Important Families of Distributions

For now, we will consider only distributions of continuous random variables
(probability density functions).  We will leave set aside discrete random
variables (probability mass function) until quite a bit later in the course.

A family of distributions is a collection of distributions that share some
common features.  Typically, these are described by giving a pdf that has one or
more **parameters**. A parameter is simply a number that describes (a feature
of) a distribution that distinguishes between members of the family. In this
section we describe briefly some of the important distributions and how to work
with them in R

### Triangle Distributions

The example distribution in the previous section is usually referred to as a
triangle distribution (or triangular distribution) because of the shape of its
pdf.  There are, of course, many triangle distributions.  A triangle
distribution is specified with three numbers: $a$, the minimum; $b$, the
maximum, and $c$, the location of the peak. A triangle distribution is symmetric
if the peak is halfway between the minimum and maximum ($c = \frac{a+b}{2}$).

When $X$ is a random variable with a triangle distribution, we will write
$X \sim\Tri(a,b,c)$.  For many of the most common
distributions, R has several functions that facilitate computation with
those distributions.  The triangle distributions are not in the base R 
distribution, but they can be added by requiring the **`triangle`** package.

For each distribution, there are four functions in R that always start
with a single letter followed by a name for the distribution.  In the case 
of the triangle distributions, these functions are 


| Function                 | What it does                                                                                   |	
|--------------------------|------------------------------------------------------------------------------------------------|
| `dtriangle(x, a, b, c)`  | Computes value of the pdf at `x`.                                                              |
| `ptriangle(q, a, b, c)`  | Computes value of the cdf at `x`, i.e., $\Prob(X \le \texttt{q})$.                             |
|	`qtriangle(p, a, b, c)`  | Computes quantiles, that is a value $q$ so that $\Prob(X \le \texttt{q}) = \texttt{p}$.        |
|	`rtriangle(n, a, b, c)`  | Randomly samples `n` values from the $\Tri(\texttt{a},\texttt{b},\texttt{c})$ distribution.    |

: Triangle distributions in R {#tbl-triangle-in-R}


::: {.example #exm-triangle-in-R}

**Q.** Let $X \sim\Tri0,4,1)$.  Use R to answer the following questions.

:::: {.enumerate}
a. Plot the pdf for $X$.
a. What is $\Prob(X \le 1)$?
a. What is $\Prob(X \le 2)$?
a. What is the median of $X$?
a. What is the mean of $X$?
:::: 
<!-- end enumerate -->

	
**A.** The `gf_dist()` function in the **ggformula** package allows us to 
graph the pdf for any function R knows how to work with in the standard way.
For example, here is a plot of the pdf of a $\Tri(0, 4, 1)$-distribution.

```{r }
library(triangle)      # a package that knows about triangle distributions
gf_dist("triangle", a = 0, b = 4, c = 1)
```

Here is the R code to answer the remaining questions.

```{r }
ptriangle(1, 0, 4, 1)   # P(X <= 4); notice that his is NOT 1/2
ptriangle(2, 0, 4, 1)   # P(X <= 4); also NOT 1/2
qtriangle(0.5, 0, 4, 1) # median is the 0.5-quantile
T <- antiD( x * dtriangle(x, 0,4,1) ~ x, lower.bound = 0)
T(4)                    # mean of X
integrate( makeFun( x * dtriangle(x, 0,4,1) ~ x) , 0, 4)
```

::: 
<!-- end example -->



::: {.problem #exr-triangle-with-geometry}

Repeat parts (2) -- (4) of @exm-triangle-in-R using geometry 
rather than R

::: 
<!-- end problem -->



::: {.solution}

:::: {.itemize}

* $\Prob(X \le 1) = \frac 12 \cdot 1 \cdot \frac12 = \frac 14$
* $\Prob(X \le 2) = 1 - \Prob(X \ge 2) =  1 - \frac12 \cdot 2 \cdot \frac13  = 1 - \frac 13 = \frac23$.
* The median $m$ is a number between 1 and 2 and satisfies
			$\frac 12 = \Prob(X \ge m) = \frac12 (4-m) \frac{4-m}{6}$.  
			Solving for $m$ we get $m = 4 - \sqrt{6} = `r 4 - sqrt(6)`$.
:::: 
<!-- end itemize -->

::: 
<!-- end solution -->


::: {.problem #exr-pdf-from-kernel}

Let 
$$
\displaystyle k(x) = (1 - x^2) \cdot\boolval{ x \in[-1,1] } = 
\begin{cases}
		1-x^2 & x \in[-1,1] \\ 
		0 & \mbox{otherwise} 
\end{cases}
$$ 

be the kernel of a continuous distribution.

:::: {.enumerate}

a. 			Determine the pdf for this distribution.
a. 			Compute the mean and variance for this distribution
:::: 
<!-- end enumerate -->

::: 
<!-- end problem -->


::: {.solution}

	The integrals are easy enough to do by hand, but here is the R code 
	to compute them.
	
<!-- TODO: originally had area as an argument, but changes to mosaicCalc seem to break this -->
```{r tidy = FALSE}
k <- makeFun( 1 - x^2 ~ x )
K <- antiD( k(x) ~ x )
area <- K(1) - K(-1); area 
f <- makeFun( (1 - x^2)/area ~ x)
F <- antiD(f(x) ~ x)
F(1) - F(-1)    # this should be 1 if we have done things right
G <- antiD( x * f(x) ~ x )
H <- antiD( x^2 * f(x) ~ x )
m <- G(1) - G(-1); m               # E(X)
H(1) - H(-1)                       # E(X^2)
H(1) - H(-1) - m^2                 # Var(X)
```

::: 
<!-- end solution -->


::: {.problem #exr-mean-median-triangle}

Let $Y \sim\Tri(0,10,4)$.  Compute $\E(Y)$ and the median of $Y$.
::: 
<!-- end problem -->

::: {.solution}

```{r }
# mean:
m <- antiD( x * dtriangle(x,0,10,4) ~ x, lower.bound = 0)(10)
m
# variance:
antiD( x^2 * dtriangle(x,0,10,4) ~ x, lower.bound = 0)(10) - m^2
# median
qtriangle( 0.5, 0, 10, 4 )
```

::: 
<!-- end solution -->


### Uniform Distributions

A uniform distribution is a described by a constant function over some interval.  Its 
shape is a rectangle.  This makes it particularly easy to calculate probabilities 
for a uniform distribution.  Despite its simplicity, the family of uniform distributions
has many applications.

We will let $X \sim \Unif(a,b)$ denote that $X$ is a uniform random variable on 
the interval from $a$ to $b$. 
In R, the parameters $a$ and $b$ are given more meaningful names: `min` and `max`.
We can use the following code to graph the $\Unif (1,4)$ distribution.

```{r }
gf_dist("unif", min = 1, max = 4, xlim = c(-1, 6)) 
```

Notice that the width of the non-zero portion of the pdf is 3, so the height must be $1/3$.

Probabilities involving uniform distributions are easily calculated using simple geometry,
but R also provides several functions for working with uniform probability distributions.

| Function             | What it does                                                                              |
|----------------------|-------------------------------------------------------------------------------------------|
| `dunif(x, min, max)` | Computes value of the pdf at `x`.                                                         |
| `punif(q, min, max)` | Computes value of the cdf at `x`, i.e., $\Prob(X \le \texttt{q})$.                        |
| `qunif(p, min, max)` | Computes quantiles, that is a value $q$ so that  $\Prob(X \le \texttt{q}) = \texttt{p}$.  |
| `runif(n, min, max)` | Randomly samples `n` values from the $\Unif(\texttt{min},\texttt{max})$ distribution.     |

: Uniform distributions in R {#tbl-unif}


Notice the pattern to these names.  They start with the same letters as 
the functions for the triangle distributions, but replace `triangle`
with `unif`. 
*There are similar functions for all of the distributions in this chapter.*

::: {.example #exm-unif-in-R}

**Q.** 	Let $X \sim \Unif(1,4)$.  Use R to calculate the following values and 
	check the values using geometry:
	
:::: {.enumerate}

a. $\Prob(X \le 2)$
b. the 80th percentile of the distribution
:::: 
<!-- end enumerate -->


**A.** 
```{r }
punif(2,1,4)   # P(X <= 2 )
(2-1) * 1/3    # P(X <= 2 ) using area
qunif(.8, 1,4) # 80th percentile
```

We could also get the 80th percentile by solving the equation $\frac{3}{(x-1)} = 0.8$
From this we get $\frac{x}{3} = 0.8 + 1/3$, so $x = 3 ( 0.8 + 1/3) = 2.4 + 1 = 3.4$.
::: 
<!-- end example -->


::: {.problem #exr-exp-var-unif}
Let $W \sim \Unif(0,10)$.  Compute $\E(W)$ and $\Var(W)$.
::: 
<!-- end problem -->


::: {.solution}

```{r }
# mean:
m <- antiD( x * dunif(x,0,10) ~ x, lower.bound = 0)(10)
m
# variance:
antiD( x^2 * dunif(x,0,10) ~ x, lower.bound = 0)(10) - m^2
```
::: 
<!-- end solution -->



### Exponential Distributions

The exponential distributions are useful for modeling the time until some "event" 
occurs.  The model is based on the assumptions that 

:::: {.enumerate}

a. The probability of an event occurring in any small interval of time 
	 is proportional to the length of the time interval.  The constant
	 of proportionality is the rate parameter, usually denoted by $\lambda$.
	 
a. The probabilities of events occurring in two small non-overlapping intervals
	 are independent.
:::: 
<!-- end enumerate -->


::: {.example #exm-exponential-dist}

Here are some situations that might be well modeled by an exponential distribution:

:::: {.enumerate}
a. The time until the next radioactive decay event is detected on a Geiger counter 

a. The time until a space satellite is struck by a meteor (or some other space junk) 
	 and disabled.

	 The model would be good if (over some time span of interest) the chances of getting
	 struck are always the same.  It would not be such a good model if the satellite moves
	 through time periods of relatively higher and then relatively lower chances of being
	 struck (perhaps because we pass through regions of more or less space debris at 
	 different times of the year.)

#. The lifetime of some manufactured device.

	 This is a pretty simple model (we'll learn better ones later) and most often
	 is *too* simple to describe the interesting features of the lifetime of a
	 device.  In this model, failure is due to some external thing "happening to"
	 the device; the device itself does not wear (or improve) over time.
:::: 
<!-- end enumerate -->

::: 
<!-- end example -->


We will let $X \sim \Exp(\lambda)$ denote that $X$ has an exponential distribution
with rate parameter $\lambda$. The kernel of such a distribution is 
$$
k(x; \lambda) = e^{-\lambda x} \; \boolval{x \ge 0}
$$
Notice that the function describing this distribution is defined only for x-values that are real numbers greater than or equal to zero (in mathematical notation, the interval $[0, \infty)$.) This interval is sometimes called the ``support" of the distribution.  When using probability distributions to model data, it's important to think about whether the support of the distribution matches well with the range of possible values observed in the data.

The exponential distribution function is a pretty easy function to integrate, but R provides the now familiar
functions to make things even easier.


| Function               | What it does                                               	                                |
|------------------------|----------------------------------------------------------------------------------------------|
| `dexp(x, rate)`        | Computes value of the pdf at `x`.                                                            |
| `pexp(q, rate)`        | Computes value of the cdf at `x`, i.e., $\Prob(X \le \texttt{q})$.                           |
|	`qexp(p, rate)`        | Computes quantiles, that is a value $q$ so that $\Prob(X \le \texttt{q}) = \texttt{p}$.      | 
|	`rexp(n, rate)`        | Randomly samples `n` values from the $\Exp(\lambda)$ distribution where $\lambda$ = `rate`.  |

: Exponential distributions in R {#tbl-exp}

<!-- \iffalse -->
<!-- \begin{boxedText} -->
<!-- 	We will write $X \sim \Exp(\lambda)$ to indicate that the random variable $X$ has  -->
<!-- 	an exponential distribution with rate parameter $\lambda$. -->
<!-- 	\begin{center} -->
<!-- 		\begin{tabular}{lll} -->
<!-- 			\hline  -->
<!-- 			parameter & $\lambda$ & \texttt{rate} -->
<!-- 			\\ -->
<!-- 			kernel & $e^{-\lambda x}$ -->
<!-- 			\\ -->
<!-- 			pdf & $f(x; \lambda) = \lambda e^{-\lambda x} \boolval{x\ge 0}$  -->
<!-- 			& \texttt{dexp(x,rate)} -->
<!-- 			\\ -->
<!-- %			mean & $\frac{1}{\lambda}$ -->
<!-- %			\\ -->
<!-- %			variance & $\frac{1}{\lambda^2}$ -->
<!-- %			\\  -->
<!-- %			\hline -->
<!-- 		\end{tabular} -->
<!-- 	\end{center} -->
<!-- \end{boxedText} -->
<!-- \fi -->


```{r gf-dist-eponential}
#| label: fig-exponential-4
#| fig-cap: An exponential distribution with rate = 4.
gf_dist("exp", rate = 4)
```


::: {.problem #exr-exp-of-exponential}

:::: {.enumerate}

a. Let $X \sim \Exp(4)$.  Use R to compute $\E(X)$.
a. Let $X \sim \Exp(10)$.  Use R to compute $\E(X)$.
a. Let $X \sim \Exp(1/5)$.  Use R to compute $\E(X)$.
a. What pattern do you notice.  Explain in terms of the definition of the exponential 
distribution why this makes sense.
:::: 
<!-- end enumerate -->

::: 
<!-- end problem -->


::: {.solution}

```{r }
antiD( x * dexp(x, 4) ~ x, lower.bound = 0)(Inf)
antiD( x * dexp(x, 10) ~ x, lower.bound = 0)(Inf)
antiD( x * dexp(x, 1/5) ~ x, lower.bound = 0)(Inf)
```

It appears that the mean of an $\Exp(\lambda)$-distribution is $1/\lambda$.  This
makes sense.  If events have at a rate of $30$ per hour, we would expect to wait
$1/30$ of an hour (on average) for the first event to happen.
::: 
<!-- end solution -->


### Gamma and Weibull Distributions

The Gamma and Weibull familities of distributions are generalizations of the exponential
distribution.  Each family has two parameters, a rate parameter as in 
the exponential distribution, and an additional parameter called the shape 
parameter (denoted by $\alpha$ below).  The reciprocal of the rate parameter
is called the scale parameter.  For the Gamma distribution, R lets us use
either rate or scale (and the default is rate).  For the Weibull, we must use the 
scale.

+----------------------------------------+----------------------------------------------------------+
+ distribution                           + kernel                                                   +
+========================================+==========================================================+
| $\Gamm(\alpha, \lambda)$               | $ k(x) = x^{\alpha} e^{-\lambda x}$ for $x \ge 0$        |
+----------------------------------------+----------------------------------------------------------+
| $\Weibull(\alpha, \lambda)$            | $ k(x) = x^{\alpha} e^{-\lambda x^{\alpha}}$ for $x > 0$ |
+----------------------------------------+----------------------------------------------------------+

: Kernels for Gamma and Weibull distributions.


Both families of distributions are supported on the interval $[0, \infty$.)
For the most part, we won't use these formulas in calculations, 
preferring to let R do the work for us. However, notice that each
of these distributions has a pdf that allows for relatively simple integration.  For the 
Gamma distributions, we need to use integration by parts ($\alpha- 1$ times).  For 
the Weibull distributions we can use a substitution: $u = x^{\alpha}$.
In each case, when $\alpha= 1$ we get an exponential distribution.

The now familiar functions are available for each of these distributions.

| Function                                      | What it does                                                                            |	
|------------------------|----------------------------------------------------------------------------------------------------------------|
| `dgamma(x, shape, rate = 1, scale = 1/rate)`  | Computes value of the pdf at `x`.                                                       |
| `pgmma(x, shape, rate = 1, scale = 1/rate)`   | Computes value of the cdf at `x`, i.e., $\Prob(X \le \texttt{q})$.                      |
|	`qgamma(x, shape, rate = 1, scale = 1/rate)`  | Computes quantiles, that is a value $q$ so that $\Prob(X \le \texttt{q}) = \texttt{p}$. |
|	`rgamma(n, shape, rate, scale = 1/rate)`      | Randomly samples `n` values from the Gamma distribution.                                |

: Gamma distributions in R.

| Function                                      | What it does                                                                                |	
|-----------------------------------------------|---------------------------------------------------------------------------------------------|
| `dweibull(x, shape, scale = 1)`               | Computes value of the pdf at `x`.                                                           |
| `pweibull(x, shape, scale = 1)`               | Computes value of the cdf at `x`, i.e., $\Prob(X \le \texttt{q})$.                          | 
| `qweibull(x, shape, scale = 1)`               | Computes quantiles, that is a value $q$ so that $\Prob(X \le \texttt{q}) = \texttt{p}$.     |
|	`rweibull(n, shape, scale = 1)`               | Randomly samples `n` values from the Weibull distribution.                                  |

: Weibull distributions in R.

<!-- $\Gamm(\alpha, \lambda)$ or                               | -->
<!-- |                                               | $\Weibull(\alpha, \beta)$ distribution where.             | -->
<!-- |                                               |   $\alpha$ = `shape`, $\lambda$ = `rate`,                 | -->
<!-- |                                               |   and $\beta$ = `scale`                                   | -->
<!-- +-----------------------------------------------+-----------------------------------------------------------+ -->

Like the exponential distributions, these distributions are skewed and only take 
on positive values.  
These distributions arise in many applications, including as more general models 
for lifetime.  As the pictures below indicate, the shape and scale parameters
are aptly named.

```{r }
#| label: fig-gamma-dists
#| fig-cap: Some example gamma distributions.
#| fig-subcap: ""
#| layout-ncol: 2
gf_dist("gamma", params = list(shape = 2, rate = 1), title = "Gamma(2,1)")
gf_dist("gamma", params = list(shape = 5, rate = 1), title = "Gamma(5,1)")
gf_dist("gamma", params = list(shape = 2, scale = 10), title = "Gamma(5,10)")
gf_dist("gamma", params = list(shape = 5, scale = 10), title = "Gamma(5,10)")
```


```{r }
#| label: fig-weibull-dists
#| fig-cap: Some example Weibull distributions.
#| fig-subcap: ""
#| layout-ncol: 2
gf_dist("weibull", params = list(shape = 2, scale = 1),title = "Weibull(2,1)")
gf_dist("weibull", params = list(shape = 5, scale = 1),title = "Weibull(5,1)")
gf_dist("weibull", params = list(shape = 2, scale = 10),title = "Weibull(2,10)")
gf_dist("weibull", params = list(shape = 5, scale = 10),title = "Weibull(5,10)")
```


### Normal Distributions

We come now to the most famous family of distributions -- the normal
distributions (also called Gaussian distributions).  These symmetric 
distributions have the famous ``bell shape'' and are described by two parameters, 
the mean $\mu$ and the standard deviation $\sigma$.  The pdf for a $\Norm(\mu, \sigma)$
distribution is 

$$ 
f(x) = \frac{ 1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/2\sigma^2}
$$ {#eq-normal-pdf}

<!-- +---------------------------------------+--------------------------------------------------------+ -->
<!-- | distribution                          | pdf                                                    | -->
<!-- +=======================================+========================================================+ -->
<!-- |	$\displaystyle \Norm(\mu, \sigma)$    | $\displaystyle f(x) =$                                 | -->
<!-- |                                       | $\frac{ 1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/2\sigma^2}$| -->
<!-- +---------------------------------------+--------------------------------------------------------+ -->

<!-- : The normal pdf. {#tbl-normal-pdf} -->


```{r echo = FALSE, fig.width = 7}
#| fig-align: center
gf_dist("norm") |>
  gf_theme(axis.text.y = element_blank()) |>
  gf_labs(y = "") |>
  gf_refine(
    scale_x_continuous(
      breaks = -3:3, 
      labels = c( 
        expression(mu - 3 * sigma),
        expression(mu - 2 * sigma),
        expression(mu - 1 * sigma),
        expression(mu),
        expression(mu + 1 * sigma),
        expression(mu + 2 * sigma),
        expression(mu + 3 * sigma) 
      )  
    )
  )
```

The inflection points 
of the normal distributions are always at $\mu -\sigma$ and $\mu+\sigma$.

Among the normal distributions is one special distribution -- the **standard normal
distribution** -- which has mean 0 and standard deviation 1. All other normal 
distributions are simply linear transformations of the standard normal distribution.
That is,
If $Z \sim \Norm(0,1)$ and $Y = a + b X$ , then $Y \sim \Norm(a, b)$. 
Conversely, if $Y \sim \Norm(\mu,\sigma)$, then 
$Z = \frac{Y - \mu}{\sigma} \sim \Norm(0,1)$.

As with the other distributions we have encountered, we have four functions
that allow us to work with normal distributions in R

| Function                     | What it does                                                                                                       |	
|------------------------------|--------------------------------------------------------------------------------------------------------------------|
| `dnorm(x, mean = 0, sd = 1)` | Computes value of the pdf at `x`.                                                                                  |
| `pnorm(q, mean = 0, sd = 1)` | Computes value of the cdf at `x`, i.e., $\Prob(X \le \texttt{q})$.                                                 |
|	`qnorm(p, mean = 0, sd = 1)` | Computes quantiles, that is a value $q$ so that $\Prob(X \le \texttt{q}) = \texttt{p}$.                            |
|	`rnorm(n, mean = 0, sd = 1)` | Randomly samples `n` values from the   $\Norm(\mu, \sigma)$ distribution where $\mu$ = `mean` and $\sigma$ = `sd`. | 

: Normal distributions in R. {#tbl-normal-in-R}

#### The 68-95-99.7 Rule

Also known as the "Empirical Rule", the 68-95-99.7 Rule provides a set of probability
benchmarks for the normal distributions because for any normal distribution: 
 
* $\approx 68$% of the normal distribution is between  $\mu - \sigma$ and $\mu + \sigma$.
* $\approx 95$% of the normal distribution is between  $\mu - 2\sigma$ and $\mu + 2\sigma$.
* $\approx 99.7$% of the normal distribution is between $\mu - 3\sigma$ and $\mu + 3\sigma$.
 

::: {.example #exm-SAT}

**Q.** 	Before they were rescaled, SAT scores used to be approximately normally
distributed with a mean of 500 and a standard deviation of 100.

1. Approximately what percent of test takers scored between 400 and 600?
2. Approximately what percent of test takers scored above 600?
3. Approximately what percent of test takers scored below 300?
4. Approximately what percent of test takers scored between 400 and 700?


**A.**

1. 68\%
2. Since 68\% are bewteen 400 and 600, the other 32\% must be outside that
			range, half above and half below.  So 16\% are above 600.
3. Since 95\% are between 300 and 700, the other 5\% must be outside that 
			range, half above and half below.  So 2.5\% are below 300.
4. 16\% are below 400 and 2.5\% are above 700, so the remaining 81.5\% 
			must be between 400 and 700.

Of course, we can get more accurate results using R:

```{r }
pnorm( 600, 500, 100) - pnorm(400, 500, 100)
pnorm( 700, 500, 100) - pnorm(300, 500, 100)
pnorm( 300, 500, 100) 
pnorm( 700, 500, 100) - pnorm(400, 500, 100)
```


The `xpnorm()` function will additionally draw pictures of the normal 
distribution with a portion of the distribution shaded in.
```{r }
xpnorm(700,500,100) - xpnorm(400, 500, 100)
```

::: 
<!-- end example -->


::: {.example #exm-qnorm-SAT}

We can use `qnorm()` to compute percentiles.  For example, let's calculate
the 75th percentile for SAT distributions.

```{r }
qnorm(.75, 500, 100)
```

::: 
<!-- end example -->


### Beta Distributions


The Beta distributions have support on the interval $(0,1)$, so they can provide a model for proportions or other quantities that
are bounded between 0 and 1.^[A more general version of the Beta
distributions can do the same thing for quantities bounded by any two numbers.
This more general family of distributions has four parameters.]
The Beta distributions have two parameters, imaginatively 
called `shape1` and `shape2`.  The kernel of the Beta distributions
is a product of a power of $x$ and a power of $(1-x)$:
$$
k(x; \alpha, \beta) = x^{\alpha-1} (1-x)^{\beta -1} \; \boolval{ x \in [0,1] }
$$
When $\alpha = \beta$, the distribution is symmetric, and when
$\alpha = \beta =1$, we have the $\Unif(0,1)$-distribution.

The two shape parameters provide a wide variety of shapes.

```{r }
#| label: fig-beta-dists
#| fig-cap: Some example gamma distributions.
#| fig-subcap: ""
#| layout-ncol: 2
gf_dist("beta", params = list(shape1 = 2, shape2 = 2), title = "Beta(2,2)")
gf_dist("beta", params = list(shape1 = 2, shape2 = 0.9), title = "Beta(2,0.9)")
gf_dist("beta", params = list(shape1 = 4, shape2 = 2), title = "Beta(4,2)")
gf_dist("beta", params = list(shape1 = 0.9, shape2 = 0.85), title = "Beta(0.9,0.85)")
```


| Function                     | What it does                                                                                                                    |	
|------------------------------|---------------------------------------------------------------------------------------------------------------------------------|
| `dbeta(x, shape1, shape2)`   | Computes value of the pdf at `x`.                                                                                               |
| `pbeta(q, shape1, shape2)`   | Computes value of the cdf at `x`, i.e., $\Prob(X \le \texttt{q})$.                                                              |
|	`qbeta(p, shape1, shape2)`   | Computes quantiles, that is a value $q$ so that $\Prob(X \le \texttt{q}) = \texttt{p}$.                                         |
|	`rbeta(n, shape1, shape2)`   | Randomly samples `n` values from the $\Beta(\alpha, \beta)$ distribution where $\alpha$ = `shape1`  and $\beta$ = `shape2`.     |

: Beta distributions in R


::: {.problem #exr-plot-pdf-and-compute-mean-var}

Use R to plot the pdf and 
compute the mean and variance of each of the following distributions.

:::: {.enumerate}

a.  $\Beta(2,3)$
a.  $\Beta(20,30)$
a.  $\Gamm(\texttt{shape} = 2, \texttt{scale} = 3)$
a.  $\Weibull(\texttt{shape} = 2, \texttt{scale} = 3)$
:::: 
<!-- end enumerate -->

::: 
<!-- end problem -->



::: {.solution}

```{r }
# Beta(2,3)
m <- antiD( x * dbeta(x,2,3) ~ x, lower.bound = 0)(1)
m
antiD( x^2 * dbeta(x,2,3) ~ x, lower.bound = 0 )(1) - m^2
```

```{r }
# Beta(20,30)
m <- antiD( x * dbeta(x,20,30) ~ x, lower.bound = 0)(1)
m
antiD( x^2 * dbeta(x,20,30) ~ x, lower.bound = 0 )(1) - m^2
```

```{r }
# Gamma(2,scale = 3)
m <- antiD( x * dgamma(x,2,scale = 3) ~ x, lower.bound = 0)(Inf)
m
antiD( x^2 * dgamma(x,2,scale = 3) ~ x, lower.bound = 0 )(Inf) - m^2
```

```{r }
# Weibull(2,scale = 3)
m <- antiD( x * dweibull(x,2,scale = 3) ~ x, lower.bound = 0)(Inf)
m
antiD( x^2 * dweibull(x,2,scale = 3) ~ x, lower.bound = 0 )(Inf) - m^2
```

::: 
<!-- end solution -->



::: {.problem #exr-proportion-between}

For each of the following distributions, determine the proportion 
of the distribution that lies between 0.5 and 1.

:::: {.enumerate}

a.  $\Exp(\texttt{rate} = 2)$
b.  $\Beta(\texttt{shape1} = 3, \texttt{shape2} = 2)$
c.  $\Norm(\texttt{mean} = 1, \texttt{sd} = 2)$
d.  $\Weibull(\texttt{shape} = 2, \texttt{scale} = 1/2)$
e.  $\Gamm(\texttt{shape} = 2, \texttt{scale} = 1/2)$
:::: 
<!-- end enumerate -->

::: 
<!-- end problem -->


::: {.solution}

```{r }
pexp(1, 2) - pexp(0.5, 2)
pbeta(1, 3, 2) - pbeta(0.5, 3, 2)
pnorm(1, 1, 2) - pnorm(0.5, 1, 2)
pweibull(1, 2, scale = 1/2) - pweibull(0.5, 2, scale = 1/2)
pgamma(1, 2, scale = 1/2)   - pgamma(0.5, 2, scale = 1/2)
```

::: 
<!-- end solution -->


### Binomial Distributions

A binomial distribution is a discrete distribution with two parameters ($n$ and $p$,
or as R calls them \verb!size! and \verb!prob!) describing a situation in which

1.  Our random process consists of $n$ identical sub-processes (called trials).
2.  Each trial has one of two outcomes (traditionally called success and failure).
3.  The probability of success is $p$ for each trial.
4.  The outcome of each trial is independent of the others.

The binomial random varialbe counts the number of successes.

::: {.example #exm-freethrow-amy}

:::: {.enumerate}

a. If we flip a coin 100 times and let $X$ be the number of heads, then
$X \sim \Binom( 100, 0.5)$.
a. Amy is a 92% free throw shooter.  If she attempts 50 free throws and we let  -->
$Y$ be the number that she makes, then $Y \sim \Binom(50, 0.92)$ (assuming that 
each shot is independent of the others.^[Whether an individual shooter's 
shots are independent or exhibit longer runs of "hot" and "cold"
streaks that we would expect under indpendence has been investigated by 
many people.  The general conclusion seems to be that the independence 
assumption matches reality pretty closely.])
:::: 
<!-- end enumerate -->


Here are some example binomial distributions. The distributions are symmetric
when $p = 0.5$.  For a fixed size $n$, the distributions become
more and more skewed as $p$ gets closer to 0 or 1.  For a fixed probability
$p$, the distributions become more and more symmetric as $n$ gets larger.

```{r }
#| fig-cap: Some example binomial distributions.
#| fig-subcap: ""
#| layout-ncol: 2
gf_dist("binom", size = 10, prob = 0.5, title = "Binom(10, 0.5)")
gf_dist("binom", size = 10, prob = 0.05, title = "Binom(10, 0.05)")
gf_dist("binom", size = 100, prob = 0.5, title = "Binom(100, 0.5)")
gf_dist("binom", size = 100, prob = 0.05, title = "Binom(100, 0.05)")
```


The pmf for a binomial distribution is given by
$$
f(x) = \binom{n}{x} p^x (1-p)^{n-x}
$$
where $\binom{n}{x} = \frac{n!}{x!(n-x)!}$ is the binomial coefficient.
As with the continuous distributions, we have our usual functions available.

| Function                     | What it does                                                                                                |	
|------------------------------|-------------------------------------------------------------------------------------------------------------|
| `dbinom(x, size, prob)`      | Computes value of the pmf at `x`.                                                                           |
| `pbinom(q, size, prob)`      | Computes value of the cdf at `x`, i.e., $\Prob(X \le \texttt{q})$.                                          |
|	`qbinom(p, size, prob)`      | Computes quantiles, that is a value $q$ so that $\Prob(X \le \texttt{q}) = \texttt{p}$.                     |
|	`rbinom(n, size, prob)`      | Randomly samples `n` values from the $\Binom(n, \pi)$ distribution where $n$ = `size` and $\pi$ = `prob`.   |

::: 
<!-- end example -->



### Poisson Distributions

The Poisson distributions are generally used as models for counting "events" that
happen in a specified amount of time or space.  If the probability of an 
event happening at any moment is the same and independent of events happening
at other moments, then the count of events in a fixed amount of time will be 
a Poisson random variable.  The Poisson family has one parameter -- often denoted
$\lambda$ and called the *rate parameter* -- which is the average number
of events that happen over the fixed amount of time or space we are observing.

::: {.example #exm-geiger}

:::: {.enumerate}

a.  Let $X$ be the number of clicks of a Gieger counter in a 1 second interval.
Since each click corresponds to a radioactive decay event which we generally assume
occur "at random" but according to some average rate, a Poisson random variable
would be a good model for this.  The rate parameter would be the average number 
of decay events per second.

a.  If you stand on along a busy highway and count the number of red cars that go 
by in 30 minutes, a Poisson random variable might be a good model.  Sometimes 
you will get bunches of red cars or periods of time with few or no red cars,
that is just as a Poisson model predicts.
:::: 
<!-- end enumerate -->

::: 
<!-- end example -->


The Poisson distributions are skewed right, but become less and less skewed as 
the rate parameter increases.  Here are a few example plots.

```{r }
#| fig-cap: Some example Poisson distributions.
#| fig-subcap: ""
#| layout-ncol: 2
gf_dist("pois", lambda = 1, title = "Pois(1)")
gf_dist("pois", lambda = 5, title = "Pois(5)")
gf_dist("pois", lambda = 15, title = "Pois(15)")
gf_dist("pois", lambda = 50, title = "Pois(50)")
```


The pmf for a $\Pois(\lambda)$ random variable is 
$$
f(x) = \frac{ e^{-\lambda} \lambda^x}{x!}
$$
and the functions in R for working with Poisson distributions are 
the following:

| Function           | What it does                                                                                   |
|--------------------|------------------------------------------------------------------------------------------------|
| `dpois(x, lambda)` | Computes value of the pmf at `x`.                                                              |
| `ppois(q, lambda)` | Computes value of the cdf at `x`, i.e., $\Prob(X \le \texttt{q})$.                             |
| `qpois(p, lambda)` | Computes quantiles, that is a value $q$ so that $\Prob(X \le \texttt{q}) = \texttt{p}$.        |
| `rpois(n, lambda)` | Randomly samples `n` values from the $\Pois(\lambda)$ distribution where $\lambda$ = `lambda`. |

: Poisson distributions in R. 


## Fitting Distributions to Data

Suppose we think a family of distributions would make a good model for some 
situation.  How do we decide which member of the family to use?  The simple answer
is that we should choose the one that fits "best."  The trick is deciding what it 
means to fit well.  In fact there is more than one way to measure how well 
a distribution fits a data set.  


::: {.example #exm-windspeed}
We can use the following code to load a data set that contains three year's worth 
of mean hourly wind speeds (mph) in Twin Falls, ID.  This kind of data is often used
to estimate how much power could be generated from a windmill placed in a given location.

```{r include = !onLine,eval = !onLine}
Wind <- read.csv("data/stob/TwinfallsWind.csv")
head(Wind, 2)
tail(Wind, 2)
histogram( ~ speed, data = Wind, binwidth = 1 )
```

```{r include = onLine,eval = onLine}
Wind <- 
  read.csv("https://rpruim.github.io/Engineering-Statistics/data/stob/TwinfallsWind.csv")
head(Wind, 2)
tail(Wind, 2)
gf_histogram( ~ speed, data = Wind, binwidth = 1 )
```

As we can see, the distribution is skewed, but it doesn't look like an 
exponential distribution would be a good fit.  Of the distributions we have seen,
it seems like a Weibull or Gamma distribution would be a potentially good choice.
A Weibull model has often been used as a model for mean hourly wind speed, and the 
shape of our histogram indicates that this is a reasonable family of distributions.

**Q.** Which Weibull distribution is the best model for our data?

**A.** The `fitdistr()` in the **`MASS`** package uses the method of 
**maximum likelihood** to fit univariate (one variable) distributions.
```{r fitdistr-windspeed, error = TRUE}
fitdistr( Wind$speed, "weibull" )
```

For `fitdistr()` to fit a Weibull distribution, all of the data must be positive,
but our data includes some 0's.  
```{r zeros}
tally( ~ (speed == 0), data = Wind)
```

Let's see how small the smallest non-zero measurements 
are.
```{r }
min( ~ speed, data = Wind |> filter(speed > 0))
```

This may well be a simple rounding issue, since the wind speeds are recorded to the 
nearest 0.01 and 0.01 is the smallest positive value.
Let's create a new variable that moves each value of 0 to 0.0025 and try again.
Why 0.0025?  If we think that 0.01 represents anything in the range 0.005 to 0.015, which
would round to 0.01, then 0 represents anything in the range 0 to 0.005.  
0.0025 is the middle of that range.

```{r fitdistr-windspeed2, warning = FALSE}
Wind <- Wind |> mutate(speed2 = ifelse( speed > 0, speed, 0.0025))
fitdistr( Wind$speed2, "weibull" )
```

```{r include = FALSE}
mle <- fitdistr( Wind$speed2, "weibull" )
```

This says that the best fitting (in the sense of maximum likelihood) Weibull
distribution is the $\Weibull(`r paste(round(mle$est,2),sep = ",")`)$-distribution.

The `gf_histogram()` function has an option to overlay the distribution
fit by `fitdistr()` so we can see how good the fit is graphically.
```{r xhistogram-Wind,warning = FALSE}
gf_dhistogram( ~ speed2, data = Wind) |>
  gf_fitdistr( ~ speed2, data = Wind, dist = "weibull")
```


This can be abbreviated a bit:

```{r xhistogram-Wind-2,warning = FALSE}
gf_dhistogram( ~ speed2, data = Wind) |>
  gf_fitdistr(dist = "weibull")
```


`gf_fitdistr()` is inheriting the formula and data from `gf_dhistogram()`.
::: 
<!-- end example -->


::: {.example #exm-windspeed-gamma}

As an alternative, we could fit a Gamma distribution to the wind speed data.
```{r warning = FALSE}
fitdistr(Wind$speed2, "gamma")
gf_dhistogram( ~ speed2, data = Wind) |>
  gf_fitdistr(dist = "gamma" , color = ~ "Gamma") |>
  gf_fitdistr(dist = "weibull" , color = ~ "Weibull")
```


By eye, it appears that the Gamma distribution fits this data set slightly better, but
there may other reasons to prefer the Weibull distribution.  In fact,
there has been a good deal of research done regarding which distributions to use 
for wind speed data fitting.  The answer to the question of which distributions should 
be used seems to be that it depends on the purpose for your modeling:  
``The fact that different distributions excel under different applications
motivates further research on model selection based upon the engineering
parameter of interest." @Morgan2011:WindSpeed
::: 
<!-- end example -->


::: {.example #exm-jordan}

1986--87 was a good season for Michael Jordan, a famous former NBA basketball player.
Possible models for the points scored each game that season are normal, Weibull, and 
Gamma distributions.
The normal distributions might be a good choice if we think that the distributions
is roughly symetric (very good games are about the same amount above average as
the very poor games are below average).  Weibull and Gamma distributions have
the built in feature that scores cannot be negative and would allow for a
skewed distribution.
The `fitdistr()` function in the **`MASS`** package can fit each of these.
```{r jordan1, warning=FALSE}
library(fastR2)     # the Jordan8687 data set is in this package
fitdistr(Jordan8687$points, "normal")
fitdistr(Jordan8687$points, "weibull")
fitdistr(Jordan8687$points, "gamma")
```

We can use a histogram with overlaid density curve to see how well these fits compare 
to the data.
```{r jordan2, warning=FALSE}
gf_dhistogram(~ points, data = Jordan8687, binwidth = 5) |>
  gf_fitdistr(dist = "dnorm", color = ~"normal") |>
  gf_fitdistr(dist = "dweibull", color = ~ "Weibull") |>
  gf_fitdistr(dist = "dgamma", color = ~ "Gamma")
```

The three fits are similar, but not identical.  

::: 
<!-- end example -->

### Maximum Likelihood

The `fitdistr()` function uses the maximum likelihood method to estimate
distribution parameters.
The maximum likelihood method is one of the most commonly used estimation methods
in all of statistics because (1) it can be used in a wide range of applications,
and (2) the resulting estimators have some some desirable properties.  Maximum likelihood estimation tries to choose the parameter values that *maximize* the *likelihood* of the observed data.  

First, let's think about the "likelihood" of an individual observed data-point.  The likelihood of the data-point is just the probability density function (or probability mass function) for the distribution of interest, evaluated at the value observed in the data. The likelihood gives some indication of how frequently we'd expect to observe this value, but it is *not* a probability (for one thing, likelihoods can exceed 1).  The figure below illustrates that the likelihood of observing a person 80 inches (6 feet, 8 inches) tall, if the person comes from a population whose heights are Normally distributed with a mean of 68 inches and a standard deviation of 6 inches is about `r round(dnorm(80,mean=68, sd=6), digits=3)`: 
```{r likelihood, echo=FALSE, fig.keep="last"}
h <- 80; mu <- 68; sdev <- 6
gf_dist("norm", params=list(mean = mu, sd = sdev), title = "", 
         xlab = "Height (inches)", ylab = "Density or Likelihood") |>
  gf_segment(0 + dnorm(h, mean = mu, sd = sdev) ~ h + h) |>
  gf_point(dnorm(h, mean = mu, sd = sdev) ~  h, shape = "triangle")
```



Given a set of specific parameter values, the likelihood of an entire observed data-set can be calculated by obtaining the value of the likelihood of each observed data-point, and summing these over all the observed data points.  Then, we can find the maximum likelihood parameter estimates by trying many candidate parameter values until satisfied that we have found the ones that maximize the likelihood. (The numerical methods used are usually a bit more sophisticated than ``guessing lots of random candidate values", but we won't get into the details here. In some cases, it is also possible to write down a mathematical expression for the likelihood of the data given the parameters, and maximize it analytically.)

We'll illustrate the main ideas of maximum likelihood with a simple example.

::: {.example #exm-michael-3-dice}

Michael has three dice in his pocket.  One is a standard die with six sides,
another has four sides, and the third has ten sides.  He challenges you to a
game.  Without showing you which die he is using, Michael is going to roll a 
die 10 times and report to you how many times the resulting number is a 
$1$ or a $2$.  Your challenge is to guess which die he is using.

**Q.**  Michael reports that $3$ of the $10$ rolls resulted in a $1$ or a
$2$.  Which die do you think he was using?

**A.** 
The probability of obtaining a $1$ or a $2$ is one of $\frac12$,
$\frac13$, or $\frac15$, depending on which die is being used.  Our data
are possible with any of the three dice, but let's see how likely they are
in each case.

::: {.itemize}

* If $\evProb{roll 1 or 2}  = \frac15$, 
then the probability of obtaining exactly Michael's data is 
$$
\left(\frac15\right)^3 \left(\frac45\right)^7  =  `r (1/5)^3 * (4/3)^7`
\;.
$$

(Whatever the order, there will be 3 events with probability
$1/5$ and 7 with probability $4/5$.  Since the events are independent,
we can multiply all of these probabilities.)

* If $\evProb{roll 1 or 2}  = \frac13$, 
then the probability of obtaining exactly Michael's data is 
$$
\left(\frac13\right)^3 \left(\frac23\right)^7 
=  `r (1/3)^3 * (2/3)^7`
\;.
$$

* If $\evProb{roll 1 or 2}  = \frac12$, 
then the probability of obtaining exactly Michael's data is 
$$
\left(\frac12\right)^3 \left(\frac12\right)^7 = `r .2^3 * .8^7`
\;.
$$
::: 
<!-- end itemize -->

Of these, the largest likelihood is for the case that 
$\evProb{roll 1 or 2}  = \frac13$, i.e.,
for the standard, six-sided die.  Our data would be more likely
to occur with that die than with either of the other two -- it is the 
maximum likelihood die.
::: 
<!-- end example -->


In general, maximum likelihood calculations are harder because instead of having
only 3 choices, there will be infinitely many choices, and instead of having only
one parameter, there may be multiple parameters.  So techniques from (multi-variable) 
calculus or numerical approximation methods are often used to maximize the likelihood function.
The `fitdistr()` function uses pre-derived formulas for some distributions
and numerical approximation methods for others.  In some cases, you will get warning
messages about attempts to apply a function to values that don't make sense (trying to
take logs or square roots of negative numbers, zero in the denominator, etc.) as the 
numerical approximation algorithm explores options in an attempt to find the best fit.
The help documenation for `fitdistr()` explains which distributions it can 
handle and what method is used for each.

### The method of moments
An easy (but sometimes fairly crude) way to estimate the parameters of a distribution
is the method of moments.  You will often see this method used in engineering textbooks,
espeically if they do not rely on software that implements others methods (like the 
maximum likelihood method).

The basic idea is to set up a system of  equations where we set the mean of the data equal to the mean of the distribution, the variance of the data equal to the variance of the distribution, etc.^[If our distribution has more than 2 parameters, we will need higher moments, which we will not cover here.]  

To employ this method, we need to know the means and variances of our favorite families of distributions (in terms of the parameters of the distributions).  For all of the distributions we have seen, one can work out formulas for the means and variances in terms of the parameters involved.  These are listed in @tbl-cont-dist

::: {.example #exm-windspeed-2}

Let's return to the wind speeds in @exm-windspeed.
The formulas for the mean and variance of a Weibull distribution involve
the gamma function $\Gamma()$, which might be unfamiliar to you.  So let's 
simplify things.

Theoretical properties and observations of wind speeds at other locations 
suggest that using a shape parameter of $\alpha = 2$ is often a good choice (but 
shape does differ from location to location depending on how consistent or 
variable the wind speeds are).
The Weibull distributions with $\alpha = 2$ have a special name, they are 
called the **Rayleigh** distributions.  
So $\Rayleigh(\beta) = \Weibull(\alpha = 2, \beta)$.
In this case, from @tbl-cont-dist, we see that to calculate 
the mean we need the value of $\Gamma(1 + \frac{1}{2}) = \Gamma(1.5) = \sqrt{\pi}/2$.
```{r }
gamma(1.5)
sqrt(pi)/2
```

From @tbl-cont-dist we see that the mean of a 
$\Rayleigh(\beta)$-distribution is 
$$
\E(X) = \beta \frac{\sqrt{\pi}}{2}
$$

Now we can choose our estimate $\hat \beta$ for $\beta$ so that 
$$
	\hat \beta \frac{\sqrt{\pi}}{2} = \overline x  ;.
$$
That is,
$$
	\hat\beta = \frac{2 \overline x }{\sqrt{\pi}}
$$

```{r }
x.bar <- mean(~speed, data = Wind) 
x.bar
beta.hat <- x.bar * 2 / sqrt(pi)
beta.hat 
```

So our method of moments fit for the data is a 
$\Rayleigh(`r round(beta.hat,2)`) = \Weibull(2, `r round(beta.hat,2)`)$

Although the Rayleigh distributions are not as flexible as the Weibull or Gamma 
distributions, and although maximum likelihood is generally preferred over the
method of moments, the method of moments fit of a Rayleigh distribution does have
one advantage: it can be computed even if all you know is the mean of some sample data.
Sometimes, that is all you can easily get your hands on (because the people who collected
the raw data only report numerical summaries).  You can find average wind speeds of for 
many locations online, for example here:
<http://www.wrcc.dri.edu/htmlfiles/westwind.final.html>
::: 
<!-- end example -->



::: {.example #exm-mom-2-params}

For distributions with two parameters, we solve a system of two equations with two unknowns.
For the normal distributions this is particularly easy since the parameters are the mean
and standard deviation, so we get

$$
\begin{aligned}
\hat\mu &= \mean x\\
\hat\sigma^2 &= s_x^2\\
\end{aligned}
$$

```{r tidy = FALSE}
x.bar <- mean(~speed, data = Wind); x.bar
v <- var(~speed, data = Wind); v
sqrt(v)
```

So the method of moments suggests a $\Norm(`r round(x.bar,2)`, `r round(sqrt(v),2)`)$
distribution.  In this case, the method of moments and maximum likelihood methods
give the same results. 

```{r }
fitdistr(Wind$speed, "normal")
```

But this doesn't mean that the fit is particularly good.  Indeed, a normal distribution is 
not a good choice for this data.  We know that wind speeds can't be negative and we 
have other distributions (exponential, Weibull, and Gamma, for example) that are also
never negative.  So choosing one of those seems like a better idea.
The following plot shows, as we expected, that the normal distribution is not a particularly
good fit.
```{r }
histogram(~speed, data = Wind, fit = "normal")
```

It is important to remember that the best fit using a poor choice for the family
of distriubtions might not be a useful fit.  
The choice of distributions is made based on a combination of theoretical 
considerations, experience from previous data sets, and the quality of 
the fit for the data set at hand.
::: 
<!-- end example -->

+---------------+---------------+--------------------+
| Fruit         | Price         | Advantages         |
+===============+===============+====================+
| Bananas       | $1.34         | - built-in wrapper |
|               |               | - bright color     |
+---------------+---------------+--------------------+
| Oranges       | $2.10         | - cures scurvy     |
|               |               | - tasty            |
+---------------+---------------+--------------------+

: Some (families of) continuous distributions. {#tbl-cont-dist}

<!-- \begin{table} -->
<!-- \begin{center} -->
<!-- \small -->
<!-- \begin{tabular}{|p{5mm}p{40mm}p{25mm}p{45mm}|} -->
<!-- \hline -->
<!-- \multicolumn{2}{|l}{\textbf{\sf distribution} } -->
<!-- && -->
<!-- \\ -->
<!--   & \textbf{\sf pdf or pmf}  -->
<!--   & \textbf{\sf mean}  -->
<!--   & \textbf{\sf variance}  -->
<!-- \\[.5mm] \hline -->
<!-- &&& \\[-2mm] -->
<!-- \multicolumn{2}{|l}{  Triangle: $\Tri(a,b,c)$} && \\ -->
<!--   &  -->
<!--   $\displaystyle  -->
<!--  	 \begin{cases} -->
<!-- 		 \frac{2(x-a)}{(b-a)(c-a)}  &  \mbox{if $x \in [a,c]$} \;,  \\ -->
<!-- 		 \frac{2(b-x)}{(b-a)(b-c)}  &  \mbox{if $x \in [c,b]$} \;,  \\ -->
<!-- 		0 & \mbox{otherwise} \\ -->
<!-- 	  \end{cases}$ -->
<!-- 	  & $\displaystyle \frac{a + b + c}{3}$  -->
<!-- 	  & $\displaystyle \frac{a^2 + b^2 + c^2 - ab -ac -bc}{18}$ -->
<!-- 	  \\[5.2mm] -->
<!-- 	  \multicolumn{2}{|l}{  Uniform: $\Unif(a,b)$} && \\ -->
<!--   &  -->
<!--   $\displaystyle  -->
<!--  	 \begin{cases} -->
<!-- 		\frac{1}{b-a}  &  \mbox{if $x \in [a,b]$} \;,  \\ -->
<!-- 		0 & \mbox{otherwise} \\ -->
<!-- 	  \end{cases}$ -->
<!-- 	  & $\displaystyle \frac{b+a}{2}$  -->
<!-- 	  & $\displaystyle \frac{(b-a)^2}{12}$ -->
<!-- 	  \\[5.2mm] -->
<!-- \multicolumn{2}{|l}{  Standard normal: $\Norm(0,1)$ } && \\ -->
<!--   &  -->
<!--   $\displaystyle \frac{1}{\sqrt{2\pi}} {e^{-\frac12 z^2}}$ -->
<!--   	& $0$ & $1$ -->
<!-- \\[4.2mm] -->
<!-- \multicolumn{2}{|l}{  Normal: $\Norm(\mu,\sigma)$ } && \\ -->
<!--   & $ -->
<!-- 	 \displaystyle \frac{1}{\sigma\sqrt{2\pi}} \cdot  -->
<!--      		e^{-\frac12 (\frac{x-\mu} -->
<!-- 			{\sigma})^2}$ -->
<!--   	& $\mu$ & $\sigma^2$ -->
<!-- \\[5.2mm] -->
<!-- \multicolumn{2}{|l}{  Exponential: $\Exp(\lambda)$ }  && \\ -->
<!-- %  	& $\lambda$ -->
<!-- 	& $\lambda e^{-\lambda x}$ -->
<!--   	& $1/\lambda$ & $1/\lambda^2$ -->
<!-- \\[4.2mm] -->
<!-- \multicolumn{2}{|l}{  Gamma: $\Gamm(\alpha, \lambda = \frac{1}{\beta})$ } && \\ -->
<!-- %  	& $\lambda$ -->
<!-- 	& $\displaystyle \frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\lambda x}$ -->
<!--   	& $\alpha/\lambda = \alpha \beta$  -->
<!-- 	& $\alpha/\lambda^2 = \alpha \beta^2$ -->
<!-- \\[4.2mm] -->
<!-- \multicolumn{2}{|l}{Weibull: $\Weibull(\alpha,\beta = \frac{1}{\lambda})$} && \\ -->
<!-- & -->
<!-- 	  $\displaystyle \frac{\alpha}{\beta^\alpha} x^{\alpha-1} e^{-(x/\beta)^\alpha}$ -->
<!-- 	&  -->
<!-- 	  $\beta \Gamma(1 + \frac1{\alpha})$ -->
<!-- 	& -->
<!-- 	  $\beta^2 \left[ \Gamma(1 + \frac{2}{\alpha})  -->
<!-- 	  - \left[ \Gamma(1 + \frac{1}{\alpha}) \right]^2 \right] -->
<!-- 	  $  -->
<!-- \\[4.0mm] -->
<!-- \multicolumn{2}{|l}{  Beta: $\Beta(\alpha, \beta)$ } && \\ -->
<!-- 	&  -->
<!--   $\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}  -->
<!-- 		x^{\alpha-1}(1-x)^{\beta-1}$ -->
<!--   	&  -->
<!-- 	  $\displaystyle \frac{\alpha}{\alpha + \beta}$ -->
<!-- 	  & -->
<!-- 	  $\displaystyle \frac{\alpha \beta }{(\alpha + \beta)^2(\alpha + \beta + 1)}$ -->
<!-- \\[4mm] -->
<!-- \hline -->
<!-- \multicolumn{2}{|l}{  Binomial: $\Binom(n, p)$ } && \\ -->
<!-- 	&  -->
<!-- 	$\displaystyle \binom{n}{x} p^x (1-p)^{n-x}$ -->
<!-- 	& -->
<!-- 	$np$ -->
<!-- 	& -->
<!-- 	$np(1-p)$ -->
<!-- \\[4mm] -->
<!-- \multicolumn{2}{|l}{  Poisson: $\Pois(\lambda)$ } && \\ -->
<!-- 	&  -->
<!-- 	$\displaystyle \frac{e^{-\lambda} \lambda^x} {x!}$ -->
<!-- 	& -->
<!-- 	$\lambda$ -->
<!-- 	& -->
<!-- 	$\lambda$ -->
<!-- \\[4mm] -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- \caption{Some common continuous distributions. -->
<!-- Standard names for parameters that appear in several distributions -->
<!-- include \texttt{rate} ($\lambda$), \texttt{shape} ($\alpha$), and \texttt{scale} ($\beta$). -->
<!-- In the normal distributions, $\mu$ and $\sigma$ are called \texttt{mean} and \texttt{sd} -->
<!-- in \R, and in the uniform distirbutions, $a$ and $b$ are called \texttt{min} and \texttt{max}. -->
<!-- The function $\Gamma(x)$ that appears in the formulas for the Weibull and Beta -->
<!-- distributions is a kind of continuous extrapolation from the factorial function. -->
<!-- The \texttt{gamma()} function will calculate these values.} -->
<!-- \label{tbl-cont-dist} -->
<!-- \end{table} -->


## Quantile-Quantile Plots

To this point we have looked at how well a distribution fits the data by overlaying a density
curve on a histogram.  While this is instructive, it is not the easiest way to make a 
graphical comparison between a data set and a theoretical distribution.   Our eyes
are much better and judging whether something is linear than they are at judging whether 
shapes have a particular kind of curve.  Furthermore, certain optical misperceptions
tend to cause people to exaggerate some kinds of differences and underestimate others.

Quantile-quantile plots offer an alternative approach.  As the name suggests, the idea is 
to compare the quantiles of our data to the quantiles of a theoretical distribution.  These
are then plotted as a scatter plot.  Let's go through those steps with a small data
set so we can see all the moving parts, then we'll learn how to automate the whole
process using `gf_qq()`.

### Normal-Quantile Plots

The normal distributions are especially important for statistics, so normal-quantile
plots will be our most important example of quantile-quantiles plots.  Also, special
properties of the normal distributions make normal-quantile plots especially easy
and useful.  We will illustrate the construction of these plots using a data set
containing Michael Jordan's game by game scoring output from the 1986--87 basketball
season.

::: {.example #exm-jordan-qq}

Let's begin by forming a randomly selected sample of 10 basketball games.
```{r }
set.seed(123)              # so you can get the same sample if you like.
SmallJordan <- sample(Jordan8687, 10)
SmallJordan
```


```{r tidy = FALSE}
probs <- seq(0.05, 0.95, by = 0.10)
probs
observed <- sort(SmallJordan$points)                                    # sorted observations
theoretical <- qnorm( probs, mean = mean(observed), sd = sd(observed) ) # theoretical quantiles

QQData <- data.frame(observed = observed, theoretical = theoretical)
QQData
```

If the observed data matched the theoretical quantiles perfectly, a scatter plot
would place all the points on the line with slope 1 passing through the origin.

```{r hand-qqplot}
gf_point( observed ~ theoretical, data = QQData, title = "Hand made QQ-plot" ) |>
  gf_fun( x ~ x, alpha = 0.6, color = "blue", linetype = "dashed")
```


Even better, we don't need to know the mean and standard deviation in advance, because
all normal distributions are linear transformations of the $\Norm(0,1)$-distribution.
So our standard practice will be to compare our data to the $\Norm(0,1)$-distribution.
If $X \sim \Norm(\mu,\sigma)$, then $X = \mu + \sigma Z$ where $Z \sim \Norm(0,1)$, so 
a plot of $X$ vs. $Z$ will have slope $\sigma$ and intercept $\mu$.

```{r }
theoretical2 <- qnorm( probs, mean = 0, sd = 1 ) # theoretical quantiles from Norm(0,1)
QQData2 <- data.frame(observed = observed, theoretical = theoretical2)
gf_point(observed ~ theoretical, data = QQData2, title = "Hand made QQ-plot", xlab = "theoretical (z)" ) |>
  gf_abline(intercept = ~ mean(SmallJordan$points), slope = ~ sd(SmallJordan$points), 
            alpha = 0.5, color = "navy", data = NA) |>
  gf_hline(yintercept = ~ mean(SmallJordan$points), alpha = 0.5) |>
  gf_vline(xintercept = ~ 0, alpha = 0.5)
```


This whole process is automated by the `gf_qq()` function.
```{r }
gf_qq( ~ points, data = SmallJordan, title = "Sub-sample" )
gf_qq( ~ points, data = Jordan8687, title = "Full data set" )
```

::: 
<!-- end example -->


### Other distributions

Working with other distributions is similar, but most families of distributions
don't have a single "master example" to which we can make all comparisons, so we
need to pick a particular member of the family (either by fitting or for some 
theoretical reason).^[There are a few other families of distributions that
have a prototypical member such that all other members are a linear transformation
of the prototype.  The exponential family is one such family.]

::: {.example #exm-windspeed-qq}

	Let's build a quantile-quantile plot for our wind speed data comparing 
	to normal, gamma and Weibull distributions.
<!-- \iffalse -->
<!-- 	As in the previous section, we'll begin by creating it manually using  -->
<!-- 	a small subset of the data and then show how to automate the process  -->
<!-- 	using \function{gf_qq()}. -->
<!-- <<>>= -->
<!-- SmallWind <- sample(Wind, 10) -->
<!-- SmallWind -->
<!-- @ -->
<!-- Now we need to compute the quantiles of our data and the quantiles of our  -->
<!-- theoretical distribution.  We start by selecting 10 evenly spaces  -->
<!-- proportions. -->
<!-- <<>>= -->
<!-- probs <- seq(0.05, 0.95, by = 0.10) -->
<!-- probs -->
<!-- @ -->
<!-- Now we compute the quantiles for these probabilities using the parameters -->
<!-- we fit previously. -->
<!-- <<tidy = FALSE>>= -->
<!-- y <- qdata( probs, SmallWind$speed2)                     # quantiles from data -->
<!-- x.gamma <- qgamma( probs, shape = 2.496, rate = 0.421 )      # quantiles from Gamma dist -->
<!-- x.weibull <- qweibull( probs, shape = 1.694, scale = 6.651 ) # quantiles from Weibull dist -->
<!-- x.normal <- qnorm( probs, mean = 5.925 , sd = 3.653  )       # quantiles from Normal dist -->
<!-- @ -->
<!-- Finally, we create the scatter plot -->
<!-- <<>>= -->
<!-- gf_point(y ~ x.gamma) -->
<!-- gf_point(y ~ x.weibull) -->
<!-- gf_point(y ~ x.normal) -->
<!-- @ -->
<!-- \fi -->
We can automate this, but we need to tell `gf_qq()` how to calculate
the quantiles.
```{r }
gf_qq( ~ speed2, data = Wind)  # normal-quantile plot; normal is not a good model
```


The normal model does not fit well, but both Gamma and Weibull are reasonable models:
```{r tidy = FALSE, warning = FALSE}
fitdistr(Wind$speed2, "gamma")
fitdistr(Wind$speed2, "Weibull")
fittedqgamma <- makeFun( qgamma(p, shape = 2.496, rate = 0.421 ) ~ p )
fittedqweibull <- makeFun( qweibull(p, shape = 1.694, scale = 6.651) ~ p ) 
gf_qq( ~speed2, data = Wind, distribution = fittedqgamma )
gf_qq( ~speed2, data = Wind, distribution = fittedqweibull )
```

::: 
<!-- end example -->



{{< pagebreak >}}

## Exercises

::: {.problem #exr-twin-falls}

:::: {.enumerate}

a.  Using @tbl-cont-dist and the method of moments,
fit an exponential distribution to the Twin Falls wind speed data.

```{r }
Wind <- 
  read.csv("https://rpruim.github.io/Engineering-Statistics/data/stob/TwinfallsWind.csv")
```


What is the estimated value of the rate parameter?

b.  Now use `fitdistr()` to fit an exponential
	distribution using maximum likelihood.
	
c.  How do the two estimates for the rate parameter compare?

d.  How well does an exponential distribution fit this data?
:::: 
<!-- end enumerate -->

::: 
<!-- end problem -->


::: {.solution}

:::: {.enumerate}

a.  The method of moments fit for $\lambda$ comes from solving 
			$\frac{1}{\lambda} = \mean x$ for $\lambda$, so 
			$\hat \lambda = \frac{1}{\mean x}$
```{r }
lambda.hat <- 1/ mean(~speed, data = Wind)
lambda.hat
```

b. 
```{r }
fitdistr(Wind$speed, "exponential")
```

c.  They are the same in this case.

d.  This fit is not that great.

```{r }
gf_dhistogram(~speed, data = Wind, fit = "exponential")
gf_qq( ~ speed, data = Wind, distribution = qexp)
```

:::: 
<!-- end enumerate -->

::: 
<!-- end solution -->


::: {.problem #exr-gamma-mom}

	A Gamma distribution can also be fit using the method of moments.
	Because there are two parameters (shape and rate or shape and scale),
	you will need to solve a system of two equations with two unknowns.
:::: {.enumerate}

a. Using @tbl-cont-dist and the method of moments,
fit a Gamma distribution to the Twin Falls wind speed data.
What are the estimated values of the shape and rate parameters?
b. How do the method of moments estimates for the parameters compare to the 
	maximum likelihood estimates from `fitdistr()`?
:::: 
<!-- end enumerate -->

::: 
<!-- end problem -->

::: {.solution}

```{r tidy = FALSE,warning = FALSE}
Wind <- Wind |> mutate(speed2 = ifelse( speed > 0, speed, 0.0025))
m <- mean(~speed2, data = Wind); m
v <- var(~speed2, data = Wind); v
fitdistr(Wind$speed2, "gamma")
```

Now we solve

$$
\begin{aligned}
\alpha \beta & = \mean x  = `r mean(~speed2, data = Wind)`
\\
\alpha \beta^2 & = s^2 = `r var(~speed2, data = Wind)`
\end{aligned}
$$

Dividing the second by the first gives $\hat \beta = \sfrac{s^2}{\mean x}$.
From this we obtain $\hat \alpha = \mean{x} / \hat \beta = \sfrac{ \mean x^2}{s^2}$.
```{r tidy = FALSE}
beta.hat <- v/m; beta.hat
alpha.hat <- m / beta.hat ; alpha.hat
lambda.hat <- 1/ beta.hat ; lambda.hat
```

The fitted values are similar to but not identical to the maximum likelihood estimates.
::: 
<!-- end solution -->


::: {.problem #exr-windspeed-sam}

Sam has found some information about wind speed at a location he
is interested in online.  Unfortunately, the web site only provides
the mean and standard deviation of wind speed.  

:::: {.center}

+---------------------+----------+
| mean:               | 10.2 mph |
+---------------------+----------+
| standard deviation: | 5.1 mph  |
+---------------------+----------+
:::: 
<!-- end center -->

:::: {.enumerate}

a. Use this information and the method of moments to estimate the 
	shape and rate parameters of a Gamma distribution.
a. 	In principal, we could do the same for a Weibull distribution, but the 
	formulas aren't as easy to work with. 
	Fit a Rayleigh distribution instead (i.e., a Weibull
	distribution with shape parameter equal to 2).
:::: 
<!-- end enumerate -->

::: 
<!-- end problem -->


::: {.solution}

We can recycle some work from the previous problem to quickly obtain
the method of moments fit for the Gamma distribution:
```{r tidy = FALSE}
m <- 10.2
v <- 5.1^2 
beta.hat <- v/m; beta.hat
alpha.hat <- m / beta.hat ; alpha.hat
lambda.hat <- 1/ beta.hat ; lambda.hat
```

For the Rayleigh distribution we solve 
$\hat \beta \frac{\sqrt{\pi}}{2} = \mean x$   for 
$\hat \beta$ and get
$$
	\hat\beta = \frac{2 \mean x }{\sqrt{\pi}}
	= \frac{2 \cdot 10.2 }{\sqrt{\pi}}
	= `r 2 * 10.2 / sqrt(pi)`
$$	
::: 
<!-- end solution -->


::: {.problem #exr-iq}

In 1964, a study was undertaken to see if IQ at 3 years of age is
	associated with amount of crying at newborn age. In the study, 38 newborns
	were made to cry after being tapped on the foot, and the number of distinct
	cry vocalizations within 20 seconds was counted.
	The subjects were followed up at 3 years of age and their IQs were measured.
<!-- 	The data from this study are in the `Baby` data frame.   -->
You can load this data using
```{r Baby}
Baby <- read.csv("https://rpruim.github.io/Engineering-Statistics/data/BabyCryIQ.csv")
head(Baby)
```


	The `cry.count` variable records the number of distinct cry vocalizations 
	within 20 seconds.  Choose a family of distributions to fit to this data
	and do the fit using `fitdistr()`. Also include a plot showing 
	a histogram and your fitted density curve.
::: 
<!-- end problem -->


::: {.solution}

	The distribution is skewed and non-negative, so a gamma or Weibull seems like 
	a good thing to try.  
```{r }
fitdistr(Baby$cry.count, "gamma")
fitdistr(Baby$cry.count, "Weibull")
gf_dhistogram( ~ cry.count, data = Baby, binwidth = 1) |>
  gf_fitdistr( ~ cry.count, data = Baby, "gamma", title = "Gamma")
gf_dhistogram(~cry.count, data = Baby, title = "Weibull") |>
  gf_fitdistr(~ cry.count, data = Baby, "dweibull")
```

::: 
<!-- end solution -->


<!-- \iffalse -->
<!-- \begin{problem} -->
<!-- 	For each of the following, give a family of distributions -->
<!-- 	that might make a reasonable model and say breifly why you chose that family -->
<!-- 	\begin{enumerate} -->
<!-- 		\item -->
<!-- 			Batting averages of MIAA base ball players.  (A batting average is roughly -->
<!-- 			the proportion of at bats where the batter gets a hit.) -->

<!-- 	\end{enumerate} -->
<!-- \end{problem} -->
<!-- \fi -->

::: {.problem #exr-HELPrct-substance-qq}

Create normal quantile plots for the ages of patients in the `HELPrct`
	data set separated by `substance`. (Getting separate or overlaid plots
	using `gf_qq()` works just like it does for other **`ggformula`** plots).

	Comment on the plots.
{{< pagebreak >}}

::: 
<!-- end problem -->


::: {.solution}

```{r }
gf_qq( ~ age | substance, data = HELPrct)
```

The qq-plot for the alchol group looks good.  The other two (especially cocaine) show
signs of skew -- indicated by the curve to the qq plot.
```{r }
gf_dens( ~ age | substance, data = HELPrct)
```

::: 
<!-- end solution -->



::: {.problem #exr-qq-matching}

Match the normal-quantile plots to the histograms.

```{r echo = FALSE}
rescale <- function(x,lim = c(0,10)) {
	return ( min(lim) + (x - min(x))/ (diff(range(x))) * (diff(range(lim))) );
}

n <- 400;

a <- qnorm(ppoints(n));
a <- rescale(a);

b <- qbeta(ppoints(n),20,5);
b <- rescale(b);

c <- qexp(ppoints(n),2);
c <- qbeta(ppoints(n),3,15);
c <- rescale(c);

d <- c(runif(n = n/2, min = 0,max = 10), qunif(ppoints(n/2),0,10) );
d <- rescale(d);

# bowl shaped
e <- 10 * c(rbeta(500,6,1),rbeta(500,1,6));
e <- c( qbeta(ppoints(100),6,1), qbeta(ppoints(100),1,6) );
e <- rescale(e);

#f <- 10 * c(0,1,qbeta(ppoints(n-2),14,15));
#f <- rescale(f)
f <- qexp(ppoints(n),2);
f <- rescale(f)

D1 <- data.frame(A = c,B = a,C = b,D = d,E = e,F = f);

sD1 <- stack(D1);

D2 <- data.frame(V = a,Z = b,Y = c,W = d,X = e,U = f);

sD2 <- stack(D2);
```


```{r compareplots1,echo = FALSE,fig.height = 3.0}
gf_dhistogram(~ values | ind, 
	data = sD1, as.table = TRUE,
	breaks = seq(-0.5,10.5,by = .5),
	size = 0.3
	)
gf_qq(~values | ind, data = sD2, as.table = TRUE)
```


::: 
<!-- end problem -->


::: {.solution}

a) Y  b) V c) Z d) W  e) X f) U
::: 
<!-- end solution -->


::: {.problem #exr-var-shortcut}

Show that $\Var(X) = \E(X^2) - \E(X)^2$ by showing that 
$$
	\int_{-\infty}^{\infty} (x - \mu_X) ^2 f(x) \; dx
	=
	\int_{-\infty}^{\infty} x^2 f(x) \; dx  -  \mu_X^2
$$
whenever $f$ is a pdf and all the integrals involved converge.
::: 
<!-- end problem -->


::: {.solution}

Some algebra and properties of integrals are all we need:

$$
\begin{aligned}
\int_{-\infty}^{\infty} (x - \mu_X) ^2 f(x) \; dx
	&=
	\int_{-\infty}^{\infty} (x^2 - 2\mu_X x + \mu_X^2) f(x) \; dx
	\\
	&=
	\int_{-\infty}^{\infty} x^2 f(x) \; dx
	- 2 \mu_X \int_{-\infty}^{\infty} x f(x) \; dx
	+ \mu_X^2 \int_{-\infty}^{\infty} f(x) \; dx
	\\
	&=
	\Var(X) - 2 \mu_X^2 + \mu_X^2
	\\
	&=
	\Var(X) - \E(X)^2 
\end{aligned}
$$

::: 
<!-- end solution -->


::: {.problem #exr-heights}

The heights of 18--22 year olds in the US follow approximately normal distributions
within each sex.  Estimated means and standard deviations appear in the table below.
	

::::{.center}	
+------------+----------------+-------------------------+
+            + mean           + standard deviation.     +
+============+================+=========================+
+ women      + 64.3 in        + 2.6 in                  +
+------------+----------------+-------------------------+
+ men        + 70.0 in        + 2.8 in                  +
+------------+----------------+-------------------------+
::::

Answer the following questions without using a computer or calculator (except for basic
arithmetic).
	
:::: {.enumerate}

a.  If a woman is 68 inches tall, what is her z-score?
b.  If a man is 74 inches tall, what is his z-score?
c.  What is more unusual, a woman who is at least 68 inches tall
			or a man who is at least 74 inches tall?
d.  Big Joe has decided to open a club for tall people.  To join his club,
			you must be in the tallest 2.5% of people of your sex. 
			How tall must a woman be to join Big Joe's club?
e.  How tall must a man be to join Big Joe's club?
:::: 
<!-- end enumerate -->

::: 
<!-- end problem -->


::: {.solution}

:::: {.enumerate}

a. 
```{r }
(68 - 64.3)/2.6
```


b. 
```{r }
(74 - 70) / 2.8
```

c. 
It's pretty close, but the z-score for the man is slightly
larger, so it is slightly more unusual for a man to be that tall.

d. 
```{r }
64.3 + 2 * 2.6
```


e. 
```{r }
70 + 2 * 2.8
```

:::: 
<!-- end enumerate -->

::: 
<!-- end solution -->


::: {.problem #exr-heights-more}

Use the information from @exr-heights to answer the following questions.

:::: {.enumerate}

a.  What proportion of women are 5'10" or taller?
b.  What proportion of men are 6'4" or taller?
c.  If a man is in the 75th percentile for height, how tall is he?
d.  If a woman is in the 30th percentile for height, how tall is she?
:::: 
<!-- end enumerate -->

::: 
<!-- end problem -->


::: {.solution}

:::: {.enumerate}

#. 
```{r }
1 - pnorm(70, mean = 64.3, sd = 2.6)
```


#. 
```{r }
1 - pnorm(76, mean = 70, sd = 2.8)
```


#. 
```{r }
qnorm(.75, mean = 70, sd = 2.8)
```


#. 
```{r }
qnorm(.30, mean = 64.3, sd = 2.6)
```


:::: 
<!-- end enumerate -->

::: 
<!-- end solution -->


