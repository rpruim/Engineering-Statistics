[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for the Physical Sciences and Engineering",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "00-chap0.html#thought-experiment-how-many-dimes",
    "href": "00-chap0.html#thought-experiment-how-many-dimes",
    "title": "Where do numbers come from?",
    "section": "Thought experiment – How many dimes?",
    "text": "Thought experiment – How many dimes?\nHere’s a thought experiment for you. Suppose a middle school class has collected a large number of dimes (10-cent coinds) in a sack. Before bringing the money to the bank, they would like to estimate how many dimes they have (using tools and methods that 6th graders have at their disposal). You’ve been brought in to consult with them about how they should do this.\n\n\nWhat method would you suggest? Why?\nWhat other methods would be possible? What makes your proposed method better?\nFor your favorite method and others, identify factors that lead the resulting estimate to be different from the exact number of dimes in the sack."
  },
  {
    "objectID": "00-chap0.html#some-important-terms",
    "href": "00-chap0.html#some-important-terms",
    "title": "Where do numbers come from?",
    "section": "Some important terms",
    "text": "Some important terms\n\n\nestimand/measureand The number we want to know. The “truth.” In our example this is the number of dimes in the bag. Typically this will be a number that describes some process or population, and typically it will be impossible to know the value exactly.\nestimate/measurement The value calculated from our data. This may be as simple as recording a value reported by some device, or it may involve recording multiple values, perhaps of multiple variables, maybe at multiple times, and making some computations with that data.}\nerror The difference between the estimate and the estimand. Because we don’t know the estimand exactly, we can’t know the error exactly either. But thinking about what the error could be is a big part of understanding the statistical properties of an estimation method. Generally, we want methods where errors tend to be small (so our estimate is “likely to be close to the estimand”) and centered around 0 (so we’re “right on average”).\nsystematic (component of) error A component of error that makes our estimate biased – in other words, leads the estimate to be either an over- or under-estimate. For example, neglecting the weight of the sack would lead us to overestimate the weight of the dimes, and therefore overestimate the number of dimes. Another way to express this idea is “a tendency to be off in a certain direction.”\nrandom (component of) error A component of error that leads to variability in estimates (but not a particular tendency toward over- or under-estimation). If random errors are larger, there will be more variability in estimates, so we will be less confident that the estimand and estimate are close together – although some estimates may still be very close to the estimand, just by chance. \n\nOne of the big questions in statistics is this:\n\nWhat does our estimate tell us about the estimand?\n\nWe will eventually learn techniques for quantifying (and attempting to reduce) the effects of error in our measurements."
  },
  {
    "objectID": "01-graphical.html#getting-started-with-rstudio",
    "href": "01-graphical.html#getting-started-with-rstudio",
    "title": "1  Graphical Summaries of Data",
    "section": "1.1 Getting Started With RStudio",
    "text": "1.1 Getting Started With RStudio\n\n\n\nWelcome to RStudio\n\n\nRStudio is an integrated development environment (IDE) for R a freely available language and environment for statistical computing and graphics. Both are freely available for Mac, PC, and Linux.\nIn addition to running RStudio on your local machine, you have the option of accessing an RStudio server via a web browser. (For best results, avoid Internet Explorer.)\n\n1.1.1 Using R as a calculator\nNotice that RStudio divides its world into four panels. Several of the panels are further subdivided into multiple tabs. The console panel is where we type commands that R will execute.\nR can be used as a calculator. Try typing the following commands in the console panel.\n\n5 + 3\n\n[1] 8\n\n15.3 * 23.4\n\n[1] 358.02\n\nsqrt(16)\n\n[1] 4\n\n\nYou can save values to named variables for later reuse\n\nproduct = 15.3 * 23.4       # save result\nproduct                     # show the result\n\n[1] 358.02\n\nproduct <- 15.3 * 23.4      # <- is assignment operator, same as =\nproduct\n\n[1] 358.02\n\n15.3 * 23.4 -> newproduct   # -> assigns to the right\nnewproduct\n\n[1] 358.02\n\n.5 * product                # half of the product\n\n[1] 179.01\n\nlog(product)                # (natural) log of the product\n\n[1] 5.880589\n\nlog10(product)              # base 10 log of the product\n\n[1] 2.553907\n\nlog(product, base = 2)        # base 2 log of the product\n\n[1] 8.483896\n\n\nThe semi-colon can be used to place multiple commands on one line. One frequent use of this is to save and print a value all in one go:\n\n15.3 * 23.4 -> product; product    # save result and show it\n\n[1] 358.02\n\n\n\n\n1.1.2 Loading packages\nR is divided up into packages. A few of these are loaded every time you run R but most have to be selected. This way you only have as much of R as you need.\nIn the Packages tab, check the boxes next to the following packages to load them: \n\nmosaic (a package from Project MOSAIC)\nDAAG (a package that goes with the book Data Analysis and Graphic)\n\n\nYou an also load packages by typing, for example\n\nlibrary(DAAG)       # loads the DAAG package if it is not already loaded\n\n\n\n1.1.3 Four Things to Know About R\n\n1. R is case-sensitive\nIf you mis-capitalize something in R it won’t do what you want. If you get an error message about an “object not found”, check to see that you spelled it correctly.\n\n\n2. Functions in R use the following syntax:\n\nfunctionname( argument1, argument2, ... )\n\n\nThe arguments are always surrounded by (round) parentheses and separated by commas.\nSome functions (like data()) have no required arguments, but you still need the parentheses.\nIf you type a function name without the parentheses, you will see the code for that function – which probably isn’t what you want at this point.\n\n\n\n3. TAB completion and arrows can improve typing speed and accuracy.\nIf you begin a command and hit the TAB key, R will show you a list of possible ways to complete the command. If you hit TAB after the opening parenthesis of a function, it will show you the list of arguments it expects. The up and down arrows can be used to retrieve past commands.\n\n\n4. Hit ESCAPE to break out of a mess.\nIf you get into some sort of mess typing (usually indicated by extra ‘\\(+\\)’ signs along the left edge, indicating that R is waiting for more input – perhaps because you have some sort of error in what has gone before), you can hit the escape key to get back to a clean prompt."
  },
  {
    "objectID": "01-graphical.html#data-in-r",
    "href": "01-graphical.html#data-in-r",
    "title": "1  Graphical Summaries of Data",
    "section": "1.2 Data in R",
    "text": "1.2 Data in R\n\n1.2.1 Data Frames\nMost often, data sets in R are stored in a structure called a data frame. A data frame is designed to hold “rectangular data”. The people or things being measured or observed are called observational units (or subjects or cases when they are people). Each observational unit is represented by one row. The different pieces of information recorded for each observational unit are stored in separate columns, called variables.\n\n\n1.2.2 Data in Packages\nThere are a number of data sets built into R and many more that come in various add on packages.\nYou can see a list of data sets in a particular package like this:\n\nlibrary(mosaicData)            # load the package\ndata(package = \"mosaicData\")   # see what data sets are in it\n\nYou can find a longer list of all data sets available in any loaded package using\n\ndata()\n\n\n\n1.2.3 The HELPrct data set\nThe HELPrct data frame from the mosaic package contains data from the Health Evaluation and Linkage to Primary Care randomized clinical trial. You can find out more about the study and the data in this data frame by typing\n\n?HELPrct\n\nAmong other things, this will tell us something about the subjects (observational units) in this study:\n\nEligible subjects were adults, who spoke Spanish or English, reported alcohol, heroin or cocaine as their first or second drug of choice, resided in proximity to the primary care clinic to which they would be referred or were homeless. Patients with established primary care relationships they planned to continue, significant dementia, specific plans to leave the Boston area that would prevent research participation, failure to provide contact information for tracking purposes, or pregnancy were excluded.\n\n\nSubjects were interviewed at baseline during their detoxification stay and follow-up interviews were undertaken every 6 months for 2 years.\n\nIt is often handy to look at the first few rows of a data frame. It will show you the names of the variables and the kind of data in them:\n\nhead(HELPrct)\n\n\n\n  \n\n\n\nWhen there are a lot of variables, this format can be hard to read. The glimpse() or inspect() functions provide some other options.\n\nglimpse(HELPrct)\n\nRows: 453\nColumns: 30\n$ age              <int> 37, 37, 26, 39, 32, 47, 49, 28, 50, 39, 34, 58, 58, 6…\n$ anysubstatus     <int> 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 0, 1, 1, 1, 1, 1, …\n$ anysub           <fct> yes, yes, yes, yes, yes, yes, NA, yes, yes, yes, NA, …\n$ cesd             <int> 49, 30, 39, 15, 39, 6, 52, 32, 50, 46, 46, 49, 22, 36…\n$ d1               <int> 3, 22, 0, 2, 12, 1, 14, 1, 14, 4, 0, 3, 5, 10, 2, 6, …\n$ daysanysub       <int> 177, 2, 3, 189, 2, 31, NA, 47, 31, 115, NA, 192, 6, 6…\n$ dayslink         <int> 225, NA, 365, 343, 57, 365, 334, 365, 365, 382, 365, …\n$ drugrisk         <int> 0, 0, 20, 0, 0, 0, 0, 7, 18, 20, 8, 0, 0, 0, 0, 0, 0,…\n$ e2b              <int> NA, NA, NA, 1, 1, NA, 1, 8, 7, 3, NA, NA, NA, 1, NA, …\n$ female           <int> 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,…\n$ sex              <fct> male, male, male, female, male, female, female, male,…\n$ g1b              <fct> yes, yes, no, no, no, no, yes, yes, no, no, no, no, n…\n$ homeless         <fct> housed, homeless, housed, housed, homeless, housed, h…\n$ i1               <int> 13, 56, 0, 5, 10, 4, 13, 12, 71, 20, 0, 13, 20, 13, 5…\n$ i2               <int> 26, 62, 0, 5, 13, 4, 20, 24, 129, 27, 0, 13, 31, 20, …\n$ id               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17…\n$ indtot           <int> 39, 43, 41, 28, 38, 29, 38, 44, 44, 44, 34, 11, 40, 4…\n$ linkstatus       <int> 1, NA, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0…\n$ link             <fct> yes, NA, no, no, yes, no, no, no, no, no, no, no, no,…\n$ mcs              <dbl> 25.111990, 26.670307, 6.762923, 43.967880, 21.675755,…\n$ pcs              <dbl> 58.41369, 36.03694, 74.80633, 61.93168, 37.34558, 46.…\n$ pss_fr           <int> 0, 1, 13, 11, 10, 5, 1, 4, 5, 0, 0, 13, 13, 1, 1, 7, …\n$ racegrp          <fct> black, white, black, white, black, black, black, whit…\n$ satreat          <fct> no, no, no, yes, no, no, yes, yes, no, yes, no, yes, …\n$ sexrisk          <int> 4, 7, 2, 4, 6, 5, 8, 6, 8, 0, 2, 0, 1, 4, 8, 3, 4, 4,…\n$ substance        <fct> cocaine, alcohol, heroin, heroin, cocaine, cocaine, c…\n$ treat            <fct> yes, yes, no, no, no, yes, no, yes, no, yes, yes, no,…\n$ avg_drinks       <int> 13, 56, 0, 5, 10, 4, 13, 12, 71, 20, 0, 13, 20, 13, 5…\n$ max_drinks       <int> 26, 62, 0, 5, 13, 4, 20, 24, 129, 27, 0, 13, 31, 20, …\n$ hospitalizations <int> 3, 22, 0, 2, 12, 1, 14, 1, 14, 4, 0, 3, 5, 10, 2, 6, …\n\ninspect(HELPrct)\n\n\ncategorical variables:  \n       name  class levels   n missing\n1    anysub factor      2 246     207\n2       sex factor      2 453       0\n3       g1b factor      2 453       0\n4  homeless factor      2 453       0\n5      link factor      2 431      22\n6   racegrp factor      4 453       0\n7   satreat factor      2 453       0\n8 substance factor      3 453       0\n9     treat factor      2 453       0\n                                   distribution\n1 yes (77.2%), no (22.8%)                      \n2 male (76.4%), female (23.6%)                 \n3 no (72%), yes (28%)                          \n4 housed (53.9%), homeless (46.1%)             \n5 no (62.2%), yes (37.8%)                      \n6 black (46.6%), white (36.6%) ...             \n7 no (71.5%), yes (28.5%)                      \n8 alcohol (39.1%), cocaine (33.6%) ...         \n9 no (50.3%), yes (49.7%)                      \n\nquantitative variables:  \n               name   class       min        Q1    median        Q3       max\n1               age integer 19.000000  30.00000  35.00000  40.00000  60.00000\n2      anysubstatus integer  0.000000   1.00000   1.00000   1.00000   1.00000\n3              cesd integer  1.000000  25.00000  34.00000  41.00000  60.00000\n4                d1 integer  0.000000   1.00000   2.00000   3.00000 100.00000\n5        daysanysub integer  0.000000   5.00000  33.00000 164.25000 268.00000\n6          dayslink integer  2.000000  74.00000 361.00000 365.00000 456.00000\n7          drugrisk integer  0.000000   0.00000   0.00000   1.00000  21.00000\n8               e2b integer  1.000000   1.00000   2.00000   3.00000  21.00000\n9            female integer  0.000000   0.00000   0.00000   0.00000   1.00000\n10               i1 integer  0.000000   3.00000  13.00000  26.00000 142.00000\n11               i2 integer  0.000000   4.00000  18.00000  33.00000 184.00000\n12               id integer  1.000000 119.00000 233.00000 348.00000 470.00000\n13           indtot integer  4.000000  32.00000  38.00000  41.00000  45.00000\n14       linkstatus integer  0.000000   0.00000   0.00000   1.00000   1.00000\n15              mcs numeric  6.762923  21.67575  28.60242  40.94134  62.17550\n16              pcs numeric 14.074291  40.38438  48.87681  56.95329  74.80633\n17           pss_fr integer  0.000000   3.00000   7.00000  10.00000  14.00000\n18          sexrisk integer  0.000000   3.00000   4.00000   6.00000  14.00000\n19       avg_drinks integer  0.000000   3.00000  13.00000  26.00000 142.00000\n20       max_drinks integer  0.000000   4.00000  18.00000  33.00000 184.00000\n21 hospitalizations integer  0.000000   1.00000   2.00000   3.00000 100.00000\n          mean          sd   n missing\n1   35.6534216   7.7102660 453       0\n2    0.7723577   0.4201653 246     207\n3   32.8476821  12.5144598 453       0\n4    3.0596026   6.1875894 453       0\n5   75.3073770  79.2374134 244     209\n6  255.6055684 151.0226916 431      22\n7    1.8871681   4.3365219 452       1\n8    2.5046729   2.5245232 214     239\n9    0.2362031   0.4252180 453       0\n10  17.9072848  20.0202390 453       0\n11  24.5474614  28.0201504 453       0\n12 233.4017660 134.7467214 453       0\n13  35.7284768   7.1522016 453       0\n14   0.3781903   0.4854990 431      22\n15  31.6766783  12.8393373 453       0\n16  48.0485416  10.7846027 453       0\n17   6.7064018   3.9950056 453       0\n18   4.6423841   2.8001526 453       0\n19  17.9072848  20.0202390 453       0\n20  24.5474614  28.0201504 453       0\n21   3.0596026   6.1875894 453       0\n\n\nFrom this we see that there are 453 observational units in this data set and 30 variables. That’s plenty of variables to get us started with exploration of data.\n\n\n1.2.4 The KidsFeet data set\nHere is another data set in the mosaic package:\n\nhead(KidsFeet)\n\n\n\n  \n\n\n\n\n\n1.2.5 The oldfaith data set\nA final example data set comes from the alr4 package. This package is probably not loaded (unless you already loaded it). You can load it from the Packages tab or by typing the command\n\nlibrary(alr4)      # require(alr4) will also work\n\nOnce you have done that, you will have access to the data set containing information about Old Faithful eruptions.\n\n\n\n\nhead(oldfaith)\n\n\n\n  \n\n\n\nIf you want to know the size of your data set, you can ask it how many rows and columns it has with nrow(), ncol(), or dim():\n\nnrow(oldfaith)\n\n[1] 270\n\nncol(oldfaith)\n\n[1] 2\n\ndim(oldfaith)\n\n[1] 270   2\n\n\nIn this case we have 270 observations of each of two variables (the length of an eruption and the time until the next eruption). In a data frame, the observational units are always in the rows and the variables are always in the columns. If you create data for use in R (or most other statistical packages), you need to make sure your data are also in this shape.\n\n\n1.2.6 Using your own data\nIn the Environment tab you will “Import Dataset”. Click on this import data from a CSV file, Excel spreadsheet, or a few other formats. When you do this, the R code will be displayed, so you can see how it is done in R code.\nIf you are using the RStudio server, you will first need to upload your file to the server (unless you can access the file via URL). To do this, choose “Upload” from the Files tab."
  },
  {
    "objectID": "01-graphical.html#graphical-and-numerical-summaries-of-data",
    "href": "01-graphical.html#graphical-and-numerical-summaries-of-data",
    "title": "1  Graphical Summaries of Data",
    "section": "1.3 Graphical and Numerical Summaries of Data",
    "text": "1.3 Graphical and Numerical Summaries of Data\n\n1.3.1 The Most Important Template\nUsing the mosiac and ggformula packages, we can compute a wide variety of graphical and numerical summaries using the following general template:\n\n\n\n\nThe most important template\n\n\n\nWe will see this same template used again for linear and non-linear modeling as well, so it is is important to master it.1\n\ngoal: The name of the function generally describes your goal, the thing you want the computer to produce for you. In the case of plotting, it is the name of the plot. When we do numerical summaries it will be the name of the numerical summary (mean, median, etc.).\ny: For plots, this is the variable that goes on the y-axis.\nx: For plots, this is the variable that goes on the x-axis.\nformula: Together, y ~ x is called a formula. Very often we can think of y ~ x as “y depends on x”. We will see that sometimes we can omit y ore replace x with . (there must always be something on the right-hand side). We will even see things like y ~ x | z. But the most important formula to learn is y ~ x.\nmydata: A data frame must be given in which the variables mentioned in the formula can be found. Variables not found there will be looked for in the enclosing environment. Sometimes we will take advantage of this to avoid creating a temporary data frame just to make a quick plot, but generally it is best to have all the information inside a data frame."
  },
  {
    "objectID": "01-graphical.html#scatterplots",
    "href": "01-graphical.html#scatterplots",
    "title": "1  Graphical Summaries of Data",
    "section": "1.4 Scatterplots",
    "text": "1.4 Scatterplots\nThe most common way to look at two quantitative variables is with a scatter plot. The ggformula function for this is gf_point(), and the basic syntax is\n\ngf_point( yvar ~ xvar, data = dataName)\n\nLet’s look at an example. Let’s see how bill length is related to body mass in some penguins.\n\nlibrary(palmerpenguins)\nhead(penguins) \n\n\n\n\n\n\nFigure 1.1: A scatter plot of penguin data.\n\n\ngf_point(bill_length_mm ~ body_mass_g, data = penguins)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 1.2: A scatter plot of penguin data.\n\n\n\n\nThat’s all there is to it. We can replace bill_length_mm, body_mass_g, and penguins with any variables and data set we like to get the scatter plot we want.\n\n1.4.1 Adding Color\nLet’s add some color. Consider the next two examples.\n\ngf_point(bill_length_mm ~ body_mass_g, color = \"navy\", data = penguins)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\ngf_point(bill_length_mm ~ body_mass_g, color = ~ species, data = penguins)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nFigure 1.3: Setting color\n\n\n\n\n\n\n\nFigure 1.4: Mapping color\n\n\n\n\n\n\n\n\nIn the first we are setting the color of the dots to be navy.\nIn the second, we are mapping color based on species. Think of color = ~ species as “color depends on species”. \n\nThe warnings, by the way, are indicating that two rows of data were not used in making the plot because they were missing data needed for the plot (either bill_length_mm or body_mass_g).\n\n\nCode\n# This is a bit of bonus code, \n# in case you are interested to see how we find these two penguins.\npenguins |> \n  filter(is.na(bill_length_mm) | is.na(body_mass_g))\n\n\n\n\n  \n\n\n\n\n\n1.4.2 Transparency and dot size\nWith so much data in so little space, overplotting (dots on top of each other) can make it hard to see what is going on. We can improve this plot by making the dots smaller and semi-transparent.\n\ngf_point(bill_length_mm ~ body_mass_g, \n         color = ~ species, data = penguins,\n         size = 0.8, alpha = 0.6)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAdjustments to alpha and size.\n\n\n\n\nThere are many other options we can use to refine our plots. We’ll learn about some of them as we go along. You can use R ’s built-in help to find out more. Our you can type\n\ngf_point()\n\ngf_point() uses \n    * a formula with shape y ~ x. \n    * geom:  point \n    * key attributes:  alpha, color, size, shape, fill, group, stroke\n\nFor more information, try ?gf_point\n\n\n\n\n1.4.3 Conditional plots (aka Faceting)\nThe formula for a ggformula plot can be extended to create multiple panels, called facets, based on a “condition”, often given by another variable. The general syntax for this becomes\n\nplotname( y ~ x | condition, data = dataName )\n\nYou can read the formula y ~ x | condition as saying that we want to know how y depends on x separately for each condition. In our penguins example, we might divide up the data according to the islands on which the penguins were spotted.\n\ngf_point(bill_length_mm ~ body_mass_g | island, \n         color = ~ species, data = penguins,\n         size = 0.8, alpha = 0.5)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 1.5: Using facets to separate the species.\n\n\n\n\n\n\n1.4.4 Other types of plots\nA scatter plot will be our most common plot for two quantitative variables, but we can use the same template for any other type of plot. Here are two examples.\n\ngf_density2d(bill_length_mm ~ body_mass_g, color = ~ species, data = penguins, alpha = 0.5)\n\nWarning: Removed 2 rows containing non-finite values (`stat_density2d()`).\n\ngf_hex(bill_length_mm ~ body_mass_g, data = penguins, binwidth = c(250, 2))\n\nWarning: Removed 2 rows containing non-finite values (`stat_binhex()`).\n\n\n\n\n\n\n\n\n(a) A 2-d density plot.\n\n\n\n\n\n\n\n(b) A hex plot.\n\n\n\n\nFigure 1.6: More plots of penguins."
  },
  {
    "objectID": "01-graphical.html#graphing-the-distribution-of-one-variable",
    "href": "01-graphical.html#graphing-the-distribution-of-one-variable",
    "title": "1  Graphical Summaries of Data",
    "section": "1.5 Graphing the Distribution of One Variable",
    "text": "1.5 Graphing the Distribution of One Variable\nA distribution is described by telling what values occur and with what frequency. That is, the distribution answers two questions: \n\nWhat values?\nHow often? \n\nStatisticians have devised a number of graphs to help us see distributions of a variable visually. In these graphs, R can compute the y-variable for us. In this case, we simply omit the y part of the formula, so the general syntax for making a graph or numerical summary of one variable in a data frame is\n\nplotname( ~ variable, data = dataName )\n\nIn other words, there are three pieces of information we must provide to R in order to get the plot we want:\n\n\nThe kind of plot. (gf_histogram(), gf_bar(), gf_density(), gf_boxplot(), etc.)\nThe name of the variable to plot.\nThe name of the data frame this variable is a part of.\n\n\nNote: The same syntax works for numerical summaries as well – thanks to the mosaic package we can apply the same syntax for mean(), median(), sd(), var(), max(), min(), etc. Later we will use this syntax again to compute linear and nonlinear models.\n\n1.5.1 Histograms (and density plots) for quantitative variables\nHistograms (and density plots) are the two most common ways of displaying the distribution of a quantitative variable.\nHere are a couple examples:\n\ngf_histogram( ~ Duration, data = oldfaith)\ngf_histogram( ~ age, data = HELPrct)\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 1.7: Example histograms.\n\n\n\nIn each of these plots the height of the bar indicates how many observations fall within the range indicated by the bottom of the bar. So in the histogram below, the red bar indicates that there are almost 60 eruptions of duration between 100 and 125 seconds in this data set.\n\n\n\n\n\nFigure 1.8: The red bar indicates that there are almost 60 eruptions of duration between 100 and 125 seconds in this data set.\n\n\n\n\nWe can control the (approximate) number of bins using the bins argument. The number of bins (and to a lesser extent the positions of the bins) can make a histogram look quite different.\n\ngf_histogram( ~ Duration, data = oldfaith, bins = 8 )\ngf_histogram( ~ Duration, data = oldfaith, bins = 15 )\ngf_histogram( ~ Duration, data = oldfaith, bins = 30 )\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\nFigure 1.9: Histograms with different numbers of bins.\n\n\n\nWe can use binwidth to set the width of the bins.\n\ngf_histogram( ~ Duration, data = oldfaith, binwidth = 60 )\ngf_histogram( ~ Duration, data = oldfaith, binwidth = 20 )\ngf_histogram( ~ Duration, data = oldfaith, binwidth = 5 )\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\nFigure 1.10: Histograms with different bin widths.\n\n\n\nR also provides a “smooth” version called a density plot and a triangular version called a frequency polygon:;\n\ngf_density( ~ Duration, data = oldfaith )\ngf_dens( ~ Duration, data = oldfaith )\ngf_freqpoly( ~ Duration, data = oldfaith )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\nFigure 1.11: Density plots and frequency polygons.\n\n\n\n\n\n1.5.2 Describing the shape of a distribution\nIf we make a histogram of our data, we can describe the overall shape of the distribution. Keep in mind that the shape of a particular histogram may depend on the choice of bins. Choosing too many or too few bins can hide the true shape of the distribution. (When in doubt, make more than one histogram.)\nHere are some words we use to describe shapes of distributions.\n\n\nsymmetric The left and right sides are mirror images of each other.\nskewed The distribution stretches out farther in one direction than in the other. (We say the distribution is skewed toward the long tail.)\nuniform The heights of all the bars are (roughly) the same. (So the data are equally likely to be anywhere within some range.)\nunimodal There is one major “bump” where there is a lot of data.\nbimodal There are two “bumps”.\noutlier An observation that does not fit the overall pattern of the rest of the data. \n\nWe’ll learn about another graph used for quantitative variables (boxplots) soon.\n\n\n1.5.3 Bar graphs for categorical variables\nBar graphs are a way of displaying the distribution of a categorical variable.\n\ngf_bar( ~ species, data = penguins)   # vertical bars\ngf_bar(species ~ ., data = penguins)  # horizontal bars\n\n\n\n\n\n\n\n(a) vertical bars\n\n\n\n\n\n\n\n(b) horizontal bars\n\n\n\n\nFigure 1.12: Bar graphs\n\n\n\nStatisticians rarely use pie charts because they are harder to read except in a few special cases (like comparing a proportion to 50%).\n\n\n1.5.4 Overlaying and faceting data\nOverlaying and faceting work the same for these one variables plots as they did for the scatterplots above.\n\ngf_bar( ~ species | island, data = penguins)\ngf_histogram( ~ body_mass_g | species, data = penguins)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\ngf_dens( ~ body_mass_g, color = ~ species, data = penguins)\n\nWarning: Removed 2 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\nFigure 1.13: Overlaying and faceting in plots\n\n\n\nFor example, we might like to see how the ages of men and women compare in the HELP study, or whether the distribution of lengths of boys’ feet is different from the distribution for girls.\n\ngf_histogram( ~ age | sex, data = HELPrct, binwidth = 5)\ngf_dens( ~ length | sex, data = KidsFeet )\n\n\n\n\n\n\n\n\n\n\n\nWe can do the same thing for bar graphs.\n\ngf_bar( ~ substance | sex, data = HELPrct)\n\n\n\n\nFigure 1.14: Facets in a bar graph.\n\n\n\n\n\n\n1.5.5 Grouping and bar charts\nWhen dividing bar charts into multiple colors, we can present the segmented bars “stacked” (the default) or “dodged”:\n\ngf_bar(~substance, fill = ~sex, data = HELPrct)\ngf_bar(~substance, fill = ~sex, data = HELPrct, position = \"dodge\")\n\n\n\n\n\n\n\n(a) Stacked bar graph.\n\n\n\n\n\n\n\n(b) Dodged bar graph.\n\n\n\n\nFigure 1.15: We can show different groups using stacked or dodged bar graphs.\n\n\n\n\n\n1.5.6 Proportions and bar charts\nSometimes it is better to display bars with proportions rather than counts. But then we must decide what to use for the denominator. In the first example below, the total of all the bars adds to 1. In the second plot, the total adds to one for each x variable, which makes it easier to see how the proportions of male and female .\n\ngf_props(~substance, fill = ~sex, data = HELPrct)\ngf_props(~substance, fill = ~sex, data = HELPrct, denom = ~ x)"
  },
  {
    "objectID": "01-graphical.html#labeling-plots",
    "href": "01-graphical.html#labeling-plots",
    "title": "1  Graphical Summaries of Data",
    "section": "1.6 Labeling plots",
    "text": "1.6 Labeling plots\nOften the defaults labels are not ideal for publication purposes. You can add titles and captions and change the labeling of variables using gf_labs().\n\ngf_point(bill_length_mm ~ body_mass_g | island, \n         color = ~ species, data = penguins,\n         size = 0.8, alpha = 0.6) |>\n  gf_labs(x = \"body mass (g)\", y = \"bill length (mm)\", \n          title = \"Penguin measurments on 3 islands\", caption = \"Data Source: palmerpenguins\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 1.16: Informative labels and make plots easier to read.\n\n\n\n\nNotice the |> (called the pipe symbol and sometimes read “then”) in the example above. This important and connects the labeling below to the plot above."
  },
  {
    "objectID": "01-graphical.html#exporting-plots",
    "href": "01-graphical.html#exporting-plots",
    "title": "1  Graphical Summaries of Data",
    "section": "1.7 Exporting Plots",
    "text": "1.7 Exporting Plots\nYou can save plots to files or copy them to the clipboard using the Export menu in the Plots tab. It is quite simple to copy the plots to the clipboard and then paste them into a Word document, for example. You can even adjust the height and width of the plot first to get it the shape you want. But there are much better ways to produce documents with R graphics in them! See the next section."
  },
  {
    "objectID": "01-graphical.html#reproducible-documents",
    "href": "01-graphical.html#reproducible-documents",
    "title": "1  Graphical Summaries of Data",
    "section": "1.8 Reproducible Documents",
    "text": "1.8 Reproducible Documents\nCopy-and-paste is a bad workflow for lots of reasons, including: \n\nIt is tedious, unless there is very little to copy and paste.\nIt is error-prone – it’s easy to copy to little or too much, or to grab the wrong thing, or to copy when you want to cut or cut when you want to copy.\nIf something changes, you have to start all over.\nYou have no record of what you did (unless you are an unusual person who takes detailed notes about all the copying and pasting).  So while copy-and-paste seems easy and convenient, it is not reproducible. Reproducibility is important when projects are large, when it is important to have record of exactly what was done, or when the same analysis is applied to multiple data sets (or a data set that is growing over time).\n\nRStudio makes it easy to use techniques of reproducible research to create documents that include text, R commands, R output, and R graphics.\n\n1.8.1 Quarto\nThe simplest version of this uses a format called Quarto.2 Quarto is a simple mark up language that allows for a few basic improvements on plain text (section headers, bulletted lists, numbered lists, bold, italics, etc.). Quarto adds the ability to mix in the R stuff. (And it has some fancier stuff, like embedding videos.) The end product can be an HTML file, a PDF document, even a Word or PowerPoint document. HTML is especially good for producing web documents, but it is also useful as preview mode, even if you eventually render to PDF (or Word).3\n\n1.8.1.1 Creating a new document\nTo create a new Quarto document, go to “File”, “New File”, then “Quarto document”.\n\n\n\n\n\nWhen you do this, a file editing pane will open with a simple example quarto file inserted. If you click on “Render”, RStudio will turn this into an HTML file and display it for you. Give it a try. You will be asked to name your file if you haven’t already done so. If you are using the RStudio server in a browser, then your file will live on the server (“in the cloud”) rather than on your computer.\n\n\n\n1.8.2 Three kinds of sections\nIf you look at the example file you will see that the file has three kinds of sections.\n\n1.8.2.1 YAML header\nAt the top of the document is the YAML (Yet Another Markup Language) header. This is where you set things like the author and title for your document and specify certain settings that control how the document is processed.\nIt is important that this section is syntactically correct (mathing quatoation marks, correct indentation, etc.). If you get an error message with YAML or yaml in it, you will know that something has gone wrong in the YAML header.\n\n\n\n\n\n\nFixing YAML errors\n\n\n\nIf you can’t figure out the problem with your YAML header, copy and paste in the YAML from some other document (a previous document you have made or a new one) and edit from there.\n\n\n\n\n1.8.2.2 Text blocks\nSome of this file is just normal text (with some styling to make headers, bold, italics, etc.)\n\n\n1.8.2.3 Code chunks\nThe third type of section is an R code chunk. These are colored differently to make them easier to see. You can insert a new code chunk by clicking on the icon pictured below.\n\n\n\n\n\nYou can put any R code in these code chunks and the results (text output or graphics) as well as the R code will be displayed in your HTML file.4\n\n\n1.8.2.4 Source vs Visual view\nRStudio provides two different views of Quarto files. The default is the Visual view. This looks a bit like a simplified version of Word. There is a toolbar you can use to create bold, italics, headers, etc.\nIf you click on Source in the upper left corner, you will get a different view. This will show your documents as Markdown. You will see that in this view, headers are indicated with #, ##, ###, etc. *italics* turns into italics, and **bold** turns into bold.\nYou can get a list of all of these mark up shortcuts by selecting the “Markdown Quick Reference” in the help menu.\n\n\n\n\n\nIf you type these while in the Visual view, the will be immediately converted over for you.\n\n\n\nQuarto files must be self-contained\nQuarto files do not have access to things you have done in your console. (This is good, else your document would change based on things not in the file.) This means that you must explicitly load any data or packages that you use in the Quarto file. In this class, this means that most of your Quarto files will have a chunk near the beginning that includes\n```{r}\n#| include: false\nlibrary(mosaic)\n```\nThe special #| include: false comments tells Quarto to run the code, but not to include anything in your document. That way you don’t have to see all of the package loading messages in your document.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.8.3 Chunk options\nQuarto provides a number of chunk options that control how R code is processed. You can use them to do things like:\n\nrun the code without displaying it (good for polished reports – your client doesn’t want to see the code)\nshow the code without running it – mainly useful for demonstration purposes\nset the size and alignment of graphics\n\nYou can set default values for the chunk options and you can also override them in individual chunks. See https://quarto.org/docs/computations/execution-options.html for more information about chunk options.\nThe default plots are often bigger than required. You can adjust this by sitting the fig-width and fig-height chunk options. They can be adjusted as necessary.\n\n```{r}\n#| fig-width: 4\n#| fig-height: 3\n#| fig-align: center\ngf_histogram(~ Duration, data = oldfaith)\n```\n\n\n\n\n\n\n\n\n\n```{r}\n#| fig-width: 8\n#| fig-height: 2\n#| fig-align: center\ngf_histogram(~ Duration, data = oldfaith)\n```\n\n\n\n\n\n\n\n\nYou can set default sizes for the entire document by putting them in your YAML header at the top of the document:\n\nformat:\n  html:\n    fig-width: 8\n    fig-height: 2\n    fig-align: center\n  pdf:\n    fig-width: 4\n    fig-height: 3\n    fig-align: center"
  },
  {
    "objectID": "01-graphical.html#a-few-bells-and-whistles-for-plotting",
    "href": "01-graphical.html#a-few-bells-and-whistles-for-plotting",
    "title": "1  Graphical Summaries of Data",
    "section": "1.9 A Few Bells and Whistles for Plotting",
    "text": "1.9 A Few Bells and Whistles for Plotting\nThere are lots of arguments that control how plots look. Here are just a few examples, some of which we have already seen.\n\n\n\n\n\n\n\n\n1.9.1 alpha, size\nSometimes it is nice to have elements of a plot be partly transparent. When such elements overlap, they get darker, showing us where data are “piling up.” Setting the alpha argument to a value between 0 and 1 controls the degree of transparency: 1 is completely opaque, 0 is invisible. The size argument controls the size of lines and points.\nHere is another example using data on 150 iris plants of three species.\n\ngf_point(Sepal.Length ~ Sepal.Width, color = ~ Species, data = iris, \n    alpha = .6, size = 1.8) \n\n\n\n\nFigure 1.17: 150 iris plants from 3 species.\n\n\n\n\n\n\ntitle, subtitle, caption, xlab, ylab\nYou can add a title or subtitle, or change the default labels of the axes.\n\ngf_point(Sepal.Length ~ Sepal.Width, color = ~Species, data = iris, \n    title = \"Some Iris Data\",\n    subtitle = \"(R. A. Fisher analysized this data in 1936)\",\n    caption = \"Source: R's built-in iris data set\",\n    xlab = \"sepal width (cm)\",\n    ylab = \"sepal length (cm)\",\n    alpha = .6 \n)\n\n\n\n\nFigure 1.18: We can add labels using title, subtitle, xlab, and ylab. (Using gf_labs() is another way to do this.)\n\n\n\n\n\nlinetype, linewidth, fill, shape\nThese can be used to change the line type, line width, plot symbol, and color of filled in regions. \n\ngf_dens( ~age, data = HELPrct, color = ~ sex, linetype = ~ sex)\ngf_dens( ~age, data = HELPrct, color = ~ sex, linetype = \"dotted\")\ngf_histogram( ~ age, data = HELPrct, fill = 'steelblue')\n\n\n\n\n\n\n\n(a) Mapping linetype\n\n\n\n\n\n\n\n\n\n(b) Setting linetype\n\n\n\n\n\n\n\n\n\n(c) Use fill, rather than color, for filled in regions.\n\n\n\n\nFigure 1.19: Examples using linetype and fill.\n\n\n\nYou can see a list of the hundreds of available color names using colors():\n\ncolors()"
  },
  {
    "objectID": "01-graphical.html#getting-help-in-rstudio",
    "href": "01-graphical.html#getting-help-in-rstudio",
    "title": "1  Graphical Summaries of Data",
    "section": "1.10 Getting Help in RStudio",
    "text": "1.10 Getting Help in RStudio\n\n1.10.1 The RStudio help system\nThere are several ways to get RStudio to help you when you forget something. Most objects in packages have help files that you can access by typing something like:\n\n?bargraph\n?histogram\n?HELPrct\n\nYou can search the help system using\n\nhelp.search('Grand Rapids')    # Does R know anything about Grand Rapids?\n\nThis can be useful if you don’t know the name of the function or data set you are looking for.\n\n\n1.10.2 Tab completion\nAs you type the name of a function in RStudio you can hit the tab key and it will show you a list of all the ways you could complete that name, and after you type the opening parenthesis, if you hit the tab key, you will get a list of all the arguments and (sometimes) some helpful hints about what they are.)\n\n\n1.10.3 History\nIf you know you have done something before, but can’t remember how, you can search your history. The history tab shows a list of recently executed commands. There is also a search bar to help you find things from longer ago.\n\n\n1.10.4 Error messages\nWhen things go wrong, R tries to help you out by providing an error message. If you can’t make sense of the message, you can try copying and pasting your command and the error message and sending to me in an email. One common error message is illustrated below.\n\nfred <- 23\nfrd\n\nError in eval(expr, envir, enclos): object 'frd' not found\n\n\nThe object frd is not found because it was mistyped. It should have been fred. If you see an “object not found” message, check your typing and check to make sure that the necessary packages have been loaded."
  },
  {
    "objectID": "01-graphical.html#graphical-summaries-important-ideas",
    "href": "01-graphical.html#graphical-summaries-important-ideas",
    "title": "1  Graphical Summaries of Data",
    "section": "1.11 Graphical Summaries – Important Ideas",
    "text": "1.11 Graphical Summaries – Important Ideas\n\n1.11.1 The Most Important Template\nThe plots we have created have all followed a single template\n\n\n\nThe most important template\n\n\nWe will see this same template used again for numerical summaries and linear and non-linear modeling as well, so it is is important to master it. To use it, you just fill in the boxes with the information for your particular task.\n\n\ngoal: The name of the function generally describes your goal, the thing you want the computer to produce for you. In the case of plotting, it is the name of the plot. When we do numerical summaries it will be the name of the numerical summary (mean, median, etc.).\nformula: For plotting, the formula describes which variables are used on the x-axis, the y-axis and for conditioning. The general scheme is y ~ x | z where z is the conditioning variable. Sometimes y or z are missing (but the right-hand side x must always be included in a formula).\nmydata: A data frame must be given in which the variables mentioned in the formula can be found. Variables not found there will be looked for in the enclosing environment. Sometimes we will take advantage of this to avoid creating a temporary data frame just to make a quick plot, but generally it is best to have all the information inside a data frame.\n... There are many optional arguments to control sizes, colors, etc. We will introduce these as they are needed, but several examples have been given in this chapter as well. Consult the help files for assistance.\n\n\nJust fill in the boxes and get your plot.\n\n\n1.11.2 Patterns and Deviations from Patterns\nThe goal of a statistical plot is to help us see \n\npotential patterns in the data, and\ndeviations from those patterns. \n\n\n\n1.11.3 Different Plots for Different Kinds of Variables\nGraphical summaries can help us see the distribution of a variable or the relationships between two (or more) variables. The type of plot used will depend on the kinds of variables involved. There is a nice summary of these on page~48. You can use demo() to see how to get R to make the plots in this section.\nLater, when we do statistical analysis, we will see that the analysis we use will also depend on the kinds of variables involved, so this is an important idea.\n\n\n1.11.4 Side-by-side Plots and Overlays Can Reveal Importance of Additional Factors\nThe ggformula graphics plots make it particularly easy to generate plots that divide the data into groups and either produce a panel for each group (using |!) or display each group in a different way (different colors or symbols, using the groups argument). These plots can reveal the possible influence of additional variables – sometimes called covariates.\n\n\n1.11.5 Area = (relative) frequency\nMany plots are based on the key idea that our eyes are good at comparing areas. Plots that use area (e.g., histograms, mosaic plots, bar charts, pie charts) should always obey this principle\n\nArea \\(=\\) (relative) frequency\n\nPlots that violate this principle can be deceptive and distort the true nature of the data."
  },
  {
    "objectID": "01-graphical.html#exercises",
    "href": "01-graphical.html#exercises",
    "title": "1  Graphical Summaries of Data",
    "section": "1.12 Exercises",
    "text": "1.12 Exercises\nIn your solutions to these exercises include both the plots and the code you used to make them as well as any required discussion. Once you get the plots figured out, feel free to use some of the bells and whistles to make the plots even better.\n\nExercise 1.1 Old Faithful\nCreate a scatterplot using the two variables in the oldfaith data frame (available in the alr4 package). What do we learn about Old Faithful eruptions from this plot?\n\n\nExercise 1.2 CPS85\nWhere do the data in the CPS85 data frame (in the mosaic package) come from? What are the observational units? How many are there?\n\n\nExercise 1.3 Quantitative variable in CPS85\nChoose a quantitative variable that interests you in the CPS85 data set. Make an appropriate plot and comment on what you see.\n\n\nExercise 1.4 Categorical variable in CPS85\nChoose a categorical variable that interests you in the CPS85 data set. Make an appropriate plot and comment on what you see.\n\n\nExercise 1.5 Multiple variables in CPS85\nCreate a plot that displays two or more variables from the CPS85 data. At least one should be quantitative and at least one should be categorical. Comment on what you can learn from your plot.\n\n\nExercise 1.6 mpg\nWhere do the data in the mpg data frame (in the ggplot2 package) come from? What are the observational units? How many are there?\n\n\nExercise 1.7 Quantitative variable in mpg\nChoose a quantitative variable that interests you in the mpg data set. Make an appropriate plot and comment on what you see.\n\n\nExercise 1.8 Categorical variable in mpg\nChoose a categorical variable that interests you in the mpg data set. Make an appropriate plot and comment on what you see.\n\n\nExercise 1.9 Multiple variables in mpg\nCreate a plot that displays two or more variables from the mpg data. At least one should be quantitative and at least one should be categorical. Comment on what you can learn from your plot.\n\n\nExercise 1.10 Fires\nThe file at https://rpruim.github.io/Engineering-Statistics/data/Fires.csv is a csv file containing data on wild lands fires in the US over a number of years. You can load this data one of two ways.\n\nGo to the Environment tab, select Import Dataset, choose From Text (either option will due) and follow the instructions.\nUse the following command in R\n\n\nurl <- \"https://rpruim.github.io/Engineering-Statistics/data/Fires.csv\"\n\n# equivalent to \"From Text (base)\" in RStudio\nFires <- read.csv(url)\n\n# alternative method -- equivalent to \"From Text (readr)\" in RStudio\nlibrary(readr)\nFires <- read_csv(url)\n\nRows: 52 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): Year, Fires, Acres\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou can also use either of these methods to read from a file rather than from a web URL, so this is a good way to get your own data into R\n\nThe source for these data claim that data before a certain year should not be compared to data from after that year because the older data were computed a different way and are not considered as reliable. What year is the break point? Use graphs of the data over time to estimate when something changed.\nCreate a data set that contains only the data from the new data regime (based on your answer in the previous problem). You can trim the data to just the subset you want using filter(). For example, to get just the subset of years since 1966, you could use the code below. (Be sure to save the result with a new new name if you want to keep the original data available.)\n\n\nFires2 <-               # create a new data set called Fires2\n  Fires |>              # start with all the Fires data\n  filter(Year > 1966)   # then filter to keep only years after 1966\n\n\nUsing only the data from the smaller set you just created, how would you describe what is happening with fires over time?\n\n\n\nExercise 1.11 What are i1 and i2?\nUse R s help system to find out what the i1 and i2 variables are in the HELPrct data frame. Make histograms for each variable and comment on what you find out. How would you describe the shape of these distributions? Do you see any outliers (observations that don’t seem to fit the pattern of the rest of the data)?\n\n\nExercise 1.12 i1 and i2 among men and among women\nCompare the distribution of i1 among men vs. among women. Do the same for i2.\n\n\n\nSolution. \ngf_dens( ~max_drinks, color = ~ sex, data = HELPrct )\n\n\n\ngf_dens( ~avg_drinks, color = ~ sex, data = HELPrct )\n\n\n\n\n\n\nExercise 1.13 i1 and i2 among different treatment groups\nCompare the distributions of i1 among the three substance groups. Do the same for i2.\n\n\n\nSolution. \ngf_dens( ~i1, color = ~ substance, data = HELPrct )\n\n\n\ngf_dens( ~i2, color = ~ substance, data = HELPrct )\n\n\n\n\n\ngf_dens( ~i1 | sex, color = ~ substance, data = HELPrct )\n\n\n\ngf_dens( ~i2 | sex, color = ~ substance, data = HELPrct )\n\n\n\n\n\ngf_point( i2 ~ i1, color = ~ sex, data =  HELPrct, alpha = .6, size = .6 )\n\n\n\n\n\n\nExercise 1.14 Snow in GR\nThe SnowGR contains historical data on snowfall in Grand Rapids, MI. The snowfall total for January, 2014 was 36.6 inches.\n\nCreate a histogram of January snowfall totals. How unusual is 36.6 inches of snow in January?\nIf there is a lot of snow in January, should we expect to have unusually much or little snow in February? Make a scatter plot comparing January and February snowfall totals and comment on what you see there.\n\n\n\n\nSolution. \ngf_histogram( ~ Jan, data = SnowGR)\n\nWarning: Removed 1 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n36.6 inches is pretty high, but not the highest ever for a January. Certainly well above average.\n\ngf_point(Feb ~ Jan, data = SnowGR)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThere is no clear trend. Sometimes the snow keeps on coming, but a heavy snow January could be followed by either a heavy or light snow February."
  },
  {
    "objectID": "02-numerical.html#tabulating-data",
    "href": "02-numerical.html#tabulating-data",
    "title": "2  Numerical Summaries",
    "section": "2.1 Tabulating Data",
    "text": "2.1 Tabulating Data\nA table is one kind of numerical summary of a data set. In fact, you can think of histograms and bar graphs as graphical representations of summary tables. But sometimes it is nice to have the table itself. R provides several ways of obtaining such tables.\n\n2.1.1 Tabulating a categorical variable\n\nThe formula interface\nThere are several functions for tabulating categorical variables. tally() uses a syntax that is very similar to bargraph(). We’ll call this method the formula interface. (R calls anything with a tilde (~) a formula.)\ntally() allows us to choose raw counts, proportions, or percentages.\n\ntally( ~ sex, data = KidsFeet )\n\nsex\n B  G \n20 19 \n\ntally( ~ sex, data = KidsFeet, format = \"prop\" )\n\nsex\n        B         G \n0.5128205 0.4871795 \n\ntally( ~ sex, data = KidsFeet, format = \"perc\" )\n\nsex\n       B        G \n51.28205 48.71795 \n\n\n\n\nThe $-interface\ntable() and its cousins use the $ operator which selects one variable out of a data frame.\n\nKidsFeet$sex      # general syntax: dataframe$variable\n\n [1] B B B B B B B G G B B B B B G G G G G G B B G G G B G B B B G G G B B G G G\n[39] G\nLevels: B G\n\ntable(KidsFeet$sex)\n\n\n B  G \n20 19 \n\n# tally() is bilingual:\ntally(KidsFeet$sex)\n\nX\n B  G \n20 19 \n\n\nWe’ll call this interface the $-interface.\n\n\n\n2.1.2 Two interfaces\nSome functions in R require the formula interface, some require the $-interface, and some allow you to use either one.1\n\n\n\n\n\nMy advice is to use formula interfaces whenever they are available and to choose tools that make this possible."
  },
  {
    "objectID": "02-numerical.html#tabulating-a-quantitative-variable",
    "href": "02-numerical.html#tabulating-a-quantitative-variable",
    "title": "2  Numerical Summaries",
    "section": "2.2 Tabulating a quantitative variable",
    "text": "2.2 Tabulating a quantitative variable\nAlthough tally() and table() work with quantitative variables as well as categorical variables, this is only useful when there are not too many different values for the variable.\n\ntally( ~age, data = HELPrct )\n\nage\n19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \n 1  2  3  8  5  8  7 13 18 15 18 18 20 28 35 18 25 23 20 18 27 10 20 10 13  7 \n45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 \n13  5 14  5  8  2  1  1  3  1  2  1  2  2  2  1 \n\n\n\n2.2.1 Tabulating in bins (optional)\nUsually a graph is the best way to display and summarize quantitative data, but if you need to crest a summary table, you may need to group quantitative data into bins. We just have to tell R what the bins are. For example, suppose we wanted to group the 20s, 30s, 40s, etc. together.\n\n# let's add a new variable to HELPrct\nHELPrct <- \n  HELPrct |>\n  mutate(binnedAge = cut(age, breaks = c(10,20,30,40,50,60,70) ))\nhead(HELPrct)\n\n\n\n  \n\n\ntally( ~ binnedAge, data = HELPrct) \n\nbinnedAge\n(10,20] (20,30] (30,40] (40,50] (50,60] (60,70] \n      3     113     224      97      16       0 \n\n\nThat’s not quite what we wanted: 30 is in with the 20s, for example. Here’s how we fix that.\n\nHELPrct <- HELPrct |>\n  mutate(binnedAge = cut(age, breaks = c(10,20,30,40,50,60,70), right = FALSE))\ntally( ~ binnedAge, data = HELPrct ) \n\nbinnedAge\n[10,20) [20,30) [30,40) [40,50) [50,60) [60,70) \n      1      97     232     105      17       1 \n\n\nWe won’t use this very often, since typically seeing this information in a histogram is more useful."
  },
  {
    "objectID": "02-numerical.html#cross-tables-tabulating-two-or-more-variables",
    "href": "02-numerical.html#cross-tables-tabulating-two-or-more-variables",
    "title": "2  Numerical Summaries",
    "section": "2.3 Cross-tables: Tabulating two or more variables",
    "text": "2.3 Cross-tables: Tabulating two or more variables\ntally() can also compute cross tables for two (or more) variables.\n\ntally(sex ~ substance, data = HELPrct)\n\n        substance\nsex      alcohol cocaine heroin\n  female      36      41     30\n  male       141     111     94\n\ntally(~ sex + substance, data = HELPrct)\n\n        substance\nsex      alcohol cocaine heroin\n  female      36      41     30\n  male       141     111     94"
  },
  {
    "objectID": "02-numerical.html#working-with-pre-tabulated-data",
    "href": "02-numerical.html#working-with-pre-tabulated-data",
    "title": "2  Numerical Summaries",
    "section": "2.4 Working with Pre-Tabulated Data",
    "text": "2.4 Working with Pre-Tabulated Data\nSometimes data arrive pre-tabulated. We can use gf_col() instead of gf_bar() to graph pre-tabulated data.\n\nlibrary(abd)           # data sets from Analysis of Biological Data\nTeenDeaths\n\n\n\n  \n\n\ngf_col(deaths ~ cause, data = TeenDeaths) |>\n  gf_refine(coord_flip())    # flip x and y axes\n\n\n\n\nNotice that by default the causes are displayed in alphabetical order. R assumes that categorical data is nominal (that is, there is no particular natural or logical ordering to the categories) unless you say otherwise.\nHere is an easy way to have things appear in a different order. The causes of death are reordered in order of increasing number of deaths caused.\n\ngf_col( deaths ~ reorder(cause, deaths), data = TeenDeaths) |>\n  gf_refine(coord_flip()) |>\n  gf_labs(x = 'Cause of Death', y = 'Number of Deaths')"
  },
  {
    "objectID": "02-numerical.html#summarizing-distributions-of-quantitative-variables",
    "href": "02-numerical.html#summarizing-distributions-of-quantitative-variables",
    "title": "2  Numerical Summaries",
    "section": "2.5 Summarizing Distributions of Quantitative Variables",
    "text": "2.5 Summarizing Distributions of Quantitative Variables\n\nImportant Note\nNumerical summaries are a convenient way to describe a distribution, but remember that numerical summaries do not necessarily tell you everything there is to know about a distribution. When working with a new dataset, it is always important to explore the data as fully as possible (commonly including graphical as well as numerical summaries, and sometimes even examining the data table directly) before accepting any simplified summary as a good representation of the data. You might discover certain patterns in the data, interesting features, or even outliers or mistakes in the data, that make certain summaries misrepresentations of the whole.\n\n\nNotation\nIn statistics \\(n\\) (or sometimes \\(N\\)) almost always means the number of observations (i.e., the number of rows in a data frame).\nIf \\(y\\) is a variable in a data set with \\(n\\) cases, we can denote the \\(n\\) values of \\(y\\) as \n\n\\(y_1, y_2, y_3, \\dots y_n\\) (in the original order of the data).\n\\(y_{(1)}, y_{(2)}, y_{(3)}, \\dots y_{(n)}\\) (in sorted order from smallest to largest).\n\n\nThe symbol \\(\\displaystyle \\sum\\) represents summation (adding up a bunch of values)."
  },
  {
    "objectID": "02-numerical.html#measures-of-center",
    "href": "02-numerical.html#measures-of-center",
    "title": "2  Numerical Summaries",
    "section": "2.6 Measures of Center",
    "text": "2.6 Measures of Center\nMeasures of center attempt to give us a sense of what is a typical value for the distribution.\n\\[\n\\begin{aligned}\n\\mbox{mean of $y$}\n&=\n\\overline{y}\n= \\frac{\\displaystyle \\sum_{i = 1}^{n} y_i}{n}\n= \\frac{\\mbox{sum of values}}{\\mbox{number of values}}\n\\\\[3mm]\n\\mbox{median of $y$}\n&=\n\\mbox{the ``middle'' number}\n\\\\\n& \\quad\\quad \\mbox{(after putting the numbers in increasing order)}\n\\end{aligned}\n\\]\n\n\nThe mean is the “balancing point” of the distribution.\nThe median2 is the 50th percentile: half of the distribution is below the median, half is above.\nIf the distribution is symmetric, then the mean and median are the same.\nIn a skewed distribution, the mean is pulled farther toward the tail than the median is.\nA few very large or very small values can change the mean a lot, so the mean is sensitive to outliers and is a better measure of center when the distribution is symmetric than when it is skewed.\nThe median is a resistant measure (resistant to the presence of outlier) – it is not affected much by a few very large or very small values."
  },
  {
    "objectID": "02-numerical.html#measures-of-spread",
    "href": "02-numerical.html#measures-of-spread",
    "title": "2  Numerical Summaries",
    "section": "2.7 Measures of Spread",
    "text": "2.7 Measures of Spread\n\\[\n\\begin{aligned}\n\\mbox{variance of $y$}\n= s^2_y\n&=\n\\frac{\\displaystyle \\sum  {i = 1}^{n} (y_i - \\overline{y})^2 }{n-1}\n\\\\[4mm]\n\\mbox{standard deviaiton of $y$}  = s_y\n&= \\sqrt{s^2_y}\n\\\\\n&= \\mbox{square root of variance}\n\\\\[4mm]\n\\mbox{interquartile range}  = \\mbox{IQR}\n&= Q_3 - Q_1\n\\\\\n& = \\mbox{difference between first and third quartiles (defined shortly)}\n\\end{aligned}\n\\]\n\n\nRoughly, the standard deviation is the “average deviation from the mean”. (That’s not exactly right because of the squaring involved and because we are dividing by \\(n-1\\) instead of by \\(n\\). More on that denominator later.)\nThe mean and standard deviation are especially useful for describing normal distributions and other unimodal, symmetric distributions that are roughly “bell-shaped”. (We’ll learn more about normal distributions later.)\nLike the mean, the variance and standard deviation are sensitive to outliers and less suited for summarizing skewed distributions.\nIt is perhaps of some value to compute the variance and standard deviation by hand once or twice to make sure you understand how these measures are defined, but we will typically let R do the calculations for us. \n\nTo get a numerical summary of a variable (a statistic), we need to tell R which statistic we want and the variable and data frame involved. There several ways we can do this in R Here are several ways to get the mean, for example:\n\nmean(HELPrct$age)            # this is the old fashioned way\n\n[1] 35.65342\n\nmean(~ age, data = HELPrct)  # similar to our plotting methods; only works for some functions\n\n[1] 35.65342\n\ndf_stats(~ age, data = HELPrct, mean)  # formula-based and very flexible\n\n\n\n  \n\n\n\nUsing the formula style, we can now compute several different statistics.\n\nmean( ~ age, data = HELPrct)\n\n[1] 35.65342\n\nsd( ~ age, data = HELPrct)\n\n[1] 7.710266\n\nvar( ~ age, data = HELPrct)\n\n[1] 59.4482\n\n\n\nmedian( ~ age, data = HELPrct)\n\n[1] 35\n\nIQR( ~ age, data = HELPrct) \n\n[1] 10\n\ndf_stats( ~ age, data = HELPrct)  # this computes several statistics at once\n\n\n\n  \n\n\n\nIt is also possible to compute these statistics separately for each of several groups. The syntax is much like the the syntax we used when plotting. In fact, we have two choices for the formula: y ~ x or ~ x | z.\n\nmean(age ~ sex, data = HELPrct)\n\n  female     male \n36.25234 35.46821 \n\nsd(age ~ sex, data = HELPrct)\n\n  female     male \n7.584858 7.750110 \n\ndf_stats( ~ age | sex, data = HELPrct )\n\n\n\n  \n\n\n\n\n2.7.1 A word of caution\nNone of these measures (especially the mean and median) is a particularly good summary of a distribution if the distribution is not unimodal. The histogram below shows the lengths of eruptions of the Old Faithful geyser at Yellowstone National Park.\n\nlibrary(faraway)\ndf_stats(~ Duration, data = oldfaith)\n\n\n\n\n\n\nFigure 2.1: The distribution of Old Faithful eruption times is bimodal, suggesting two types of eruptions. A single number doesn’t do a good job of summarizing this sort of distribution.\n\n\ngf_histogram( ~ Duration, data = oldfaith,  bins = 20) |> \n    gf_labs(title = \"Old Faithful Eruption Times\", x = \"duration (sec)\") \n\n\n\n\nFigure 2.2: The distribution of Old Faithful eruption times is bimodal, suggesting two types of eruptions. A single number doesn’t do a good job of summarizing this sort of distribution.\n\n\n\n\nNotice that the mean and median do not represent typical eruption times very well. Nearly all eruptions are either quite a bit shorter or quite a bit longer. (This is especially true of the mean.)\n\n\n2.7.2 Box plots\nBoxplots (also called box-and-whisker plots) are a graphical representation of a 5-number summary of a quantitative variable. The five numbers are the five quantiles: \n\n\\(Q_0\\), the minimum\n\\(Q_1\\), the first quartile (25th percentile)\n\\(Q_2\\), the median (50th percentile)\n\\(Q_3\\), the third quartile (75th percentile)\n\\(Q_4\\), the maximum\n\n\n\ngf_boxplot(~age, data = HELPrct)\n\n\n\n\nFigure 2.3: ?(caption)\n\n\n\n\nBoxplots provide a way of comparing multiple groups that is especially informative and visually effective. Here is one way to make boxplots of multiple groups (it should look familiar from what we know about histogram):\n\ngf_boxplot(~age | substance ~ ., data = HELPrct)\n\n\n\n\nFigure 2.4: Facets are not the best way to display this sort of plot.\n\n\n\n\nBut gf_boxplot() has a better way. Put the quantitative variable on one side of the wiggle and the categorical on the other. The placement determines which goes along the vertical axis and which along the horizontal axis – just like it did for gf_point().\n\ngf_boxplot(substance ~ age, data = HELPrct)\ngf_boxplot(age ~ substance, data = HELPrct)\n\n\n\n\n\n\n\n(a) Horizontal boxplots often use space more efficiently\n\n\n\n\n\n\n\n(b) But vertical boxplots follow the common “response variable on the y-axis” pattern.\n\n\n\n\nFigure 2.5: Boxplots are primarily used to compare multiple distributions. We can have a categorical variable on one of the axes to produce “side-by-side boxplots”.\n\n\n\nAnd we can combine this idea with conditioning. \n\n\ngf_boxplot(age ~ substance | homeless, data = HELPrct)\ngf_boxplot(substance ~ age | homeless ~ ., data = HELPrct)\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\nFigure 2.6: Side-by-side boxplots with faceting.\n\n\n\n\n2.7.3 Small data sets\nWhen we have relatively small data sets, it may not make sense to use a boxplot. With very few observations, boxplots can be misleading, in that they suggest the presence of more observations than are really contained in the dataset. In these cases, it may be better to display all the data. gf_jitter() allows you to put a categorical variable along one axis and a quantitative variable along the other. For some data sets, either option can produce a plot that gives a good picture of the data.\n\ngf_jitter( weight ~ sex, data = Mosquitoes, width = 0.1, height = 0)\ngf_boxplot( weight ~ sex, data = Mosquitoes)\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 2.7: Jitter can be useful for displaying relatively small data sets that have duplicated values.\n\n\n\nNote the effect of the width = 0.1, height = 0 – this tells gf_jitter() to move each data point slightly left or right, but not at all up or down. This can be used to reduce overplotting (data points being plotted on top of one another) without losing any information, making it clearer how many data points were observed for each possible combination of x- and y-values."
  },
  {
    "objectID": "02-numerical.html#summarizing-categorical-variables",
    "href": "02-numerical.html#summarizing-categorical-variables",
    "title": "2  Numerical Summaries",
    "section": "2.8 Summarizing Categorical Variables",
    "text": "2.8 Summarizing Categorical Variables\nThe most common summary of a categorical variable is the proportion of observations in each category. For a single category:\n\\[\n\\begin{aligned}\n\\hat p & = \\frac{\\mbox{number in one category}}{n}\n\\end{aligned}\n\\]\nProportions can be expressed as fractions, decimals or percents. For example, if there are 10 observations in one category and \\(n = 50\\) observations in all, then\n\\[\n\\hat p = \\frac{10}{25} = \\frac{2}{5} =  0.40 = 40\\%\n\\]\nIf we code our categorical variable using 1 for observations in a single category of interest – “the one category” – and 0 for observations in any other category, then a proportion is a sample mean.\n\\[\n\\frac{ 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 +\n0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 }{25} = \\frac{10}{25}\n\\]"
  },
  {
    "objectID": "02-numerical.html#relationships-between-two-variables",
    "href": "02-numerical.html#relationships-between-two-variables",
    "title": "2  Numerical Summaries",
    "section": "2.9 Relationships Between Two Variables",
    "text": "2.9 Relationships Between Two Variables\nIt is also possible to give numerical summaries of the relationship between two variables. The most common one is the correlation coefficient, which we will learn about later."
  },
  {
    "objectID": "02-numerical.html#exercises",
    "href": "02-numerical.html#exercises",
    "title": "2  Numerical Summaries",
    "section": "2.10 Exercises",
    "text": "2.10 Exercises\n\nExercise 2.1 Small variance\nCreate a data set with \\(n = 6\\) values, each an integer between 0 and 10 (inclusive) that has the smallest possible variance. Compute the mean and variance of this data set “by hand” (that is, without using mean() or sd() or var() or `df_stats(), etc. in R or similar features on a calculator). You can check your hand calculation using R, but show the hand calculation.\n\n\nSolution. The variance will be smallest if all the values are equal to the mean value. In that case the variance will be 0.\nIf you require all of the numbers to be distinct, the best you can do is 6 consecutive numbers\n\ntibble(x = 1:6) |> df_stats(~ x, mean, var, sd)\n\n\n\n  \n\n\n\n\n\nExercise 2.2 Large variance\nCreate a data set with \\(n = 6\\) values, each an integer between 0 and 10 (inclusive) that has the largest possible variance. Compute the variance of this data set “by hand” (that is, without using mean() or sd() or var() or df_stats(), etc. in R or similar features on a calculator). You can check your hand calculation using R, but show the hand calculation.\n\n\nSolution. The variance will be largest if all the values are far from the mean. If we have a data set with three 0’s and 3 10’s, then the mean is 5 and the variance is\n\n( 3*(0-5)^2 + 3*(10-5)^2 ) / 5\n\n[1] 30\n\n\nIf you require the numbers to be distinct, then the best we can do is\n\ntibble(x = c(0,1,2,8,9,10)) |> df_stats(~ x, mean, var, sd)\n\n\n\n  \n\n\n\n\n\nExercise 2.3 Drinks per day, graphical summary\nCreate side-by-side boxplots of the variable i1 (average number of drinks per day) comparing the different substance groups in the HELPrct data frame.\nFor each variable substance group, explain how you can tell from the boxplots whether the mean will be larger than the median or the median larger than the mean.\n\n\n\nSolution. \ngf_boxplot(i1 ~ substance , data = HELPrct)\n\n\n\n\nThe means are larger because the distributions have longer tails in the higher direction.\n\n\nExercise 2.4 Drinks per day, numerical summary\nCompute the mean and median values of i1 (average number of drinks per day) for each of the substance groups in the HELPrct data frame.\n\n\n\nSolution. \nmean(i1 ~ substance, data = HELPrct)\n\n  alcohol   cocaine    heroin \n29.192090 12.131579  8.879032 \n\nmedian(i1 ~ substance, data = HELPrct)\n\nalcohol cocaine  heroin \n     25       6       4"
  },
  {
    "objectID": "03-probability.html#key-definitions-and-ideas",
    "href": "03-probability.html#key-definitions-and-ideas",
    "title": "3  Probability",
    "section": "3.1 Key Definitions and Ideas",
    "text": "3.1 Key Definitions and Ideas\nThe terms below will help us talk about randomness and probability.\n\n\nrandom process A repeatable process that has multiple unpredictable potential outcomes.\nAlthough we sometimes use language that suggests that a particular result is random, it is really the process that is random, not its results.\noutcome A potential result of a random process.\nsample space The set of all possible potential outcomes of a random process.\nevent A subset of the sample space.\nThat is, a set of outcomes (possibly all or none of the outcomes).\nStatisticians often use capital letters from the beginning of the alphabet for events.\ntrial One repetition of a random process.\nmutually exclusive events. Events that cannot happen on the same trial.\nprobability A numerical value between 0 and 1 assigned to an event to indicate how often the event occurs (in the long run).\nrandom variable A random variable is a variable whose value is a numerical outcome of a random process.\nprobability distribution The distribution of a random variable. (Remember that a distribution describes what values? and with what frequency?)\n\n\n\n3.1.1 Examples of random variables\nBelow are some random processes that result in a number.\n\nRoll a die and record the number.\nRoll two dice and record the sum.\nFlip 100 coins and count the number of heads.\nSample 1000 people and count how many approve of the job the president is doing.\n\nNote: Statisticians usually use capital letters (often from the end of the alphabet) for random variables, like this:\n\nLet \\(X\\) be the number of heads in 10 flips of a fair coin. What is \\(\\Prob(X = 5)\\)?\n\nAs an example of a probability distribution, we can first consider a discrete random variable. Most of the examples of random variables given above are discrete. In other words, the values they can take on come from a set containing a finite number of possible values. For example, if you roll a 6-sided die and record the number that comes up, there are only size possible outcomes, which are equally likely: the integers 1, 2, 3, 4, 5 and 6. For discrete random variables, the probability distribution shows all the possible values on the x-axis, and the likelihood of observing each of those values on the y-axis. Since there are a finite number of possible values that can be observed, these likelihoods are actually the probabilities of observing each outcome, and the sum of all the probabilities must be 1 (see Section 3.3 for details). For our example, where we rolled a die and recorded the value:\n\n\n\n\n\nThings are a bit more complicated for continuous random variables (the ones that can take on any numerical value). Here, there sample space (the set of possible distinct values the random variable can take on) is infinite. One consequence of this fact is that the interpretation of the y-axis values of the probability distribution changes. The y-axis will still indicate the relative likelihood of observing any given value of the random variable. However, here the random variable can take on an infinite number of possible values. In this case, we can’t interpret the y-axis values as probabilities. They y axis units are called “Likelihood” or “Density”, and they indicate the relative frequency of each outcome. For a densityplot, Density is scaled such that the integral over all possible x-values (the area under the curve) is 1. (For a histogram, Density is scaled so that the total area of all the boxes added together is 1.) We can think of the histograms and density plots we have been creating using continuous variables from R datasets as attempts to use data to approximate the distributions of random variables. For example, we might consider the growth of flower petals of the iris Iris setosa as a random process, and let X be a random variable that is the length of each iris petal. We could plot a histogram to approximate the distribution of X using the variable Petal.Length from the iris data (from the datasets package in base R)."
  },
  {
    "objectID": "03-probability.html#calculating-probabilities-empirically",
    "href": "03-probability.html#calculating-probabilities-empirically",
    "title": "3  Probability",
    "section": "3.2 Calculating Probabilities Empirically",
    "text": "3.2 Calculating Probabilities Empirically\nWe would like to calculate the probability of an event \\(A\\), denoted \\(\\Prob(A)\\).\nIn the next section, we will see how to calculate probabilities based on the Axioms of probability, and logic. But first, we will consider ways to make the calculations empirically – based on observing many repetitions of a random process (in real life or in a computer simulation) and observing how often an event of interest occurs.\nRandom processes are repeatable, so practically, we can calculate empirical probabilities by simply repeating the process over and over and keeping track of how often the event \\(A\\) occurs. For example, we could flip a coin 10,000 times and see what fraction are heads.1\n\\[\n\\mbox{Empirical Probability} =\n\\frac{\\mbox{number of times $A$ occured}}\n{\\mbox{number of times random process was repeated}}\n\\]\nModern computing provides another way to compute empirical probabilities. If we can simulate our random process on a computer, then we can repeat the process many times very quickly.\n\nExample 3.1 Q. What is the probability of getting exactly 5 heads if you flip a fair coin 10 times? Using our random variable notation, let \\(X\\) be the number of heads in 10 flips of a fair coin. We want to know \\(\\Prob(X = 5)\\).\nA. The rflip() function simulates flipping a coin as many times as we like.\n\nrflip(10)\n\n\nFlipping 10 coins [ Prob(Heads) = 0.5 ] ...\n\nT H H H T H T H T H\n\nNumber of Heads: 6 [Proportion Heads: 0.6]\n\n\nThe do() function allows us to execute an R command (“do” something in R) over and over, as many times as we choose. Here, our rflip() command simulates 10 coin-flips. First we’ll “do” our command three times and show the results. Then we’ll do it 10,000 times and store the results in a variable called tosses, so we can create a table and a plot showing the empirical distribution.\n\ndo(3) * rflip(10)\n\n\n\n  \n\n\ndo(10000) * rflip(10) -> tosses\ntally(~ heads, data = tosses, format = \"prop\")\n\nheads\n     0      1      2      3      4      5      6      7      8      9     10 \n0.0008 0.0091 0.0422 0.1254 0.1982 0.2466 0.2114 0.1176 0.0392 0.0088 0.0007 \n\ngf_histogram( ~ heads, data = tosses, binwidth = 1)\n\n\n\n\nBased on this sample, we would estimate that \\(\\Prob(X = 5) \\approx 0.2466\\).\n\n\nExample 3.2 Q. Use simulations to estimate the probability of rolling doubles using two fair standard dice.\nA. We can simulate rolling a die with the following code:\n\n1:6                # the numbers 1 through 6\n\n[1] 1 2 3 4 5 6\n\nresample(x = 1:6, size = 10)  # ten rolls of a 6-sided die\n\n [1] 2 1 4 1 1 2 6 2 5 4\n\n\nThe first 2 input arguments of resample() are x (the set of values from which you want to resample) and size (the number of items to choose from x). You can also think of size as the number of times to sample from x, if you are imagining sampling one item from x each time.\nIf we do this 10,000 times for each of two dice…\n\ndie1 <- resample(1:6, 10000)\ndie2 <- resample(1:6, 10000)\n# let's check that things look reasonable\nhead(die1) \n\n[1] 6 2 1 6 6 4\n\nhead(die2)\n\n[1] 1 2 4 4 1 2\n\n\nThen we can tabulate how often the two numbers matched in one of two ways:\n\ntally( ~(die1 == die2) )    # NOTE the double == here\n\n(die1 == die2)\n TRUE FALSE \n 1634  8366 \n\nprop( ~(die1 == die2) )     # NOTE the double == here\n\nprop_TRUE \n   0.1634 \n\n\nSo the probability appears to be approximately 0.1634.\n\n\nExample 3.3 Q. Use simulation to estimate the probability of rolling a sum of 8 when rolling two fair six-sided dice.\nA. We have already generated 10000 random rolls, so let’s just reuse them. (Alternatively, we could generate new rolls.)\n\ns <- die1 + die2 \n# R adds element-wise: \n#   first entry of die1 + first of die2, \n#   second to second, etc.\nprop( ~ (s == 8) )\n\nprop_TRUE \n   0.1443 \n\n\nWe can estimate the probability of any sum the same way.\n\ntally( ~ s )\n\ns\n   2    3    4    5    6    7    8    9   10   11   12 \n 291  579  810 1067 1382 1625 1443 1102  855  571  275 \n\ntally( ~ s, format = \"percent\" )   # if we are too lazy to divide by 10000 ourselves\n\ns\n    2     3     4     5     6     7     8     9    10    11    12 \n 2.91  5.79  8.10 10.67 13.82 16.25 14.43 11.02  8.55  5.71  2.75 \n\n\nHere’s a slightly fancier version that puts all the information into a data frame. Note the use of the function data.frame() to create the data table:\n\nrolls <- data.frame( first = die1, second = die2, sum = die1 + die2 )\nhead(rolls)\n\n\n\n  \n\n\ntally( ~sum, data = rolls, format = \"proportion\")\n\nsum\n     2      3      4      5      6      7      8      9     10     11     12 \n0.0291 0.0579 0.0810 0.1067 0.1382 0.1625 0.1443 0.1102 0.0855 0.0571 0.0275 \n\ngf_histogram( ~ sum, data = rolls, binwidth = 1)    # setting width is important for integer data"
  },
  {
    "objectID": "03-probability.html#sec-theoretical-prob",
    "href": "03-probability.html#sec-theoretical-prob",
    "title": "3  Probability",
    "section": "3.3 Calculating Probabilities Theoretically",
    "text": "3.3 Calculating Probabilities Theoretically\nThe theoretical method combines \n\nSome basic facts about probability (the Probability Axioms and Rules),\nSome assumptions about the particular situation at hand, and\nMathematical reasoning (arithmetic, algebra, logic, etc.). \n\n\n3.3.1 The Three Probability Axioms\n\nLet \\(S\\) be the sample space and let \\(A\\) and \\(B\\) be events.\n\nProbability is between 0 and 1: \\(0 \\le \\Prob(A) \\le 1\\).\nThe probability of the sample space is 1: \\(\\Prob(S) = 1\\).\nAdditivity: If \\(A\\) and \\(B\\) are mutually exclusive, then \\(\\Prob(A \\tor B) = \\Prob(A) + \\Prob(B)\\).\n\n\n\nNotation Notes\n\\(\\Prob(A \\tor B)\\) is the probability that either \\(A\\) or \\(B\\) (or both) occurs. Often this is written \\(\\Prob(A \\union B)\\). \\(A \\union B\\) is usually read “\\(A\\) union \\(B\\)”. The union of two sets is the set that contains all elements of both sets.\n\\(\\Prob(A \\tand B)\\) is the probability that both \\(A\\) and \\(B\\) occur. This is also written \\(\\Prob(A \\intersect B)\\). \\(A \\intersect B\\) is usually read “\\(A\\) intersect \\(B\\)”.\nSaying that \\(A\\) and \\(B\\) are mutually exclusive is the same as saying that there are no outcomes in \\(A\\intersect B\\), i.e., that \\(A \\intersect B = \\emptyset\\).\n\n\n\n3.3.2 Other Probability Rules\nThese rules all follow from the axioms (although we will not necessarily prove them all here).\n\n\nThe Addition Rule\nIf events \\(A\\) and \\(B\\) are mutually exclusive, then \\(\\Prob(A \\tor B) = \\Prob(A) + \\Prob(B)\\). That’s the additivity axiom. A more general rule holds even when the events are not mutually exclusive: \\[\n\\Prob(A \\tor B) = \\Prob(A) + \\Prob(B) - \\Prob(A \\tand B) \\; .\n\\]\n\n\nThe Complement Rule\n\\[\n\\Prob(\\tnot  A) = 1 - \\Prob(A)\n\\]\n\n\nThe Equally Likely Rule\nIf the sample space consists of \\(n\\) equally likely outcomes, then the probability of an event \\(A\\) is given by\n\\[\n\\Prob(A)\n= \\frac{ \\mbox{number of outcomes in $A$}}{n}\n= \\frac{ \\card{A} }{\\card{S} }\\; .\n\\]\nWarning: One of the most common mistakes in probability is to apply this rule when the outcomes are not equally likely.\n\n\n\nExample 3.4 Here are several examples where we can (and cannot) use the Equally Likely Rule.\n\nCoin Toss: \\(\\Prob(\\mbox{heads}) = \\frac{1}{2}\\) if heads and tails are equally likely.\nRolling a Die: \\(\\Prob(\\mbox{even}) = \\frac{3}{6}\\) if the die is fair (each of the six numbers equally likely to occur).\nSum of two Dice: the sum is a number between 2 and 12, but these numbers are NOT equally likely.\nThere are 36 equally likely combinations of two dice:\n\n\n\n\n\n\n\n\n\nLet \\(X\\) be the sum of two dice.\n\n\\(\\Prob(X = 3) = \\frac{2}{36} = \\frac{1}{18}\\)\n\\(\\Prob(X = 7) = \\frac{6}{36} = \\frac{1}{6}\\)\n\\(\\Prob(\\mbox{doubles}) = \\frac{6}{36} = \\frac{1}{6}\\)\n\nPunnet Squares\nThis example comes from animal or human genetics. Here, we consider a gene with two alleles: A is the dominant allele, and a is the recessive one. Each individual has two copies of every gene, so there are three possible combinations of alleles (called “genotypes”): AA, Aa, and aa. AA and Aa individuals have the dominant A physical characteristic (called the “phenotype”); aa individuals have the recessive a phenotype. Imagine that two Aa individuals mate and produce offspring. In this Aa \\(\\times\\) Aa cross, if A is the dominant allele, then the probability of the dominant phenotype is \\(\\frac{3}{4}\\), and the probability of the recessive phenotype is \\(\\frac{1}{4}\\) because each of the four possible crossings is equally likely.\n\n\n\nA Punnet Square\n\n\n\n\n\n\n\n\nA\na\n\n\nA\nAA\nAa\n\n\na\naA\naa\n\n\n\n\n\n\nhttp://xkcd.com/634/\n\n\n\n\n\n\n\nExample 3.5 Q. Suppose a family has two children and one of them is a boy. What is the probability that the other is a girl?\nA.     We’ll make the simplifying assumption that boys and girls are equally likely (which is not exactly true). Under that assumption, there are four equally likely families: BB, BG, GB, and GG. But only three of these have at least one boy, and we already know our family has at least one boy, so our sample space is really \\(\\{BB, BG, GB\\}\\). Of these, two have a girl as well as a boy. So the probability is \\(2/3\\).\nWe illustrate the restricted sample space (3 outcomes in a box) in the figure below. Two of these three outcomes have at least one girl. (See Equation 3.1.)\n\\[\n\\mbox{GG} \\ \\ \\  \\fbox{BG \\ \\ \\ GB \\ \\ \\ BB}\n\\tag{3.1}\\]\nWe can also think of this in a different way. In our original sample space of four equally likely families,\n\\[\n\\begin{aligned}\n\\evProb{at least one girl} & =  3/4 \\; , \\\\ % \\mbox{ , and } \\\\\n\\evProb{at least one girl \\emph{and} at least one boy} & =  2/4 \\; , \\tand\\\\\n\\frac{2/4}{3/4} & =  2/3 \\; ;\n\\end{aligned}\n\\]\nso \\(2/3\\) of the time when there is at least one boy, there is also a girl. We will denote this probability as \\(\\Prob(\\mbox{at least one girl} \\mid \\mbox{at least one boy})\\). We’ll read this as “the probability that there is at least one girl given that there is at least one boy”. See Figure 3.1 and Definition 3.1.\n\n\n\n\n\n\nFigure 3.1: A Venn diagram illustrating the definition of conditional probability. \\(\\Prob(A \\mid B)\\) is the ratio of the area of the football shaped region that is both shaded and striped (\\(A \\intersect B\\)) to the area of the shaded circle (\\(B\\)).\n\n\n\n\n\n\n\n\nDefinition 3.1 Let \\(A\\) and \\(B\\) be two events such that \\(\\Prob(B) \\neq 0\\).\nThe of \\(A\\) given \\(B\\) is defined by \\[\n\\Prob(A \\mid B) = \\frac{\\Prob(A \\intersect B) }{ \\Prob(B) } ; .\n\\] If \\(\\Prob(B) = 0\\), then \\(\\Prob(A \\mid B)\\) is undefined.\n\n\n\nExample 3.6 A class of \\(5\\)th graders was asked what color should be used for the class T-shirt, red or purple. The table below contains a summary of the students’ responses:\n\n\n\n\n\n\n\n\n\nRed\nPurple\n\n\nGirls\n\\(7\\)\n\\(9\\)\n\n\nBoys\n\\(10\\)\n\\(8\\)\n\n\n\nQ. Suppose we randomly select a student from this class. Let \\(R\\) be the event that a child prefers a red T-shirt. Let \\(B\\) be the event that the child is a boy, and let \\(G\\) be the event that the child is a girl.\nExpress each of the following probabilities in words and determine their values:\n\n\n\n\\(\\Prob(R)\\),\n\\(\\Prob(R \\mid B)\\),\n\\(\\Prob(B \\mid R)\\),\n\\(\\Prob(R \\mid G)\\),\n\\(\\Prob(G \\mid R)\\),\n\\(\\Prob(B \\mid G)\\).\n\n\n\n\n\nA. The conditional probabilities can be computed in two ways. We can use the formula from the definition of conditional probability directly, or we can consider the condition event to be a new, smaller sample space and read the conditional probability from the table.\n\n\n\\(\\Prob(R) = 17/34 = 1/2\\) because \\(17\\) of the \\(34\\) kids prefer red\nThis is the probability that a randomly selected student prefers red\n\\(\\displaystyle \\Prob(R \\mid B) = \\frac{10/34}{18/34} = \\frac{10}{18}\\) because \\(10\\) of the \\(18\\) boys prefer red\nThis is the probability that a randomly selected boy prefers red\n\\(\\displaystyle \\Prob(B \\mid R)= \\frac{10/34}{17/34} = \\frac{10}{17}\\) because \\(10\\) of the \\(17\\) students who prefer red are boys.\nThis is the probability that a randomly selected student who prefers red is a boy.\n\\(\\displaystyle \\Prob(R \\mid G) = \\frac{7/34}{16/34} = \\frac{7}{16}\\) because \\(7\\) of the \\(16\\) girls prefer red\nThis is the probability that a randomly selected girl prefers red\n\\(\\displaystyle \\Prob(G \\mid R) = \\frac{7/34}{17/34} = \\frac{7}{17}\\) because \\(7\\) of the \\(17\\) kids who prefer red are girls.\nThis is the probability that a randomly selected kid who prefers red is a girl.\n\\(\\displaystyle \\Prob(B \\mid G) = \\frac{0}{16/34} = 0\\) because none of the girls are boys.\nThis is the probability that a randomly selected girl is a boy.\n\n\n\n\nOne important use of conditional probability is as a tool to calculate the probability of an intersection.\n\nLemma 3.1 Let \\(A\\) and \\(B\\) be events with non-zero probability. Then\n\\[\n\\begin{aligned}\n\\Prob(A \\intersect B) & = \\Prob(A) \\cdot\\Prob(B \\mid A)\n\\\\ & = \\Prob(B) \\cdot\\Prob(A \\mid B) ;.\n\\end{aligned}\n\\]\nThis follows directly from the definition of conditional probability by a little bit of algebra and can be generalized to more than two events.\n\n\n\nExample 3.7 Q. If you roll two standard dice, what is the probability of doubles? (Doubles is when the two numbers match.)\nA. Let \\(A\\) be the event that we get a number between \\(1\\) and \\(6\\) on the first die. So \\(\\Prob(A) = 1\\). Let \\(B\\) be the event that the second number matches the first. Then the probability of doubles is \\(\\Prob(A \\intersect B) = \\Prob(A) \\cdot\\Prob(B \\mid A) = 1 \\cdot\\frac{1}{6} = \\frac{1}{6}\\) since regardless of what is rolled on the first die, \\(1\\) of the \\(6\\) possibilities for the second die will match it.\n\n\nExample 3.8 Q. A \\(5\\)-card hand is dealt from a standard \\(52\\)-card deck. What is the probability of getting a flush (all cards the same suit)?\nA. Imagine dealing the cards in order. Let \\(A_i\\) be the event that the \\(i\\)th card is the same suit as all previous cards. Then\n\\[\n\\begin{aligned}\n\\evProb{flush} & =  \\Prob(A_1\n  \\intersect  A_2\n  \\intersect  A_3\n  \\intersect  A_4\n  \\intersect  A_5) \\\\\n  & =  \\Prob(A_1)\n    \\cdot \\Prob(A_2 \\mid A_1)\n    \\cdot \\Prob(A_3 \\mid A_1 \\intersect A_2)\n    \\\\ & \\quad\n    \\cdot \\Prob(A_4 \\mid A_1 \\intersect A_2 \\intersect A_3)\n    \\\\ & \\quad\n    \\cdot \\Prob(A_5 \\mid A_1 \\intersect A_2 \\intersect A_3 \\intersect A_4) \\\\\n    & = 1 \\cdot \\frac{12}{51} \\cdot \\frac{11}{50} \\cdot \\frac{10}{49}\n    \\cdot \\frac{9}{48} \\;\n\\end{aligned}\n\\]\n\n\nExample 3.9 Q. In a bowl are 4 red Valentine hearts and 2 blue Valentine hearts.\nIf you reach in without looking and select two of the Valentines, let \\(X\\) be the number of blue Valentines. Fill in the following probability table.\n\n\n\n\n\n\n\n\n\nvalue of \\(X\\)\n0\n1\n2\n\n\nprobability\n\n\n\n\n\n\nA.\n\\[\n\\begin{aligned}\n\\Prob(X = 2) &=\n\\Prob(\\mbox{first is blue} \\tand \\mbox{second is blue})\n\\\\\n&= \\Prob(\\mbox{first is blue}) \\cdot\n      \\Prob(\\mbox{second is blue} \\mid \\mbox{first is blue})\n\\\\\n&= \\frac26 \\cdot \\frac15\n\\\\\n&= \\frac{2}{30} \\;.\n\\end{aligned}\n\\]\nSimilarly\n\\[\n\\begin{aligned}\n\\Prob(X = 0) &=\n\\Prob(\\mbox{first is red} \\tand \\mbox{second is red})\n\\\\\n&= \\Prob(\\mbox{first is red}) \\cdot\n     \\Prob( \\mbox{second is red} \\mid \\mbox{first is red})\n\\\\\n&= \\frac46 \\cdot \\frac35\n\\\\\n&= \\frac{12}{30}\n\\end{aligned}\n\\]\nFinally, \\(\\Prob(X = 1) = 1 - \\Prob(X = 0) - \\Prob(X = 2) = 1 - \\frac{14}{30} = \\frac{16}{30}\\).\nWe can represent this using a tree diagram as well.\n\n\n\n\n\nThe edges in the tree represent conditional probabilities which we can multiply together to the probability that all events on a particular branch happen. The first level of branching represents what kind of Valentine is selected first, the second level represents the second selection.\n\n\n\nExample 3.10 Q. Suppose a test correctly identifies diseased people \\(99\\)% identifies healthy people \\(98\\)% Furthermore assume that in a certain population, one person in \\(1000\\) has the disease. If a random person is tested and the test comes back positive, what is the probability that the person has the disease?\nA. We begin by introducing some notation. Let \\(D\\) be the event that a person has the disease. Let \\(H\\) be the event that the person is healthy. Let \\(+\\) be the event that the test comes back positive (meaning it indicates disease – probably a negative from the perspective of the person tested). Let \\(-\\) be the event that the test is negative.\n\n\n\\(\\Prob(D) = 0.001\\), so \\(\\Prob(H) = 0.999\\).\n\\(\\Prob(+ \\mid D) = 0.99\\), so \\(\\Prob(- \\mid D) = 0.01\\).\n\\(\\Prob(+ \\mid D)\\) is called the sensitivity of the test. (It tells how sensitive the test is to the presence of the disease.)\n\\(\\Prob(- \\mid H) = 0.98\\), so \\(\\Prob(+ \\mid H) = 0.02\\).\n\\(\\Prob(- \\mid H)\\) is called the specificity of the test. \n\\(\\!\\!\\!\\!\\!\\)\n\n\\[\n\\begin{aligned}\n\\Prob(D \\mid +) & = \\frac{\\Prob(D \\intersect +)}{\\Prob(+)}\n\\\\[5mm]\n&= \\frac{\\Prob(D) \\cdot \\Prob(+ \\mid D)}{\\Prob(D \\intersect +)  +\n    \\Prob(H \\intersect +)  }\n\\\\[5mm]\n& = \\frac{0.001 \\cdot 0.99}{0.001 \\cdot 0.99 + 0.999 \\cdot 0.02}\n                      =  0.0472\n\\end{aligned}\n\\]\nA tree diagram is a useful way to visualize these calculations.\n\n\n\n\n\nThis low probability surprises most people the first time they see it. This means that if the test result of a random person comes back positive, the probability that that person has the disease is less than \\(5\\)%, even though the test is “highly accurate”. This is one reason why we do not routinely screen an entire population for a rare disease – such screening would produce many more false positives than true positives.\nOf course, if a doctor orders a test, it is usually because there are some other symptoms. This changes the a priori probability that the patient has the disease. Exercise 3.9 gives you a chance to explore this further.\n\n\n\n\n\n3.3.3 Independence\n\nDefinition 3.2 Let \\(A\\) and \\(B\\) be two events such that \\(\\Prob(B) = \\Prob(B \\mid A)\\). Such events are called independent.\n\n\nWhen events are independent, then\n\\[\n\\Prob(A \\tand B) = \\Prob(A) \\cdot \\Prob(B \\mid A) = \\Prob(A) \\cdot \\Prob(B) \\;.\n\\] This makes probability calculations much simpler – but it only applies for independent events.\n\nExample 3.11 Q. What is the probability of rolling double sixes with standard 6-sided dice?\nA. Let \\(A\\) be the event that the first die is a 6 and let \\(B\\) be the event that the second die is a 6. Since \\(A\\) and \\(B\\) are independent, \\(\\Prob(A \\tand B) = \\Prob(A) \\cdot \\Prob(B) = \\frac16 \\cdot \\frac16 = \\frac{1}{36}\\).\n\n\nExample 3.12 Q. What is the probability of flipping a coin five times and getting 5 heads?\nA. Since each coin toss is independent of the others, the probability of getting five heads is the product of the probabilities of each coin coming up heads:\n\\[\n\\Prob(\\mbox{5 heads in 5 flips}) = (0.5)^5 = 0.03125\n\\]\n\n\nExample 3.13 Q. A manufacturer claims that 99% of its parts will still be functioning properly two years after purchase. If you purchase 10 of these parts, what is the probability that all 10 of them are still functioning properly two years later (assuming the manufacturer’s claim is correct)?\nA. Let \\(G_i\\) be the event that part \\(i\\) is still functioning properly after two years. We want to calculate\n\\[\n\\Prob(G_1 \\tand G_2\n\\tand\\cdots\\tand G\\_{10});.\n\\] If we assume the lifetimes of the parts are independent, then\n\\[\n\\begin{aligned}\n\\Prob(G_1 \\tand G_2 \\tand\\cdots\\tand G_{10})\n& =\n\\underbrace{.99 \\cdot.99 \\cdot.99 \\cdots.99}_{\\mbox{10 of these}}\n\\\\\n& = .99^{10} \\\\\n& = 0.9043821\\;.\n\\end{aligned}\n\\]\nThe independence assumption may or may not be valid. That depends on the manufacturing process. For example, if the primary way a part goes bad is that the package is dropped during shipping, then if you by a box of 10 and the first part is bad, they will all be bad. And if the box was handled carefully and never dropped, and the first part used is good, they will likely all be good. So in that extreme case, the probability that all 10 are functioning properly after two years is 99%."
  },
  {
    "objectID": "03-probability.html#exercises",
    "href": "03-probability.html#exercises",
    "title": "3  Probability",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\n\nExercise 3.1 Free throw Amy\nAmy is a 92% free throw shooter. If she shoots 100 free throws after practice, what is the probability that she makes at least 95 of them? Use simulation to estimate this probability.\n(You can use rflip() to simulate shooting free throws. The prob argument lets you set the probability. In this case, you need to set it to \\(0.92\\). Then think of a head as a made free throw and a tail as a missed free throw.)\n\n\n\n\nSolution. \nset.seed(123)    # so we get the same simulated results each time we compile.\nsims <- do(1000) * rflip(100, prob = 0.92)\ntally( ~ heads > 95, data = sims)\n\nheads > 95\n TRUE FALSE \n   94   906 \n\n\nSo the probability that Amy makes more than 95 shots in 100 attempts is approximately 0.094\n\n\n\nExercise 3.2 Dice\n\n\nUse simulation to estimate the probability of rolling a difference of 2 when rolling two fair six-sided dice.\nMake a histogram showing the results for all of the possible differences.\n\n\n\n\n\n\n\nSolution. \nd <- die1 - die2\ntally( ~ d )\n\nd\n  -5   -4   -3   -2   -1    0    1    2    3    4    5 \n 279  610  765 1092 1409 1634 1423 1165  792  555  276 \n\nprop( ~ (d == 8) )\n\nprop_TRUE \n        0 \n\ngf_histogram( ~ d, binwidth = 1)    # setting width is important for integer data\n\n\n\n\n\n\n\nExercise 3.3 Cards\nUse simulation to estimate the probability that when dealing 5 cards from a standard (well-shuffled) deck of 52 cards all five are diamonds.\nYou can simulate the deck of cards using the numbers 1 through 52 and consider the numbers 1 through 13 to be the diamonds. Instead of using resample(), which would allow you to get the same card more than once, we need to use sample(), which does not. (You can also use deal() which does the same thing.)\n\nsample(1:52, 5)\n\n[1] 10 38 43 25 37\n\nsample(1:52, 5)\n\n[1] 19 15 36 24 31\n\ndeal(1:52, 5)\n\n[1] 46 43 20 29 40\n\ndeal(1:52, 5)\n\n[1] 17 24 10 11 37\n\n\nThere is another way to make the calculation, using the function sum(). R can tell you how many cards are below 14 using sum() because R turns TRUE into 1 and FALSE into 0 when you do a sum.\n\nsum( sample(1:52, 5) < 14 ) \n\n[1] 2\n\nsum( sample(1:52, 5) < 14 )\n\n[1] 0\n\nsum( sample(1:52, 5) < 14 )\n\n[1] 1\n\n\nYou can use do() to do this many times. (Three is not many! We just do a small number here for illustration purposes.)\n\ndo(3) * sum( sample( 1:52, 5 ) < 14 )\n\n\n\n  \n\n\n\n\n\n\n\nSolution. \ntally( ~ sum, do(10000) * sum(sample(1:52, 5) < 14) )\n\nsum\n   0    1    2    3    4    5 \n2218 3992 2826  853  107    4 \n\n\n\n\n\nExercise 3.4 Quality control\nParts in a manufacturing plant go through two quality control checks before they are shipped. 99% of parts pass inspection A and 98% parts pass inspection B. 0.5% fail both inspections.\nWhat percentage of parts pass both inspections?\n\n\n\nSolution. \n$\\Prob(\\mbox{fail at least one}) = \\Prob(\\mbox{fail A or fail B}) =\n\\Prob(\\mbox{fail A}) + \\Prob(\\mbox{fail B}) - \\Prob(\\mbox{fail both})\n= 0.01 + 0.02 - 0.05 = 0.025$.\n\nSo $\\Prob(\\mbox{pass both}) = 1 - 0.025 = 0.975$.\n\n\n\nExercise 3.5 Sum of dice\nLet \\(X\\) be the sum of the results of rolling two fair six-sided dice.\n\n\nWhat is \\(\\Prob(X \\mbox{ is even} \\tand X < 5)\\)?\nWhat is \\(\\Prob(X \\mbox{ is even} \\tor X < 5)\\)?\n\n\n\n\n\n\nSolution. The probability that \\(X\\) is even and less than five is \\(\\Prob(X = 2 \\tor X = 4) = 1/36 + 3/36 = 4/36 = 0.1111111\\).\nThe probability that \\(X\\) is even or less than five is \\(\\Prob(\\mbox{X is even}) + \\Prob(X = 3) = 18/36 + 2/36 = 20/36 = 0.5555556\\).\n\n\n\nExercise 3.6 Difference in dice\nLet \\(Y\\) be the difference between the larger and smaller number when two fair dice are rolled. (So if you roll a 2 and a 4, then the value of \\(Y\\) is 2.)\n\n\nWhat is \\(\\Prob(Y = 2)\\)?\nWhat are the other possible values of \\(Y\\)?\nCalculate the probability for each possible value of \\(Y\\) and put those values in a table.\n\n\n\n\n\n\nSolution. \\(Y = 2\\) for rolls of \\((3,1)\\), \\((4,2)\\), \\((5,3)\\), and \\((6,4)\\). Each of these can also happen in the other order, so the probability is \\(8/36 = 2/9 = 0.2222222\\).\n\n\n\nvalue of \\(Y\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\nprobability\n6/36\n10/36\n8/36\n6/36\n4/36\n2/36\n\n\n\n\nc(6,10,8,6,4,2) / 36\n\n[1] 0.16666667 0.27777778 0.22222222 0.16666667 0.11111111 0.05555556\n\n\n\n\n\nExercise 3.7 Kids\nFor the probabilities below, you may assume the that for each birth, the probability of having a boy or a girl is \\(1/2\\) and that each birth is independent of other births.\n\n\nSuppose a family has three kids. What is the probability that at least one of the kids is a boy?\nSuppose a family has three kids, at least one of which is a girl. Now what is the probability that at least one of the kids is a boy?\nSuppose a family has three kids, at least two of which are girls. Now what is the probability that at least one of the kids is a boy?\n\n\n\n\n\n\nExercise 3.8 **A device with two parts\nA device is assembled from two primary parts. 2% of the first type of part are defective and 3% of the other type of part are defective. The device only functions properly if both parts are functioning properly.\n\n\nWhat assumption do you need to make to calculate the probability that a device assembled in this way will function properly? Is it a reasonable assumption in this situation? Explain.\nWhat is the probability that that a device assembled in this way will function properly?\n\n\n\n\n\n\nSolution. Assuming failure of each part is independent of failure of the other, the probability that both function properly is \\(0.98 \\cdot 0.97 = 0.9506\\).\nAlternatively, if we assume that failures are mutually exclusive, then the probability of failure would be \\(0.02 + 0.03 = 0.05\\) and the probability of proper functioning would be \\(0.95\\).\nThe independence assumption is reasonable if, for example, the two parts are made by separate manufacturing processes and the device is assembled by randomly selecting a part of each type, which may or may not be a good part right from the start.\nThe mutually exclusive failure is probably harder to justify, since likely there will be some situations in which both parts fail.\n\n\n\nExercise 3.9 Testing for a disease, revisited\nIn the situation of Example Example 3.10, how does the answer change if the baseline probability of having the disease is \\(1/10\\) instead of \\(1/1000\\)? (This might be the case if a person is exhibiting symptoms, for example.)\n\n\n\n\nSolution. \n# P(D) = 0.001\n0.001 * 0.99 / (0.001 * 0.99 + 0.999 * 0.02)\n\n[1] 0.0472103\n\n# P(D) = 0.10\n0.1 * 0.9 / (0.10 * 0.99 + 0.9 * 0.02)\n\n[1] 0.7692308\n\n\n\n\n\nExercise 3.10 Smoking and lung cancer\nAccording to the CDC, “Compared to nonsmokers, men who smoke are about 23 times more likely to develop lung cancer and women who smoke are about 13 times more likely.” According to the American Lung Association: “In 2008, 21.1 million (18.3%) women smoked in the United States compared to 24.8 million (23.1%) men.”\n\n\nIf you learn that a person is a smoker and no nothing else about the person, what is the probability that the person is a woman?\nIf you learn that a woman has been diagnosed with lung cancer, and you know nothing else about her, what is the probability that she is a smoker?\nIf you learn that a man has been diagnosed with lung cancer, and you know nothing else about him, what is the probability that he is a smoker?\n\n\n\n\n\n\n\nSolution. \n# a)\n21.1 / ( 21.1 + 24.8)\n\n[1] 0.459695\n\n\nPart b is the most interesting (once you can do that, you can do part c the same way). Let \\(W\\) be the event that someone is a woman, \\(S\\) that they are a smoker, and \\(C\\) that they get cancer. Let \\(x = \\Prob( C | W \\intersect S^c)\\). Then \\(\\Prob( C | W \\intersect S ) = 13x\\).\n\\[\n\\begin{aligned}\n    \\Prob( S | W \\intersect C )\n    & = \\frac{ \\Prob(S \\intersect W \\intersect C) }{\\Prob(W \\intersect C)}\n    \\\\\n    & = \\frac{ \\Prob(S \\intersect W \\intersect C) }\n    {\\Prob(S \\intersect W \\intersect C) + \\Prob( S^c \\intersect W \\intersect C)}\n\\end{aligned}\n\\]\nSo we just need to compute the two probabilities in the denominator.\n\\[\n\\begin{aligned}\n    \\Prob(S \\intersect W \\intersect C)\n    &= \\Prob(W) \\cdot \\Prob(S \\mid W) \\cdot \\Prob(C \\mid W \\intersect S)\n    \\\\\n    &= \\Prob(W) \\cdot 0.183 \\cdot 13x\n    \\\\\n    \\Prob(S^c \\intersect W \\intersect C)\n    &= \\Prob(W) \\cdot \\Prob(S^c \\mid W) \\cdot \\Prob(C \\mid W \\intersect S^c)\n    \\\\\n    &= \\Prob(W) \\cdot 0.817 \\cdot x\n\\end{aligned}\n\\]\nAfter factoring out \\(x \\cdot \\Prob(W)\\), the arithmetic is now easy:\n\n# b) After factoring out a constant from numerator and denominator we are left with\n0.183 * 13 / ( 0.183 * 13 + .817 * 1 )\n\n[1] 0.744368\n\n# c) After factoring out a constant from numerator and denominator we are left with\n0.231 * 23 / ( 0.231 * 23 + .769 * 1 )\n\n[1] 0.8735613\n\n\nNote: Another approach to part b is to consider the sample space to be only the women. If you do it that way, you can avoid mentioning any probabilities involving \\(W\\). (In the end, they factor out anyway.) Of course, you can do a similar thing for the men.\n\n\n\nExercise 3.11 Parts by day\nA manufacturing plant has kept records that show that the number of parts produced each day and on the proportion of parts that are defective.\n\n\nIf you order a part from this company, what is the probability that it was produced on a Monday or a Thursday?\nIf you order a part from this company and it is defective, what is the probability that it was produced on a Monday or a Thursday?\nIf you order a part from this company and it functions properly, what is the probability that it was produced on a Monday or Thursday?\n\n\n\nExpress your answers to 3 significant digits and avoid internal rounding.\n\n\n\nSolution. You may find a tree diagram useful here to visualize these probabilities.\n\n# part a\n.20 + .27\n\n[1] 0.47\n\n# part b: P( Wed-Thur | defective ) = P( Wed-Thur and defective ) / P(defective)\na <- .20 * .02 +       # Monday and defective\n     .27 * .03         # Thursday and defective\nb <- .25 * .015 +      # Tuesday and defective\n     .28 * .01         # Wednesday and defective \na / (a + b)\n\n[1] 0.6487936\n\n# part c: P( Wed-Thur | good ) = P( Wed-Thur and good ) / P(good)\nc <- .20 * .98 +       # Monday and good\n     .27 * .97         # Thursday and good\nd <- .25 * .985 +      # Tuesday and good\n     .28 * .99         # Wednesday and good \nc / (c + d)\n\n[1] 0.4666021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.12 Acceptance sampling\nAn engineer orders a shipment of 100 identical parts.\nBefore accepting the shipment, he tests three of them.\nIf the they all test good, he accepts the entire shipment.\nIf any of them tests bad, he rejects the shipment.\nGiven the good price he has gotten on these parts, the engineer would be satisfied if at least 95%\n\n\nSuppose that there are 5 bad parts in the shipment. What is the probability that the shipment is rejected (even though the engineer would actually have been satisfied with the shipment)?\nSuppose that there are 10 bad parts in the shipment. What is the probability that the shipment is accepted (even though the engineer would not be satisfied with this shipment)?\n\n\n\n\n\n\n\nSolution. \n\nThe engineer will accept if all three are among the 95 good parts. The probability of accepting is\n\n\n95/100 * 94/99 * 93/98\n\n[1] 0.8559988\n\n\nThe probability of rejecting is\n\n1 - 95/100 * 94/99 * 93/98\n\n[1] 0.1440012\n\n\n\nNow we accept if all three are among the 90 good parts. The probability of accepting is\n\n\n90/100 * 89/99 * 88/98\n\n[1] 0.7265306\n\n\n\n\nWe see that this is not the most effective test. It could be improved by taking a larger sample, and similar arithmetic could be used to determine how well a new protocol works (perhaps one that takes a larger sample).\n\n\n\nExercise 3.13 M&M’s\n\nThe blue M&M was introduced in 1995. Before then, the color mix in a bag of plain M&Ms was 30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan. Afterward it was 24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown.\nA friend of mine has two bags of M&Ms, and he tells me that one is from 1994 and one from 1996. He won’t tell me which is which, but he gives me one M&M from each bag. One is yellow and one is green. What is the probability that the yellow M&M came from the 1994 bag?\n\n\n\nSolution. Let’s use the following notation for events related to this problem:\n\n\n\n\\(G_4 =\\) green from 1994 bag\n\\(G_6 =\\) green from 1996 bag\n\\(Y_4 =\\) yellow from 1994 bag\n\\(Y_6 =\\) yellow from 1996 bag\n\n\n\nThen our question becomes \\[\n\\begin{aligned}\n\\Prob( Y_4 \\tand G_6 \\mid\n       (Y_4 \\tand G_6) \\tor (Y_6 \\tand G_4) )\n       & =\n\\frac{\\Prob( Y_4 \\tand G_6 ) }\n     {\\Prob( Y_4 \\tand G_6) \\tor (Y_6 \\tand G_4)}\n\\\\\n& = \\frac{\\Prob( Y_4 \\tand G_6 ) }\n     {\\Prob(Y_4 \\tand G_6) + \\Prob(Y_6 \\tand G_4)}\n\\end{aligned}\n\\]\nThe individual probabilities can be worked out as \\[\n\\Prob(Y_4 \\tand\nG_6) = \\Prob(Y_4) \\cdot \\Prob(G_6 \\mid Y_4) = .20 \\cdot .20 = 0.04\n\\]\n\\[\n\\Prob(Y_6 \\tand G_4) = \\Prob(Y_6) \\cdot \\Prob(G_4 \\mid Y_6) = .14\n\\cdot .10 = 0.014\n\\]\nPutting this all together we get\n% O: .20 Y, .10 G % N: .14 Y, .20 G\n\n.20 * .20 / (.20 * .20 + .14 * .10) \n\n[1] 0.7407407\n\n\nA tree diagram could be used to depict these probabilities as well."
  },
  {
    "objectID": "04-random-variables.html#discrete-random-variables",
    "href": "04-random-variables.html#discrete-random-variables",
    "title": "4  Random Variables",
    "section": "4.1 Discrete Random Variables",
    "text": "4.1 Discrete Random Variables\nA discrete random variable takes on values from a discrete set of possibilities, typically either a finite set or a subset of the integers. Here are some examples.\n\n\nIf we roll a die and record the number, there are six possible values, so the random variable is discrete.\nIf we keep flipping a coin until we get a head and record the number of coin tosses, then the possible values of the random variable are \\(1, 2, 3, \\dots\\) This is also a discrete random variable.\n\n\n\nFor each of the possible values of a discrete random variable, there is some probability of that value occurring. So to specify a discrete random variable, we need to specify those probabilities. When there are only a small number of possible value, we can do this with a table.\n\n\n\n\n\n\n\n\n\n\nvalue of \\(X\\)\n0\n1\n2\n\n\nprobability\n0.2\n0.5\n0.3\n\n\n\n\nThis is really just one way of describing a function, called the probability mass function (or pmf). The pmf satisfies \\[\nf(x) = \\Prob(X = x)\n\\tag{4.1}\\]\nSometimes instead of providing a table, we will be able to specify the pmf using a formula. For example, we could define a pmf \\(g\\) by\n\\[\ng(y) = (2 - |1-y|) / 4 \\mbox{ for $y \\in \\{0, 1, 2\\}$}\n\\] which is the same as specifying \\(g\\) with the following table.\n\n\n\n\n\n\n\n\n\n\nvalue of \\(Y\\)\n0\n1\n2\n\n\nprobability\n0.25\n0.5\n0.25\n\n\n\n\nHere’s one more example. Let \\(W\\) be a random variable that can take on any integer value and has pmf given by\n\\[\nh(w) = \\left(\\frac{1}{2}\\right)^{w+1} \\mbox {for $w = 0, 1, 2, 3, \\dots$}\n\\]\nSo, for example, \\(\\Prob(W = 2) = h(2) = \\left(\\frac{1}{2} \\right)^3 = \\frac18\\).\nProbabilities can be obtained from the pmf by adding:\n\n\\(\\Prob( X > 0 ) = 0.5 + 0.3 = 0.8\\)\n\\(\\Prob( Y > 0) = 0.5 + 0.25\\)\n\\(\\Prob(W > 0) = \\frac{1}{4} + \\frac{1}{8} + \\frac{1}{16} + \\cdots = \\frac{1}{2}\\)\n\\(\\Prob(W > 0) = 1 - \\Prob(W = 0) = 1 - \\frac{1}{2} = \\frac{1}{2}\\)\n\nThe only restrictions on a pmf are that\n\n\nthe values must all be non-negative, and\nthe sum (over all possible values of the random variable) must be 1.\n\n\n\nThat way the probabilities will behave the way probabilities should.\n\nExample 4.1 Q. Amy is a 92% free throw shooter. We watch her take shots until she misses and let \\(X\\) be the number of shots. What is the pmf for \\(X\\)?\nA. This time our table would be infinite (since there is no limit to how many consecutive free throws Amy might make), so we won’t be able to write the whole table down. But we can work out the first few probabilities:\n\n\n\\(f(1) = \\Prob(X = 1) = 0.08\\). (She has to miss the first shot.)\n\\(f(2) = \\Prob(X = 2) = (0.92)(0.08)\\). (She has to make the first and miss the second.)\n\\(f(3) = \\Prob(X = 3) = (0.92)^2(0.08)\\). (She has to make the first two, then miss the third.)\n\n\n\nAt this point, we see there is a general pattern that allows us to write down an algebraic form for the pmf:\n\\[\nf(x) = \\Prob(X = x) = (0.92)^{x-1}(0.08)\n\\] where \\(x\\) is a positive integer. (If \\(x\\) is not an integer, or \\(x < 1\\), then \\(f(x) = 0\\), since those values are not possible.)"
  },
  {
    "objectID": "04-random-variables.html#continuous-random-variables",
    "href": "04-random-variables.html#continuous-random-variables",
    "title": "4  Random Variables",
    "section": "4.2 Continuous Random Variables",
    "text": "4.2 Continuous Random Variables\n\n4.2.1 Density histograms, density plots, density functions\nA histogram is a simple picture describing the “density” of data. Histogram bars are tall in regions where there is more data – i.e., where the data are more “dense”.\n\nlibrary(alr4)\ngf_histogram(  ~ Duration, data = oldfaith)\ngf_dhistogram( ~ Duration, data = oldfaith)\n\n\n\n\n\n\n\n(a) A frequency histogram.\n\n\n\n\n\n\n\n(b) A density histogram.\n\n\n\n\nFigure 4.1: Two histograms of the Old Faithful eruption data.\n\n\n\nThe density scale is the same scale that is used by gf_dens() and gf_density(), and it is the default scale for histograms created using gf_dhistogram().\n\nlibrary(alr4)\ngf_dhistogram( ~ Duration, data = oldfaith, binwidth = 20, center = 110) |>\ngf_dens( ~ Duration, data = oldfaith)\n\n\n\n\nFigure 4.2: A histogram with an overlaid density curve.\n\n\n\n\nThe density scale is chosen so that the area of each rectangular bar (width times height) is equal to the proportion of the data set represented by the rectangle.\n\nExample 4.2 Q. Use the histogram of Old Faithful eruption times to estimate the proportion of eruptions that last between 100 and 120 seconds.\nA. In our histogram of Old Faithful eruption durations, the bar corresponding to the bin from 100–120 appears to have a height of about 0.09. That gives an area of 0.18 and indicates that approximately 18% of the eruptions last between 100 and 120 seconds.\n\ntally( ~ ( 100 < Duration & Duration <= 120), data = oldfaith, format = \"prop\" )\n\n(100 < Duration & Duration <= 120)\n     TRUE     FALSE \n0.1888889 0.8111111 \n\n\n\n\nThe key idea behind the density scale can be expressed as\n\n\nProbability \\(=\\) area\n\n\n\nThis association of area with probability means that the total area of all the bars will always be equal to 1 if we use the density scale.\nIt also provides us with a way to describe a distribution with a mathematical function.\n\nDefinition 4.1 Let \\(f: \\reals \\to \\reals\\) be a function such that\n\n\\(f(x) \\ge 0\\) for all \\(x\\),\n\\(\\displaystyle \\int_{-\\infty}^{\\infty} f(x) \\; dx = 1\\).\n\nThen \\(f\\) is called a density function (or probability density function, abbreviated pdf) and describes a continuous random variable \\(X\\) such that\n\\[\n   \\Prob(a \\le X \\le b) = \\int_a^b f(x) \\; dx \\;.\n\\tag{4.2}\\]\n\n\n\nExample 4.3 Q. Let \\(f\\) be defined by \\[\n    f(x) = \\begin{cases}  \n        1 - |x| & x \\in [-1,1] \\\\\n        0 & \\mbox{otherwise}\\\\\n    \\end{cases}\n\\] Show that \\(f\\) is a density function. Let \\(X\\) be the associated random variable, and compute the following probabilities:\n\n\\(\\Prob(X \\le 0)\\)\n\\(\\Prob(X \\le 1)\\)\n\\(\\Prob(X \\le \\frac{1}{2})\\)\n\\(\\Prob(-\\frac{1}{2} \\le X \\le \\frac{1}{2})\\)\n\nA. While we could set up integrals for these, it is easier to solve them using geometry.2\n\nf <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x )\ngf_fun( f(x) ~ x, xlim = c(-1.5, 1.5) )\n\n\n\n\nThe entire area under the curve can be found as the area of a triangle with base 2 and height 1. \\[\n\\int_{-\\infty}^{\\infty} f(x) \\; dx\n=\n\\int_{-1}^1 f(x) \\; dx\n=\n\\frac 12 \\cdot 2 \\cdot 1 = 1\n\\] This implies that \\(f\\) is a density function.\n\n\\(\\Prob(X \\le 0) = 1/2\\) by symmetry or using \\(\\frac{1}{2} \\cdot 1 \\cdot 1\\) to compute the area of the triangle.\n\\(\\Prob(X \\le 1) = \\int_{-\\infty}^{1} f(x) \\; dx = \\int_{-1}^{1} f(x) \\; dx = 1\\).\n\\(\\Prob(X \\le \\frac{1}{2}) = \\int_{-\\infty}^{1/2} f(x) \\; dx = \\int_{-1}^{1/2} f(x) \\; dx = 1 - \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac78\\).\n\\(\\Prob( -\\frac{1}{2} \\le X \\le \\frac 12 ) = \\int_{-1/2}^{1/2} f(x) \\; dx =  1 - \\frac{2}{8} = \\frac{3}{4}\\).\n\nWe can also let R do (numerical) integration for us. There are two ways to do this. The first method uses the integrate() function.\n\nintegrate( f, -Inf, 1 )\n\n1 with absolute error < 9.2e-05\n\n# this will be more accurate since we aren't asking R to approximate\n# something that we already know is exactly 0\nintegrate( f, -1, 1)\n\n1 with absolute error < 1.1e-14\n\nintegrate( f, -.5, .5 )\n\n0.75 with absolute error < 8.3e-15\n\n# if you just want the value without the text saying how accurate the approximation is\n# here are two equivalent ways to get it\nvalue(integrate( f, -.5, .5 ))        # extract the value using val()\n\n[1] 0.75\n\nintegrate( f, -.5, .5 ) |> value()   # |> is the \"then\" operator\n\n[1] 0.75\n\n\nAn alternative approach uses antiD() from the mosaicCalc package.\n\nlibrary(mosaicCalc)\nF <- antiD( f(x) ~ x)\nF(1) - F(-1)        # total probability -- better be 1\n\n[1] 1\n\nF(.5) - F(-1)       # P( -1 <= X <= 0.5 )\n\n[1] 0.875\n\nF(.5) - F(-.5)      # P( -.5 <= X <= .5 )\n\n[1] 0.75\n\n\n\n\n\n\n4.2.2 Kernels\nThe kernel of a continuous random variable is a function that is a constant multiple of the pdf. The reason that these are interesting is that any kernel can be converted into a pdf by dividing by this constant. In particular, if\n\\[\n\\int_{-\\infty}^{\\infty} k(x) \\; dx = A \\;,\n\\] then \\(k\\) is the kernel of a random variable with pdf \\[\nf(x) = \\frac{k(x)}{A} \\;.\n\\]\n\nExample 4.4 Q. The kernel of a random variable is given by\n\\[\nk(x) = x^2 \\; \\boolval{ x \\in [0,2] } \\; .\n\\] Determine the pdf.\nA. First we determine the value of the integral \\[\n\\int_{-\\infty}^{\\infty} k(x) \\; dx \\;.\n\\]\n\nk <- makeFun( x^2 * ( 0 <= x & x <= 2) ~ x )\nintegrate( k, 0, 2)\n\n2.666667 with absolute error < 3e-14\n\nK <- antiD(k(x) ~ x, lower.bound = 0)\nK(2)\n\n[1] 2.666667\n\n\n\n\n\n\n\nFigure 4.3: A kernel function.\n\n\n\n\nSince the total area is \\(8/3\\), if \\(\\frac{k(x)}{8/3}\\) is the pdf.\n\n\n\n4.2.3 Cumulative distribution functions\n\n\n\n\n\n\n\n\n\nIf \\(X\\) is a random variable, then the cumulative distribution function (cdf) for \\(X\\), often denoted \\(F_X\\), is the function defined by \\[\nF_X(x)  = \\Prob(X \\le  x)\n\\] That is, the output of the cdf reports the probability of being below a particular value.\n\n\nFor a continuous random variable, the cdf is a particular anti-derivative of the pdf. The derivative of the cdf is the pdf.\n\nExample 4.5 Continuing with our previous example, if we choose -1 as our lower endpoint, then the anti-derivative will be the cdf.\n\nf <- makeFun((1 - abs(x)) * (abs(x) <= 1) ~ x)\nF <- antiD( f(x) ~ x, lower.bound = -1)   # We can use -1 instead of -Inf here.\nF(-1)               # this should be 0 since we chose -1 as the lower bound.\n\n[1] 0\n\nF(1)                # P(X <= 1); should be 1\n\n[1] 1\n\nF(.5)               # P(X <= 0.5)\n\n[1] 0.875\n\nF(.5) - F(-.5)      # P( -0.5 <= X <= 0.5 )\n\n[1] 0.75\n\n\n\n\n\nWe have already seen that we can use a pdf \\(f\\) to calculate probabilities via integration, and that there is a special anti-derivative of \\(f\\) called the cdf such that the cdf \\(F\\) satisfies \\[\nF(x) = \\Prob(X \\le x)\n\\] This function can also be used to compute probabilities, since \\[\n\\Prob(a \\le X \\le b) = \\int_a^b f(x) \\; dx = F(b) - F(a)\n\\] Indeed, once we learn how to get the cdf function in R this will be our primary way to calculate probabilities in applications."
  },
  {
    "objectID": "04-random-variables.html#mean-and-variance",
    "href": "04-random-variables.html#mean-and-variance",
    "title": "4  Random Variables",
    "section": "4.3 Mean and Variance",
    "text": "4.3 Mean and Variance\n\n4.3.1 The mean of a random variable\nThe definition for the mean of a random variable will be motivated by the calculation of a mean of some data.\n\nExample 4.6 Q. Suppose a student has taken \\(10\\) courses and received \\(5\\) A’s, \\(4\\) B’s, and \\(1\\) C. Using the traditional numerical scale where an A is worth \\(4\\), a B is worth \\(3\\), and a C is worth \\(2\\), what is this student’s GPA (grade point average)?\nA. The first thing to notice is that \\(\\frac{4 + 3 + 2}{3} = 3\\) is not correct. We cannot simply add up the values and divide by the number of values. Clearly this student should have a GPA that is higher than \\(3.0\\), since there were more A’s than C’s.\nConsider now a correct way to do this calculation:\n\\[\n\\begin{aligned}\n    \\mbox{GPA} &= \\frac{4 + 4 + 4 + 4 + 4 + 3 + 3 + 3 + 3 + 2}{10}\n    \\\\[2mm]\n    & = \\frac{5\\cdot 4 + 4\\cdot 3 + 1 \\cdot 2}{10} \\\\[1mm]\n    & = \\frac{5}{10} \\cdot 4 + \\frac{4}{10}\\cdot 3 + \\frac{1}{10} \\cdot 2 \\\\\n    & = 4 \\cdot \\frac{5}{10} + 3 \\cdot \\frac{4}{10} + 2 \\cdot \\frac{1}{10} \\\\\n    & =  3.4 \\;.\n\\end{aligned}\n\\]\n\nThe key idea here is that the mean is a sum of values times probabilities.\n\\[\n\\mbox{mean} = \\sum \\mbox{value} \\cdot \\mbox{probability}\n\\] For a discrete random variable this translates to \\[\n\\E(X) = \\sum x f(x)\n\\] where the sum is taken over all possible values of \\(X\\).\nThe mean of a random variable also goes by another name: expected value. We can denote the mean of \\(X\\) by either \\(\\mu_X\\) or \\(\\E(X)\\).\n\nExample 4.7 Let \\(X\\) be discrete random variable with probabilities given in the table below.\n\n\n\n\n\n\n\n\n\n\nvalue of \\(X\\)\n0\n1\n2\n\n\n\n\nprobability\n0.2\n0.5\n0.3\n\n\n\n\n\n\n\nQ. What is the mean (expected value) of \\(X\\)?\nA. \\(E(X) = 0 \\cdot 0.2 + 1 \\cdot 0.5 + 2 \\cdot 0.3 = 0.5 + 0.6 = 1.1\\) This value reflects the fact that the random variable is larger than 1 a bit more often than it is less than 1.\n\n\n\nExample 4.8 A local charity is holding a raffle. They are selling \\(1000\\) raffle tickets for $\\(5\\) each. The owners of five of the raffle tickets will win a prize. The five prizes are valued at $\\(25\\), $\\(50\\), $\\(100\\), $\\(1000\\), and $\\(2000\\). Let \\(X\\) be the value of the prize associated with a random raffle ticket (\\(0\\) for non-winning tickets). Then:\n\n\n\\(\\evProb{the ticket wins a prize} = \\Prob(X > 0) = 5/1000\\).\n\\(\\evProb{the ticket wins the grand prize} = \\Prob(X = 2000) = 1/1000\\).\n\\(\\evProb{the ticket wins a prize worth more than \\$75} = \\Prob(X > 75) = 3/1000\\).\n\n\n\nThe expected value of a ticket is\n\\[\n  0 \\cdot\\frac{995}{1000}\n  + 25 \\cdot\\frac{1}{1000}\n  + 50 \\cdot\\frac{1}{1000}\n  + 100 \\cdot\\frac{1}{1000}\n  + 1000 \\cdot\\frac{1}{1000}\n  + 2000 \\cdot\\frac{1}{1000}\n\\]\n\n25 * .001 + 50 * 0.001 + 100 * 0.001 + 1000 * 0.001 + 2000 * 0.001\n\n[1] 3.175\n\n# R can help us set up this sum:\nsum(c(25, 50, 100, 1000, 2000) * 0.001)\n\n[1] 3.175\n\n\n\n\nWhen working with a continuous random variable, we replace the sum with an integral and replace the probabilities with our density function to get the following definition:\n\\[\n\\E(X) = \\mu_X = \\int_{-\\infty}^{\\infty} x f(x) \\; dx\n\\]\nIf you recall doing center of mass problems you may recognize this integral as the first moment. (For pdfs, we don’t need to divide by the “mass” because the total “mass” is the area under the curve, which will always be 1 for a random variable).\nNote: It is possible that the integral used to define the mean will fail to converge. In that case, we say that the random variable has no mean or that the mean fails to exist.3\n\nExample 4.9 Q. Compute the mean of our triangle distribution from Example 4.13.\nA. We simply compute the integral from the definition.\n\\[\n\\begin{aligned}\n\\E(X) & = \\int_{-1}^{1} x f(x) \\; dx\n\\\\\n    & = \\int_{-1}^{0} x (x-1) \\; dx + \\int_{0}^1 x ( 1-x ) \\; dx\n    \\\\\n    & = \\int_{-1}^{0} x^2-x \\; dx + \\int_{0}^1 x-x^2  \\; dx  \n    \\\\\n    & = \\left. \\frac{x^3}{3} - \\frac{x^2}{2} \\right|_{-1}^0\n    + \\left. \\frac{x^2}{2} - \\frac{x^3}{3} \\right|_{0}^1\n    \\\\\n    & = \\frac13 - \\frac{1}{2} + \\frac{1}{2} - \\frac13 = 0\n\\end{aligned}\n\\]\nThis isn’t surprising, by symmetry we would expect this result.\nWe could also calculate this numerically in R:\n\nf <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x)\nxf <- makeFun( x * f(x) ~ x )\nintegrate(xf, -1, 1)\n\n0 with absolute error < 3.7e-15\n\nF <- antiD( x * f(x) ~ x, lower.bound = -1)\nF(-1)  # should be 0\n\n[1] 0\n\nF(1)\n\n[1] 0\n\n\n\n\n\n\n4.3.2 Variance\nArguing similarly, we can compute the variance of a discrete or continuous random variable using\n\n\ndiscrete: \\(\\Var(X) = \\sigma^2_X = \\sum_x (x-\\mu_X)^2\\)\ncontinuous: \\(\\Var(X) = \\sigma^2_X = \\int_{-\\infty}^{\\infty} (x-\\mu_X)^2 f(x) \\; dx\\)\n\n\n\nThese can be combined into a single definition by writing \\[\n\\Var(X) = \\E((X - \\mu_X)^2) \\;.\n\\]\nNote: It is possible that the sum or integral used to define the mean (or the variance) will fail to converge. In that case, we say that the random variable has no mean (or variance) or that the mean (or variance) fails to exist.4\n\nExample 4.10 Q. Compute the variance of the triangle random variable from the Example 4.3.\nA.\n\nf <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x)\nxxf <- makeFun( (x-0)^2 * f(x) ~ x )\nintegrate(xxf, -1, 1)\n\n0.1666667 with absolute error < 1.9e-15\n\nG <- antiD( (x-0)^2 * f(x) ~ x)\nG(1) - G(-1)\n\n[1] 0.1666667\n\n\n\n\nSome simple algebraic manipulations of the sum or integral above shows that\n\\[\n\\begin{aligned}\n\\Var(X) &= \\E(X^2) - \\E(X)^2\n\\end{aligned}\n\\tag{4.3}\\]\n\nExample 4.11 Q. Compute the mean and variance of the random variable with pdf given by\n\\[\ng(x) = \\frac{3x^2}{8} \\boolval{x \\in[0,2]} \\;.\n\\]\nThis is the pdf computed in Example 4.4.\nA.\n\ng <- makeFun( (3 * x^2/8 ) * (0 <= x & x <= 2) ~ x )\nm <- antiD( x * g(x) ~ x, lower.bound = 0)(2)  # all in one step instead of defining F or G\nm\n\n[1] 1.5\n\nv <- antiD( (x - m)^2 * g(x) ~ x, m = m, lower.bound = 0)(2)\nv\n\n[1] 0.15\n\n# here's the alternate computation\nantiD( x^2 * g(x) ~ x, lower.bound = 0)(2) - m^2\n\n[1] 0.15\n\n\n\n\nAs with data, the standard deviation is the square root of the variance.\n\n\n4.3.3 Quantiles\nQuantiles solve equations of the form\n\\[\n\\int_{-\\infty}^x f(t) \\; dt = F(x) = \\Prob(X \\le x) = q\n\\]\nwhere \\(q\\) is known and \\(x\\) is unknown. So the 50th percentile (which is the 0.5-quantile or the median) is the number such that\n\\[\n\\Prob(X \\le x) = 0.5 \\;.\n\\]\n\nExample 4.12 Q. What is the 25th percentile of the triangle distribution in Example 4.3?\nA. We need to solve for \\(x\\) in the following equation:\n\\[\n0.25 = \\Prob(X \\le x)  \\; .\n\\] We can do this by working out the integral involved:\n\\[\n\\begin{aligned}\n0.25 &= \\int_{-1}^{x} 1 - |t| \\; dt\n    \\\\\n     &= \\int_{-1}^{x} 1 + t \\; dt\n     \\\\\n     &=  \\left( t + t^2/2 \\right) |_{-1}^{x}\n     \\\\\n     &=   x + x^2/2 + 1 - 1^2/2  \n     \\\\\n     &=  x + x^2/2 + 1/2\n     \\\\\n     0 & =  x^2/2 + x + 1/4\n     \\\\\n     0 & =  2x^2 + 4x + 1\n\\end{aligned}\n\\]\nSo by the quadratic formula, \\(x = \\frac{1}{2} \\sqrt{2} - 1 = -0.2928932\\).\nWe can check this by evaluating the cdf.\n\nf <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x )\nF <- antiD(f(x) ~ x, lower.bound = -1)\nx <- 1/2*sqrt(2) - 1\nF(x) \n\n[1] 0.25\n\n\nThis could also be done geometrically by solving \\(\\frac{1}{2} y^2 = \\frac14\\) and letting \\(x = -1 + y\\)."
  },
  {
    "objectID": "04-random-variables.html#some-important-families-of-distributions",
    "href": "04-random-variables.html#some-important-families-of-distributions",
    "title": "4  Random Variables",
    "section": "4.4 Some Important Families of Distributions",
    "text": "4.4 Some Important Families of Distributions\nFor now, we will consider only distributions of continuous random variables (probability density functions). We will leave set aside discrete random variables (probability mass function) until quite a bit later in the course.\nA family of distributions is a collection of distributions that share some common features. Typically, these are described by giving a pdf that has one or more parameters. A parameter is simply a number that describes (a feature of) a distribution that distinguishes between members of the family. In this section we describe briefly some of the important distributions and how to work with them in R\n\n4.4.1 Triangle Distributions\nThe example distribution in the previous section is usually referred to as a triangle distribution (or triangular distribution) because of the shape of its pdf. There are, of course, many triangle distributions. A triangle distribution is specified with three numbers: \\(a\\), the minimum; \\(b\\), the maximum, and \\(c\\), the location of the peak. A triangle distribution is symmetric if the peak is halfway between the minimum and maximum (\\(c = \\frac{a+b}{2}\\)).\nWhen \\(X\\) is a random variable with a triangle distribution, we will write \\(X \\sim\\Tri(a,b,c)\\). For many of the most common distributions, R has several functions that facilitate computation with those distributions. The triangle distributions are not in the base R distribution, but they can be added by requiring the triangle package.\nFor each distribution, there are four functions in R that always start with a single letter followed by a name for the distribution. In the case of the triangle distributions, these functions are\n\n\nTable 4.1: Triangle distributions in R\n\n\n\n\n\n\nFunction\nWhat it does\n\n\n\n\ndtriangle(x, a, b, c)\nComputes value of the pdf at x.\n\n\nptriangle(q, a, b, c)\nComputes value of the cdf at x, i.e., \\(\\Prob(X \\le \\texttt{q})\\).\n\n\nqtriangle(p, a, b, c)\nComputes quantiles, that is a value \\(q\\) so that \\(\\Prob(X \\le \\texttt{q}) = \\texttt{p}\\).\n\n\nrtriangle(n, a, b, c)\nRandomly samples n values from the \\(\\Tri(\\texttt{a},\\texttt{b},\\texttt{c})\\) distribution.\n\n\n\n\n\nExample 4.13 Q. Let \\(X \\sim\\Tri(0,4,1)\\). Use R to answer the following questions.\n\n\nPlot the pdf for \\(X\\).\nWhat is \\(\\Prob(X \\le 1)\\)?\nWhat is \\(\\Prob(X \\le 2)\\)?\nWhat is the median of \\(X\\)?\nWhat is the mean of \\(X\\)?\n\n\n\nA. The gf_dist() function in the ggformula package allows us to graph the pdf for any function R knows how to work with in the standard way. For example, here is a plot of the pdf of a \\(\\Tri(0, 4, 1)\\)-distribution.\n\nlibrary(triangle)      # a package that knows about triangle distributions\ngf_dist(\"triangle\", a = 0, b = 4, c = 1)\n\n\n\n\nHere is the R code to answer the remaining questions.\n\nptriangle(1, 0, 4, 1)   # P(X <= 1); notice that his is NOT 1/2\n\n[1] 0.25\n\nptriangle(2, 0, 4, 1)   # P(X <= 2); also NOT 1/2\n\n[1] 0.6666667\n\nqtriangle(0.5, 0, 4, 1) # median is the 0.5-quantile\n\n[1] 1.55051\n\nT <- antiD( x * dtriangle(x, 0,4,1) ~ x, lower.bound = 0)\nT(4)                    # mean of X\n\n[1] 1.666667\n\nintegrate( makeFun( x * dtriangle(x, 0,4,1) ~ x) , 0, 4)\n\n1.666667 with absolute error < 1.9e-14\n\n\n\n\n\n\n4.4.2 Uniform Distributions\nA uniform distribution is a described by a constant function over some interval. Its shape is a rectangle. This makes it particularly easy to calculate probabilities for a uniform distribution. Despite its simplicity, the family of uniform distributions has many applications.\nWe will let \\(X \\sim \\Unif(a,b)\\) denote that \\(X\\) is a uniform random variable on the interval from \\(a\\) to \\(b\\). In R, the parameters \\(a\\) and \\(b\\) are given more meaningful names: min and max. We can use the following code to graph the \\(\\Unif (1,4)\\) distribution.\n\ngf_dist(\"unif\", min = 1, max = 4, xlim = c(-1, 6)) \n\n\n\n\nNotice that the width of the non-zero portion of the pdf is 3, so the height must be \\(1/3\\).\nProbabilities involving uniform distributions are easily calculated using simple geometry, but R also provides several functions for working with uniform probability distributions.\n\n\nTable 4.2: Uniform distributions in R\n\n\n\n\n\n\nFunction\nWhat it does\n\n\n\n\ndunif(x, min, max)\nComputes value of the pdf at x.\n\n\npunif(q, min, max)\nComputes value of the cdf at x, i.e., \\(\\Prob(X \\le \\texttt{q})\\).\n\n\nqunif(p, min, max)\nComputes quantiles, that is a value \\(q\\) so that \\(\\Prob(X \\le \\texttt{q}) = \\texttt{p}\\).\n\n\nrunif(n, min, max)\nRandomly samples n values from the \\(\\Unif(\\texttt{min},\\texttt{max})\\) distribution.\n\n\n\n\nNotice the pattern to these names. They start with the same letters as the functions for the triangle distributions, but replace triangle with unif. There are similar functions for all of the distributions in this chapter.\n\nExample 4.14 Q. Let \\(X \\sim \\Unif(1,4)\\). Use R to calculate the following values and check the values using geometry:\n\n\n\\(\\Prob(X \\le 2)\\)\nthe 80th percentile of the distribution\n\n\n\nA.\n\npunif(2,1,4)   # P(X <= 2 )\n\n[1] 0.3333333\n\n(2-1) * 1/3    # P(X <= 2 ) using area\n\n[1] 0.3333333\n\nqunif(.8, 1,4) # 80th percentile\n\n[1] 3.4\n\n\nWe could also get the 80th percentile by solving the equation \\(\\frac{x-1}{3} = 0.8\\). From this we get \\(\\frac{x}{3} = 0.8 + 1/3\\), so \\(x = 3 ( 0.8 + 1/3) = 2.4 + 1 = 3.4\\).\n\n\n\n\n4.4.3 Exponential Distributions\nThe exponential distributions are useful for modeling the time until some “event” occurs. The model is based on the assumptions that\n\n\nThe probability of an event occurring in any small interval of time is proportional to the length of the time interval. The constant of proportionality is the rate parameter, usually denoted by \\(\\lambda\\).\nThe probabilities of events occurring in two small non-overlapping intervals are independent.\n\n\n\n\nExample 4.15 Here are some situations that might be well modeled by an exponential distribution:\n\n\nThe time until the next radioactive decay event is detected on a Geiger counter\nThe time until a space satellite is struck by a meteor (or some other space junk) and disabled.\nThe model would be good if (over some time span of interest) the chances of getting struck are always the same. It would not be such a good model if the satellite moves through time periods of relatively higher and then relatively lower chances of being struck (perhaps because we pass through regions of more or less space debris at different times of the year.)\nThe lifetime of some manufactured device.\nThis is a pretty simple model (we’ll learn better ones later) and most often is too simple to describe the interesting features of the lifetime of a device. In this model, failure is due to some external thing “happening to” the device; the device itself does not wear (or improve) over time.\n\n\n\n\n\nWe will let \\(X \\sim \\Exp(\\lambda)\\) denote that \\(X\\) has an exponential distribution with rate parameter \\(\\lambda\\). The kernel of such a distribution is \\[\nk(x; \\lambda) = e^{-\\lambda x} \\; \\boolval{x \\ge 0}\n\\] Notice that the function describing this distribution is defined only for x-values that are real numbers greater than or equal to zero (in mathematical notation, the interval \\([0, \\infty)\\).) This interval is sometimes called the ``support” of the distribution. When using probability distributions to model data, it’s important to think about whether the support of the distribution matches well with the range of possible values observed in the data.\nThe exponential distribution function is a pretty easy function to integrate, but R provides the now familiar functions to make things even easier.\n\n\nTable 4.3: Exponential distributions in R\n\n\n\n\n\n\nFunction\nWhat it does\n\n\n\n\ndexp(x, rate)\nComputes value of the pdf at x.\n\n\npexp(q, rate)\nComputes value of the cdf at q, i.e., \\(\\Prob(X \\le \\texttt{q})\\).\n\n\nqexp(p, rate)\nComputes quantiles, that is a value \\(q\\) so that \\(\\Prob(X \\le \\texttt{q}) = \\texttt{p}\\).\n\n\nrexp(n, rate)\nRandomly samples n values from the \\(\\Exp(\\lambda)\\) distribution where \\(\\lambda\\) = rate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngf_dist(\"exp\", rate = 4)\n\n\n\n\nFigure 4.4: An exponential distribution with rate = 4.\n\n\n\n\n\n\n4.4.4 Gamma and Weibull Distributions\nThe Gamma and Weibull families of distributions are generalizations of the exponential distribution. Each family has two parameters, a rate parameter as in the exponential distribution, and an additional parameter called the shape parameter (denoted by \\(\\alpha\\) below). The reciprocal of the rate parameter is called the scale parameter. For the Gamma distribution, R lets us use either rate or scale (and the default is rate). For the Weibull, we must use the scale.\n\nKernels for Gamma and Weibull distributions.\n\n\n\n\n\n\ndistribution\nkernel\n\n\n\n\n\n\n\\(\\Gamm(\\alpha, \\lambda)\\)\n\\(k(x) = x^{\\alpha} e^{-\\lambda x}\\) for \\(x \\ge 0\\)\n\n\n\\(\\Weibull(\\alpha, \\lambda)\\)\n\\(k(x) = x^{\\alpha} e^{-\\lambda x^{\\alpha}}\\) for \\(x > 0\\)\n\n\n\nBoth families of distributions are supported on the interval \\([0, \\infty\\).) For the most part, we won’t use these formulas in calculations, preferring to let R do the work for us. However, notice that each of these distributions has a pdf that allows for relatively simple integration. For the Gamma distributions, we need to use integration by parts (\\(\\alpha- 1\\) times). For the Weibull distributions we can use a substitution: \\(u = x^{\\alpha}\\). In each case, when \\(\\alpha= 1\\) we get an exponential distribution.\nThe now familiar functions are available for each of these distributions.\n\nGamma distributions in R.\n\n\n\n\n\n\nFunction\nWhat it does\n\n\n\n\ndgamma(x, shape, rate = 1, scale = 1/rate)\nComputes value of the pdf at x.\n\n\npgamma(q, shape, rate = 1, scale = 1/rate)\nComputes value of the cdf at q, i.e., \\(\\Prob(X \\le \\texttt{q})\\).\n\n\nqgamma(p, shape, rate = 1, scale = 1/rate)\nComputes quantiles, that is a value \\(q\\) so that \\(\\Prob(X \\le \\texttt{q}) = \\texttt{p}\\).\n\n\nrgamma(n, shape, rate, scale = 1/rate)\nRandomly samples n values from the Gamma distribution.\n\n\n\n\nWeibull distributions in R.\n\n\n\n\n\n\nFunction\nWhat it does\n\n\n\n\ndweibull(x, shape, scale = 1)\nComputes value of the pdf at x.\n\n\npweibull(q, shape, scale = 1)\nComputes value of the cdf at q, i.e., \\(\\Prob(X \\le \\texttt{q})\\).\n\n\nqweibull(p, shape, scale = 1)\nComputes quantiles, that is a value \\(q\\) so that \\(\\Prob(X \\le \\texttt{q}) = \\texttt{p}\\).\n\n\nrweibull(n, shape, scale = 1)\nRandomly samples n values from the Weibull distribution.\n\n\n\n\n\n\n\n\nLike the exponential distributions, these distributions are skewed and only take on positive values.\nThese distributions arise in many applications, including as more general models for lifetime. As the pictures below indicate, the shape and scale parameters are aptly named.\n\ngf_dist(\"gamma\", params = list(shape = 2, rate = 1), title = \"Gamma(2,1)\")\ngf_dist(\"gamma\", params = list(shape = 5, rate = 1), title = \"Gamma(5,1)\")\ngf_dist(\"gamma\", params = list(shape = 2, scale = 10), title = \"Gamma(2,10)\")\ngf_dist(\"gamma\", params = list(shape = 5, scale = 10), title = \"Gamma(5,10)\")\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\nFigure 4.5: Some example gamma distributions.\n\n\n\n\ngf_dist(\"weibull\", params = list(shape = 2, scale = 1),title = \"Weibull(2,1)\")\ngf_dist(\"weibull\", params = list(shape = 5, scale = 1),title = \"Weibull(5,1)\")\ngf_dist(\"weibull\", params = list(shape = 2, scale = 10),title = \"Weibull(2,10)\")\ngf_dist(\"weibull\", params = list(shape = 5, scale = 10),title = \"Weibull(5,10)\")\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\nFigure 4.6: Some example Weibull distributions.\n\n\n\n\n\n4.4.5 Normal Distributions\nWe come now to the most famous family of distributions – the normal distributions (also called Gaussian distributions). These symmetric distributions have the famous “bell shape” and are described by two parameters, the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). The pdf for a \\(\\Norm(\\mu, \\sigma)\\) distribution is\n\\[\nf(x) = \\frac{ 1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\\tag{4.4}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe inflection points of the normal distributions are always at \\(\\mu -\\sigma\\) and \\(\\mu+\\sigma\\).\nAmong the normal distributions is one special distribution – the standard normal distribution – which has mean 0 and standard deviation 1. All other normal distributions are simply linear transformations of the standard normal distribution. That is, If \\(Z \\sim \\Norm(0,1)\\) and \\(Y = a + b X\\) , then \\(Y \\sim \\Norm(a, b)\\). Conversely, if \\(Y \\sim \\Norm(\\mu,\\sigma)\\), then \\(Z = \\frac{Y - \\mu}{\\sigma} \\sim \\Norm(0,1)\\).\nAs with the other distributions we have encountered, we have four functions that allow us to work with normal distributions in R\n\n\nTable 4.4: Normal distributions in R.\n\n\n\n\n\n\nFunction\nWhat it does\n\n\n\n\ndnorm(x, mean = 0, sd = 1)\nComputes value of the pdf at x.\n\n\npnorm(q, mean = 0, sd = 1)\nComputes value of the cdf at x, i.e., \\(\\Prob(X \\le \\texttt{q})\\).\n\n\nqnorm(p, mean = 0, sd = 1)\nComputes quantiles, that is a value \\(q\\) so that \\(\\Prob(X \\le \\texttt{q}) = \\texttt{p}\\).\n\n\nrnorm(n, mean = 0, sd = 1)\nRandomly samples n values from the \\(\\Norm(\\mu, \\sigma)\\) distribution where \\(\\mu\\) = mean and \\(\\sigma\\) = sd.\n\n\n\n\n\n4.4.5.1 The 68-95-99.7 Rule\nAlso known as the “Empirical Rule”, the 68-95-99.7 Rule provides a set of probability benchmarks for the normal distributions because for any normal distribution:\n\n\\(\\approx 68\\)% of the normal distribution is between \\(\\mu - \\sigma\\) and \\(\\mu + \\sigma\\).\n\\(\\approx 95\\)% of the normal distribution is between \\(\\mu - 2\\sigma\\) and \\(\\mu + 2\\sigma\\).\n\\(\\approx 99.7\\)% of the normal distribution is between \\(\\mu - 3\\sigma\\) and \\(\\mu + 3\\sigma\\).\n\n\nExample 4.16 Q. Before they were rescaled, SAT scores used to be approximately normally distributed with a mean of 500 and a standard deviation of 100.\n\nApproximately what percent of test takers scored between 400 and 600?\nApproximately what percent of test takers scored above 600?\nApproximately what percent of test takers scored below 300?\nApproximately what percent of test takers scored between 400 and 700?\n\nA.\n\n68%\nSince 68% are between 400 and 600, the other 32% must be outside that range, half above and half below. So 16% are above 600.\nSince 95% are between 300 and 700, the other 5% must be outside that range, half above and half below. So 2.5% are below 300.\n16% are below 400 and 2.5% are above 700, so the remaining 81.5% must be between 400 and 700.\n\nOf course, we can get more accurate results using R:\n\npnorm( 600, 500, 100) - pnorm(400, 500, 100)\n\n[1] 0.6826895\n\npnorm( 700, 500, 100) - pnorm(300, 500, 100)\n\n[1] 0.9544997\n\npnorm( 300, 500, 100) \n\n[1] 0.02275013\n\npnorm( 700, 500, 100) - pnorm(400, 500, 100)\n\n[1] 0.8185946\n\n\nThe xpnorm() function will additionally draw pictures of the normal distribution with a portion of the distribution shaded in.\n\nxpnorm(700,500,100) - xpnorm(400, 500, 100)\n\n\n\n\nIf X ~ N(500, 100), then \n\n\n    P(X <= 700) = P(Z <= 2) = 0.9772\n\n\n    P(X >  700) = P(Z >  2) = 0.02275\n\n\n\n\n\nIf X ~ N(500, 100), then \n\n\n    P(X <= 400) = P(Z <= -1) = 0.1587\n\n\n    P(X >  400) = P(Z >  -1) = 0.8413\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.8185946\n\n\n\n\n\nExample 4.17 We can use qnorm() to compute percentiles. For example, let’s calculate the 75th percentile for SAT distributions.\n\nqnorm(.75, 500, 100)\n\n[1] 567.449\n\n\n\n\n\n\n\n4.4.6 Beta Distributions\nThe Beta distributions have support on the interval \\((0,1)\\), so they can provide a model for proportions or other quantities that are bounded between 0 and 1.5 The Beta distributions have two parameters, imaginatively called shape1 and shape2. The kernel of the Beta distributions is a product of a power of \\(x\\) and a power of \\((1-x)\\): \\[\nk(x; \\alpha, \\beta) = x^{\\alpha-1} (1-x)^{\\beta -1} \\; \\boolval{ x \\in [0,1] }\n\\] When \\(\\alpha = \\beta\\), the distribution is symmetric, and when \\(\\alpha = \\beta =1\\), we have the \\(\\Unif(0,1)\\)-distribution.\nThe two shape parameters provide a wide variety of shapes.\n\ngf_dist(\"beta\", params = list(shape1 = 2, shape2 = 2), title = \"Beta(2,2)\")\ngf_dist(\"beta\", params = list(shape1 = 2, shape2 = 0.9), title = \"Beta(2,0.9)\")\ngf_dist(\"beta\", params = list(shape1 = 4, shape2 = 2), title = \"Beta(4,2)\")\ngf_dist(\"beta\", params = list(shape1 = 0.9, shape2 = 0.85), title = \"Beta(0.9,0.85)\")\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\nFigure 4.7: Some example gamma distributions.\n\n\n\n\nBeta distributions in R\n\n\n\n\n\n\nFunction\nWhat it does\n\n\n\n\ndbeta(x, shape1, shape2)\nComputes value of the pdf at x.\n\n\npbeta(q, shape1, shape2)\nComputes value of the cdf at x, i.e., \\(\\Prob(X \\le \\texttt{q})\\).\n\n\nqbeta(p, shape1, shape2)\nComputes quantiles, that is a value \\(q\\) so that \\(\\Prob(X \\le \\texttt{q}) = \\texttt{p}\\).\n\n\nrbeta(n, shape1, shape2)\nRandomly samples n values from the \\(\\Beta(\\alpha, \\beta)\\) distribution where \\(\\alpha\\) = shape1 and \\(\\beta\\) = shape2.\n\n\n\n\n\n4.4.7 Binomial Distributions\nA binomial distribution is a discrete distribution with two parameters (\\(n\\) and \\(p\\), or as R calls them size and prob describing a situation in which\n\nOur random process consists of \\(n\\) identical sub-processes (called trials).\nEach trial has one of two outcomes (traditionally called success and failure).\nThe probability of success is \\(p\\) for each trial.\nThe outcome of each trial is independent of the others.\n\nThe binomial random variable counts the number of successes.\n\nExample 4.18  \n\n\nIf we flip a coin 100 times and let \\(X\\) be the number of heads, then \\(X \\sim \\Binom( 100, 0.5)\\).\nAmy is a 92% free throw shooter. If she attempts 50 free throws and we let –> \\(Y\\) be the number that she makes, then \\(Y \\sim \\Binom(50, 0.92)\\) (assuming that each shot is independent of the others.6)\n\n\n\nHere are some example binomial distributions. The distributions are symmetric when \\(p = 0.5\\). For a fixed size \\(n\\), the distributions become more and more skewed as \\(p\\) gets closer to 0 or 1. For a fixed probability \\(p\\), the distributions become more and more symmetric as \\(n\\) gets larger.\n\ngf_dist(\"binom\", size = 10, prob = 0.5, title = \"Binom(10, 0.5)\")\ngf_dist(\"binom\", size = 10, prob = 0.05, title = \"Binom(10, 0.05)\")\ngf_dist(\"binom\", size = 100, prob = 0.5, title = \"Binom(100, 0.5)\")\ngf_dist(\"binom\", size = 100, prob = 0.05, title = \"Binom(100, 0.05)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome example binomial distributions.\n\n\n\n\nThe pmf for a binomial distribution is given by \\[\nf(x) = \\binom{n}{x} p^x (1-p)^{n-x}\n\\] where \\(\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\\) is the binomial coefficient. As with the continuous distributions, we have our usual functions available.\n\n\n\n\n\n\n\nFunction\nWhat it does\n\n\n\n\ndbinom(x, size, prob)\nComputes value of the pmf at x.\n\n\npbinom(q, size, prob)\nComputes value of the cdf at q, i.e., \\(\\Prob(X \\le \\texttt{q})\\).\n\n\nqbinom(p, size, prob)\nComputes quantiles, that is a value \\(q\\) so that \\(\\Prob(X \\le \\texttt{q}) = \\texttt{p}\\).\n\n\nrbinom(n, size, prob)\nRandomly samples n values from the \\(\\Binom(n, \\pi)\\) distribution where \\(n\\) = size and \\(\\pi\\) = prob.\n\n\n\n\n\n\n\n4.4.8 Poisson Distributions\nThe Poisson distributions are generally used as models for counting “events” that happen in a specified amount of time or space. If the probability of an event happening at any moment is the same and independent of events happening at other moments, then the count of events in a fixed amount of time will be a Poisson random variable. The Poisson family has one parameter – often denoted \\(\\lambda\\) and called the rate parameter – which is the average number of events that happen over the fixed amount of time or space we are observing.\n\nExample 4.19  \n\n\nLet \\(X\\) be the number of clicks of a Geiger counter in a 1 second interval. Since each click corresponds to a radioactive decay event which we generally assume occur “at random” but according to some average rate, a Poisson random variable would be a good model for this. The rate parameter would be the average number of decay events per second.\nIf you stand on along a busy highway and count the number of red cars that go by in 30 minutes, a Poisson random variable might be a good model. Sometimes you will get bunches of red cars or periods of time with few or no red cars, that is just as a Poisson model predicts.\n\n\n\n\n\nThe Poisson distributions are skewed right, but become less and less skewed as the rate parameter increases. Here are a few example plots.\n\ngf_dist(\"pois\", lambda = 1, title = \"Pois(1)\")\ngf_dist(\"pois\", lambda = 5, title = \"Pois(5)\")\ngf_dist(\"pois\", lambda = 15, title = \"Pois(15)\")\ngf_dist(\"pois\", lambda = 50, title = \"Pois(50)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome example Poisson distributions.\n\n\n\n\nThe pmf for a \\(\\Pois(\\lambda)\\) random variable is \\[\nf(x) = \\frac{ e^{-\\lambda} \\lambda^x}{x!}\n\\] and the functions in R for working with Poisson distributions are the following:\n\nPoisson distributions in R.\n\n\n\n\n\n\nFunction\nWhat it does\n\n\n\n\ndpois(x, lambda)\nComputes value of the pmf at x.\n\n\nppois(q, lambda)\nComputes value of the cdf at q, i.e., \\(\\Prob(X \\le \\texttt{q})\\).\n\n\nqpois(p, lambda)\nComputes quantiles, that is a value \\(q\\) so that \\(\\Prob(X \\le \\texttt{q}) = \\texttt{p}\\).\n\n\nrpois(n, lambda)\nRandomly samples n values from the \\(\\Pois(\\lambda)\\) distribution where \\(\\lambda\\) = lambda."
  },
  {
    "objectID": "04-random-variables.html#fitting-distributions-to-data",
    "href": "04-random-variables.html#fitting-distributions-to-data",
    "title": "4  Random Variables",
    "section": "4.5 Fitting Distributions to Data",
    "text": "4.5 Fitting Distributions to Data\nSuppose we think a family of distributions would make a good model for some situation. How do we decide which member of the family to use? The simple answer is that we should choose the one that fits “best.” The trick is deciding what it means to fit well. In fact there is more than one way to measure how well a distribution fits a data set.\n\nExample 4.20 We can use the following code to load a data set that contains three year’s worth of mean hourly wind speeds (mph) in Twin Falls, ID. This kind of data is often used to estimate how much power could be generated from a windmill placed in a given location.\n\nWind <- \n  read.csv(\"https://rpruim.github.io/Engineering-Statistics/data/stob/TwinfallsWind.csv\")\nhead(Wind, 2)\n\n\n\n  \n\n\ntail(Wind, 2)\n\n\n\n  \n\n\ngf_histogram( ~ speed, data = Wind, binwidth = 1 )\n\n\n\n\nAs we can see, the distribution is skewed, but it doesn’t look like an exponential distribution would be a good fit. Of the distributions we have seen, it seems like a Weibull or Gamma distribution would be a potentially good choice. A Weibull model has often been used as a model for mean hourly wind speed, and the shape of our histogram indicates that this is a reasonable family of distributions.\nQ. Which Weibull distribution is the best model for our data?\nA. The fitdistr() in the MASS package uses the method of maximum likelihood to fit univariate (one variable) distributions.\n\nfitdistr( Wind$speed, \"weibull\" )\n\nError in fitdistr(Wind$speed, \"weibull\"): Weibull values must be > 0\n\n\nFor fitdistr() to fit a Weibull distribution, all of the data must be positive, but our data includes some 0’s.\n\ntally( ~ (speed == 0), data = Wind)\n\n(speed == 0)\n TRUE FALSE \n   48 26225 \n\n\nLet’s see how small the smallest non-zero measurements are.\n\nmin( ~ speed, data = Wind |> filter(speed > 0))\n\n[1] 0.01\n\n\nThis may well be a simple rounding issue, since the wind speeds are recorded to the nearest 0.01 and 0.01 is the smallest positive value. Let’s create a new variable that moves each value of 0 to 0.0025 and try again. Why 0.0025? If we think that 0.01 represents anything in the range 0.005 to 0.015, which would round to 0.01, then 0 represents anything in the range 0 to 0.005.\n0.0025 is the middle of that range.\n\nWind <- Wind |> mutate(speed2 = ifelse( speed > 0, speed, 0.0025))\nfitdistr( Wind$speed2, \"weibull\" )\n\n      shape         scale   \n  1.694422851   6.650586935 \n (0.007957624) (0.025551827)\n\n\nThis says that the best fitting (in the sense of maximum likelihood) Weibull distribution is the \\(\\Weibull(1.69, 6.65)\\)-distribution.\nThe gf_histogram() function has an option to overlay the distribution fit by fitdistr() so we can see how good the fit is graphically.\n\ngf_dhistogram( ~ speed2, data = Wind) |>\n  gf_fitdistr( ~ speed2, data = Wind, dist = \"weibull\")\n\n\n\n\nThis can be abbreviated a bit:\n\ngf_dhistogram( ~ speed2, data = Wind) |>\n  gf_fitdistr(dist = \"weibull\")\n\n\n\n\ngf_fitdistr() is inheriting the formula and data from gf_dhistogram().\n\n\n\nExample 4.21 As an alternative, we could fit a Gamma distribution to the wind speed data.\n\nfitdistr(Wind$speed2, \"gamma\")\n\n      shape         rate    \n  2.495582854   0.421178362 \n (0.020485581) (0.003828652)\n\ngf_dhistogram( ~ speed2, data = Wind) |>\n  gf_fitdistr(dist = \"gamma\" , color = ~ \"Gamma\") |>\n  gf_fitdistr(dist = \"weibull\" , color = ~ \"Weibull\")\n\n\n\n\nBy eye, it appears that the Gamma distribution fits this data set slightly better, but there may be other reasons to prefer the Weibull distribution. In fact, there has been a good deal of research done regarding which distributions to use for wind speed data fitting. The answer to the question of which distributions should be used seems to be that it depends on the purpose for your modeling:\n``The fact that different distributions excel under different applications motivates further research on model selection based upon the engineering parameter of interest.” Morgan et al. (2011)\n\n\n\nExample 4.22 1986–87 was a good season for Michael Jordan, a famous former NBA basketball player. Possible models for the points scored each game that season are normal, Weibull, and Gamma distributions. The normal distributions might be a good choice if we think that the distributions is roughly symmetric (very good games are about the same amount above average as the very poor games are below average). Weibull and Gamma distributions have the built in feature that scores cannot be negative and would allow for a skewed distribution. The fitdistr() function in the MASS package can fit each of these.\n\nlibrary(fastR2)     # the Jordan8687 data set is in this package\nfitdistr(Jordan8687$points, \"normal\")\n\n      mean          sd    \n  37.0853659    9.8639541 \n ( 1.0892915) ( 0.7702454)\n\nfitdistr(Jordan8687$points, \"weibull\")\n\n     shape        scale   \n   4.1227692   40.7746012 \n ( 0.3454908) ( 1.1516943)\n\nfitdistr(Jordan8687$points, \"gamma\")\n\n      shape         rate    \n  12.42843002    0.33513033 \n ( 1.91535288) ( 0.05270279)\n\n\nWe can use a histogram with overlaid density curve to see how well these fits compare to the data.\n\ngf_dhistogram(~ points, data = Jordan8687, binwidth = 5) |>\n  gf_fitdistr(dist = \"norm\", color = ~\"normal\") |>\n  gf_fitdistr(dist = \"weibull\", color = ~ \"Weibull\") |>\n  gf_fitdistr(dist = \"gamma\", color = ~ \"Gamma\")\n\n\n\n\nThe three fits are similar, but not identical.\n\n\n\n4.5.1 Maximum Likelihood\nThe fitdistr() function uses the maximum likelihood method to estimate distribution parameters. The maximum likelihood method is one of the most commonly used estimation methods in all of statistics because (1) it can be used in a wide range of applications, and (2) the resulting estimators have some some desirable properties. Maximum likelihood estimation tries to choose the parameter values that maximize the likelihood of the observed data.\nFirst, let’s think about the “likelihood” of an individual observed data-point. The likelihood of the data-point is just the probability density function (or probability mass function) for the distribution of interest, evaluated at the value observed in the data. The likelihood gives some indication of how frequently we’d expect to observe this value, but it is not a probability (for one thing, likelihoods can exceed 1). The figure below illustrates that the likelihood of observing a person 80 inches (6 feet, 8 inches) tall, if the person comes from a population whose heights are Normally distributed with a mean of 68 inches and a standard deviation of 6 inches is about 0.009:\n\n\n\n\n\nGiven a set of specific parameter values, the likelihood of an entire observed data-set can be calculated by obtaining the value of the likelihood of each observed data-point, and summing these over all the observed data points. Then, we can find the maximum likelihood parameter estimates by trying many candidate parameter values until satisfied that we have found the ones that maximize the likelihood. (The numerical methods used are usually a bit more sophisticated than ``guessing lots of random candidate values”, but we won’t get into the details here. In some cases, it is also possible to write down a mathematical expression for the likelihood of the data given the parameters, and maximize it analytically.)\nWe’ll illustrate the main ideas of maximum likelihood with a simple example.\n\nExample 4.23 Michael has three dice in his pocket. One is a standard die with six sides, another has four sides, and the third has ten sides. He challenges you to a game. Without showing you which die he is using, Michael is going to roll a die 10 times and report to you how many times the resulting number is a \\(1\\) or a \\(2\\). Your challenge is to guess which die he is using.\nQ. Michael reports that \\(3\\) of the \\(10\\) rolls resulted in a \\(1\\) or a \\(2\\). Which die do you think he was using?\nA. The probability of obtaining a \\(1\\) or a \\(2\\) is one of \\(\\frac{1}{2}\\), \\(\\frac13\\), or \\(\\frac15\\), depending on which die is being used. Our data are possible with any of the three dice, but let’s see how likely they are in each case.\n\n\nIf \\(\\evProb{roll 1 or 2} = \\frac15\\), then the probability of obtaining exactly Micheal’s data is \\[\n\\left(\\frac15\\right)^3 \\left(\\frac45\\right)^7  =  0.0016777\n\\;.\n\\]\n\n(Whatever the order, there will be 3 events with probability \\(1/5\\) and 7 with probability \\(4/5\\). Since the events are independent, we can multiply all of these probabilities.)\n\nIf \\(\\evProb{roll 1 or 2} = \\frac13\\), then the probability of obtaining exactly Micheal’s data is \\[\n\\left(\\frac13\\right)^3 \\left(\\frac23\\right)^7\n=  0.0021677\n\\;.\n\\]\nIf \\(\\evProb{roll 1 or 2} = \\frac{1}{2}\\), then the probability of obtaining exactly Micheal’s data is \\[\n\\left(\\frac{1}{2}\\right)^3 \\left(\\frac{1}{2}\\right)^7 = 0.0016777\n\\;.\n\\]\n\n\n\nOf these, the largest likelihood is for the case that \\(\\evProb{roll 1 or 2} = \\frac13\\), i.e., for the standard, six-sided die. Our data would be more likely to occur with that die than with either of the other two – it is the maximum likelihood die.\n\n\nIn general, maximum likelihood calculations are harder because instead of having only 3 choices, there will be infinitely many choices, and instead of having only one parameter, there may be multiple parameters. So techniques from (multi-variable) calculus or numerical approximation methods are often used to maximize the likelihood function. The fitdistr() function uses pre-derived formulas for some distributions and numerical approximation methods for others. In some cases, you will get warning messages about attempts to apply a function to values that don’t make sense (trying to take logs or square roots of negative numbers, zero in the denominator, etc.) as the numerical approximation algorithm explores options in an attempt to find the best fit. The help documentation for fitdistr() explains which distributions it can handle and what method is used for each.\n\n\n4.5.2 The method of moments\nAn easy (but sometimes fairly crude) way to estimate the parameters of a distribution is the method of moments. You will often see this method used in engineering textbooks, especially if they do not rely on software that implements other methods (like the maximum likelihood method).\nThe basic idea is to set up a system of equations where we set the mean of the data equal to the mean of the distribution, the variance of the data equal to the variance of the distribution, etc.7\nTo employ this method, we need to know the means and variances of our favorite families of distributions (in terms of the parameters of the distributions). For all of the distributions we have seen, one can work out formulas for the means and variances in terms of the parameters involved. These are listed in Table 4.5\n\nExample 4.24 Let’s return to the wind speeds in Example 4.20. The formulas for the mean and variance of a Weibull distribution involve the gamma function \\(\\Gamma()\\), which might be unfamiliar to you. So let’s simplify things.\nTheoretical properties and observations of wind speeds at other locations suggest that using a shape parameter of \\(\\alpha = 2\\) is often a good choice (but shape does differ from location to location depending on how consistent or variable the wind speeds are). The Weibull distributions with \\(\\alpha = 2\\) have a special name, they are called the Rayleigh distributions.\nSo \\(\\Rayleigh(\\beta) = \\Weibull(\\alpha = 2, \\beta)\\). In this case, from Table 4.5, we see that to calculate the mean we need the value of \\(\\Gamma(1 + \\frac{1}{2}) = \\Gamma(1.5) = \\sqrt{\\pi}/2\\).\n\ngamma(1.5)\n\n[1] 0.8862269\n\nsqrt(pi)/2\n\n[1] 0.8862269\n\n\nFrom Table 4.5 we see that the mean of a \\(\\Rayleigh(\\beta)\\)-distribution is \\[\n\\E(X) = \\beta \\frac{\\sqrt{\\pi}}{2}\n\\]\nNow we can choose our estimate \\(\\hat \\beta\\) for \\(\\beta\\) so that \\[\n    \\hat \\beta \\frac{\\sqrt{\\pi}}{2} = \\overline x  ;.\n\\] That is, \\[\n    \\hat\\beta = \\frac{2 \\overline x }{\\sqrt{\\pi}}\n\\]\n\nx.bar <- mean(~speed, data = Wind) \nx.bar\n\n[1] 5.925238\n\nbeta.hat <- x.bar * 2 / sqrt(pi)\nbeta.hat \n\n[1] 6.685915\n\n\nSo our method of moments fit for the data is a \\(\\Rayleigh(6.69) = \\Weibull(2, 6.69)\\)\nAlthough the Rayleigh distributions are not as flexible as the Weibull or Gamma distributions, and although maximum likelihood is generally preferred over the method of moments, the method of moments fit of a Rayleigh distribution does have one advantage: it can be computed even if all you know is the mean of some sample data. Sometimes, that is all you can easily get your hands on (because the people who collected the raw data only report numerical summaries). You can find average wind speeds for many locations online, for example here: http://www.wrcc.dri.edu/htmlfiles/westwind.final.html\n\n\n\nExample 4.25 For distributions with two parameters, we solve a system of two equations with two unknowns. For the normal distributions this is particularly easy since the parameters are the mean and standard deviation, so we get\n\\[\n\\begin{aligned}\n\\hat\\mu &= \\mean x\\\\\n\\hat\\sigma^2 &= s_x^2\\\\\n\\end{aligned}\n\\]\n\nx.bar <- mean(~speed, data = Wind); x.bar\n\n[1] 5.925238\n\nv <- var(~speed, data = Wind); v\n\n[1] 13.34635\n\nsqrt(v)\n\n[1] 3.653265\n\n\nSo the method of moments suggests a \\(\\Norm(5.93, 3.65)\\) distribution. In this case, the method of moments and maximum likelihood methods give the same results.\n\nfitdistr(Wind$speed, \"normal\")\n\n      mean          sd    \n  5.92523770   3.65319577 \n (0.02253814) (0.01593687)\n\n\nBut this doesn’t mean that the fit is particularly good. Indeed, a normal distribution is not a good choice for this data. We know that wind speeds can’t be negative and we have other distributions (exponential, Weibull, and Gamma, for example) that are also never negative. So choosing one of those seems like a better idea. The following plot shows, as we expected, that the normal distribution is not a particularly good fit.\n\ngf_histogram(~speed, data = Wind, fit = \"normal\")\n\n\n\n\nIt is important to remember that the best fit using a poor choice for the family of distributions might not be a useful fit.\nThe choice of distributions is made based on a combination of theoretical considerations, experience from previous data sets, and the quality of the fit for the data set at hand.\n\n\n\n4.5.2.1 Important Families of Distributions\nStandard names for parameters that appear in several distributions in Table 4.5 include (\\(\\lambda\\)), (\\(\\alpha\\)), and (\\(\\beta\\)). In the normal distributions, \\(\\mu\\) and \\(\\sigma\\) are called mean and in R, and in the uniform distributions, \\(a\\) and \\(b\\) are called and . The function \\(\\Gamma(x)\\) that appears in the formulas for the Weibull and Beta distributions is a kind of continuous extrapolation from the factorial function. The function will calculate these values in R.”\n\n\n\n\nTable 4.5: Some common (families of) distributions.\n\n\n\n\n\n\n\n\n\ndistribution\nnotation\npdf or pmf\nmean\nvariance\n\n\n\n\nUniform\n\\(\\Unif(a, b)\\)\n\\(\\displaystyle \\frac{1}{b-a}\\) for \\(x \\in [a, b]\\)\n\\(\\displaystyle \\frac{b+a}{2}\\)\n\\(\\displaystyle \\frac{(b-a)^2}{12}\\)\n\n\nStandard normal\n\\(\\Norm(0,1)\\)\n\\(\\displaystyle \\frac{1}{\\sqrt{2\\pi}} {e^{-\\frac{1}{2} z^2}}\\)\n0\n1\n\n\nNormal\n\\(\\Norm(\\mu,\\sigma)\\)\n\\(\\displaystyle \\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot e^{-\\frac{1}{2} (\\frac{x-\\mu} {\\sigma})^2}\\)\n\\(\\mu\\)\n\\(\\sigma\\)\n\n\nExponential\n\\(\\Exp(\\lambda)\\)\n\\(\\lambda e^{-\\lambda x}\\) for \\(x > 0\\)\n\\(1/\\lambda\\)\n\\(1/\\lambda^2\\)\n\n\nGamma\n\\(\\Gamm(\\alpha, \\lambda = \\frac{1}{\\beta})\\)\n\\(\\displaystyle \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\lambda x}\\) for \\(x > 0\\)\n\\(\\alpha/\\lambda = \\alpha \\beta\\)\n\\(\\alpha/\\lambda^2 = \\alpha \\beta^2\\)\n\n\nWeibull\n\\(\\Weibull(\\alpha,\\beta = \\frac{1}{\\lambda})\\)\n\\(\\displaystyle \\frac{\\alpha}{\\beta^\\alpha} x^{\\alpha-1} e^{-(x/\\beta)^\\alpha}\\) for \\(x > 0\\)\n\\(\\beta \\Gamma(1 + \\frac1{\\alpha})\\)\n\\(\\beta^2 \\left[ \\Gamma(1 + \\frac{2}{\\alpha}) - \\left[ \\Gamma(1 + \\frac{1}{\\alpha}) \\right]^2 \\right]\\)\n\n\nBeta\n\\(\\Beta(\\alpha, \\beta)\\)\n\\(\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1}(1-x)^{\\beta-1}\\) for \\(x \\in (0,1)\\)\n\\(\\displaystyle \\frac{\\alpha}{\\alpha + \\beta}\\)\n\\(\\displaystyle \\frac{\\alpha \\beta }{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\)\n\n\nBinomial\n\\(\\Binom(n, p)\\)\n\\(\\displaystyle \\binom{n}{x} p^x (1-p)^{n-x}\\) for integer \\(x\\) with \\(0 \\le x \\le n\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\nPoisson\n\\(\\Pois(\\lambda)\\)\n\\(\\displaystyle \\frac{e^{-\\lambda} \\lambda^x} {x!}\\)\n\\(\\lambda\\)\n\\(\\lambda\\)"
  },
  {
    "objectID": "04-random-variables.html#quantile-quantile-plots",
    "href": "04-random-variables.html#quantile-quantile-plots",
    "title": "4  Random Variables",
    "section": "4.6 Quantile-Quantile Plots",
    "text": "4.6 Quantile-Quantile Plots\nTo this point we have looked at how well a distribution fits the data by overlaying a density curve on a histogram. While this is instructive, it is not the easiest way to make a graphical comparison between a data set and a theoretical distribution. Our eyes are much better at judging whether something is linear than they are at judging whether shapes have a particular kind of curve. Furthermore, certain optical misperceptions tend to cause people to exaggerate some kinds of differences and underestimate others.\nQuantile-quantile plots offer an alternative approach. As the name suggests, the idea is to compare the quantiles of our data to the quantiles of a theoretical distribution. These are then plotted as a scatter plot. Let’s go through those steps with a small data set so we can see all the moving parts, then we’ll learn how to automate the whole process using gf_qq().\n\n4.6.1 Normal-Quantile Plots\nThe normal distributions are especially important for statistics, so normal-quantile plots will be our most important example of quantile-quantiles plots. Also, special properties of the normal distributions make normal-quantile plots especially easy and useful. We will illustrate the construction of these plots using a data set containing Michael Jordan’s game by game scoring output from the 1986–87 basketball season.\n\nExample 4.26 Let’s begin by forming a randomly selected sample of 10 basketball games.\n\nset.seed(123)              # so you can get the same sample if you like.\nSmallJordan <- sample(Jordan8687, 10)\nSmallJordan\n\n\n\n  \n\n\n\n\nprobs <- seq(0.05, 0.95, by = 0.10)\nprobs\n\n [1] 0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95\n\nobserved <- sort(SmallJordan$points)                                    # sorted observations\ntheoretical <- qnorm( probs, mean = mean(observed), sd = sd(observed) ) # theoretical quantiles\n\nQQData <- data.frame(observed = observed, theoretical = theoretical)\nQQData\n\n\n\n  \n\n\n\nIf the observed data matched the theoretical quantiles perfectly, a scatter plot would place all the points on the line with slope 1 passing through the origin.\n\ngf_point( observed ~ theoretical, data = QQData, title = \"Hand made QQ-plot\" ) |>\n  gf_fun( x ~ x, alpha = 0.6, color = \"blue\", linetype = \"dashed\")\n\n\n\n\nEven better, we don’t need to know the mean and standard deviation in advance, because all normal distributions are linear transformations of the \\(\\Norm(0,1)\\)-distribution. So our standard practice will be to compare our data to the \\(\\Norm(0,1)\\)-distribution. If \\(X \\sim \\Norm(\\mu,\\sigma)\\), then \\(X = \\mu + \\sigma Z\\) where \\(Z \\sim \\Norm(0,1)\\), so a plot of \\(X\\) vs. \\(Z\\) will have slope \\(\\sigma\\) and intercept \\(\\mu\\).\n\ntheoretical2 <- qnorm( probs, mean = 0, sd = 1 ) # theoretical quantiles from Norm(0,1)\nQQData2 <- data.frame(observed = observed, theoretical = theoretical2)\ngf_point(observed ~ theoretical, data = QQData2, title = \"Hand made QQ-plot\", xlab = \"theoretical (z)\" ) |>\n  gf_abline(intercept = ~ mean(SmallJordan$points), slope = ~ sd(SmallJordan$points), \n            alpha = 0.5, color = \"navy\", data = NA) |>\n  gf_hline(yintercept = ~ mean(SmallJordan$points), alpha = 0.5) |>\n  gf_vline(xintercept = ~ 0, alpha = 0.5)\n\n\n\n\nThis whole process is automated by the gf_qq() function.\n\ngf_qq( ~ points, data = SmallJordan, title = \"Sub-sample\" )\n\n\n\ngf_qq( ~ points, data = Jordan8687, title = \"Full data set\" )\n\n\n\n\n\n\n\n\n4.6.2 Other distributions\nWorking with other distributions is similar, but most families of distributions don’t have a single “master example” to which we can make all comparisons, so we need to pick a particular member of the family (either by fitting or for some theoretical reason).8\n\nExample 4.27  \nLet's build a quantile-quantile plot for our wind speed data comparing \nto normal, gamma and Weibull distributions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can automate this, but we need to tell gf_qq() how to calculate the quantiles.\n\ngf_qq( ~ speed2, data = Wind)  # normal-quantile plot; normal is not a good model\n\n\n\n\nThe normal model does not fit well, but both Gamma and Weibull are reasonable models:\n\nfitdistr(Wind$speed2, \"gamma\")\n\n      shape         rate    \n  2.495582854   0.421178362 \n (0.020485581) (0.003828652)\n\nfitdistr(Wind$speed2, \"Weibull\")\n\n      shape         scale   \n  1.694422851   6.650586935 \n (0.007957624) (0.025551827)\n\nfittedqgamma <- makeFun( qgamma(p, shape = 2.496, rate = 0.421 ) ~ p )\nfittedqweibull <- makeFun( qweibull(p, shape = 1.694, scale = 6.651) ~ p ) \ngf_qq( ~speed2, data = Wind, distribution = fittedqgamma )\n\n\n\ngf_qq( ~speed2, data = Wind, distribution = fittedqweibull )"
  },
  {
    "objectID": "04-random-variables.html#exercises",
    "href": "04-random-variables.html#exercises",
    "title": "4  Random Variables",
    "section": "4.7 Exercises",
    "text": "4.7 Exercises\n\nExercise 4.1 Let \\(f(x) = 5/4 - x^3\\) on \\([0,1]\\).\n\nShow that \\(f\\) is a pdf.\nCalculate \\(\\Prob(X \\le\\frac{1}{2})\\).\nCalculate \\(\\Prob(X \\ge\\frac{1}{2})\\).\nCalculate \\(\\Prob(X = \\frac{1}{2})\\).\n\n\n\n\n\nSolution. \nf <- makeFun( (5/4 - x^3) * ( abs(x-.5) <= .5 ) ~ x )\nplotFun(f(x) ~ x, x.lim = c(-1,2))  # quick plot to make sure things look correct.\n\n\n\nF <- antiD(f(x) ~x)  \n# part a:  f(x) >=0, so we just need to check that the total area is 1\nF(1) - F(0) == 1  \n\n[1] TRUE\n\nF(1/2) - F(0)     # part b\n\n[1] 0.609375\n\nF(1) - F(1/2)     # part c\n\n[1] 0.390625\n\nF(1/2) - F(1/2)   # part d\n\n[1] 0\n\n\n\n\n\nExercise 4.2 Repeat parts (2) – (4) of Example 4.13 using geometry rather than R\n\n\n\n\nSolution. \n\n\\(\\Prob(X \\le 1) = \\frac 12 \\cdot 1 \\cdot \\frac{1}{2} = \\frac 14\\)\n\\(\\Prob(X \\le 2) = 1 - \\Prob(X \\ge 2) = 1 - \\frac{1}{2} \\cdot 2 \\cdot \\frac13 = 1 - \\frac 13 = \\frac23\\).\nThe median \\(m\\) is a number between 1 and 2 and satisfies \\(\\frac 12 = \\Prob(X \\ge m) = \\frac{1}{2} (4-m) \\frac{4-m}{6}\\).\nSolving for \\(m\\) we get \\(m = 4 - \\sqrt{6} = 1.5505103\\).\n\n\n\n\n\n\nExercise 4.3 Let \\[\n\\displaystyle k(x) = (1 - x^2) \\cdot\\boolval{ x \\in[-1,1] } =\n\\begin{cases}\n        1-x^2 & x \\in[-1,1] \\\\\n        0 & \\mbox{otherwise}\n\\end{cases}\n\\]\nbe the kernel of a continuous distribution.\n\n\n     Determine the pdf for this distribution.\n     Compute the mean and variance for this distribution\n\n\n\n\n\n\nSolution. \nThe integrals are easy enough to do by hand, but here is the R code \nto compute them.\n\n\nk <- makeFun( 1 - x^2 ~ x )\nK <- antiD( k(x) ~ x )\narea <- K(1) - K(-1); area \n\n[1] 1.333333\n\nf <- makeFun( (1 - x^2)/area ~ x)\nF <- antiD(f(x) ~ x)\nF(1) - F(-1)    # this should be 1 if we have done things right\n\n[1] 1\n\nG <- antiD( x * f(x) ~ x )\nH <- antiD( x^2 * f(x) ~ x )\nm <- G(1) - G(-1); m               # E(X)\n\n[1] 0\n\nH(1) - H(-1)                       # E(X^2)\n\n[1] 0.2\n\nH(1) - H(-1) - m^2                 # Var(X)\n\n[1] 0.2\n\n\n\n\n\nExercise 4.4 Let \\(Y \\sim\\Tri(0,10,4)\\). Compute \\(\\E(Y)\\) and the median of \\(Y\\).\n\n\n\n\nSolution. \n# mean:\nm <- antiD( x * dtriangle(x,0,10,4) ~ x, lower.bound = 0)(10)\nm\n\n[1] 4.666665\n\n# variance:\nantiD( x^2 * dtriangle(x,0,10,4) ~ x, lower.bound = 0)(10) - m^2\n\n[1] 4.222229\n\n# median\nqtriangle( 0.5, 0, 10, 4 )\n\n[1] 4.522774\n\n\n\n\n\nExercise 4.5 Let \\(W \\sim \\Unif(0,10)\\). Compute \\(\\E(W)\\) and \\(\\Var(W)\\).\n\n\n\n\nSolution. \n# mean:\nm <- antiD( x * dunif(x,0,10) ~ x, lower.bound = 0)(10)\nm\n\n[1] 5\n\n# variance:\nantiD( x^2 * dunif(x,0,10) ~ x, lower.bound = 0)(10) - m^2\n\n[1] 8.333333\n\n\n\n\n\nExercise 4.6  \n\n\nLet \\(X \\sim \\Exp(4)\\). Use R to compute \\(\\E(X)\\).\nLet \\(X \\sim \\Exp(10)\\). Use R to compute \\(\\E(X)\\).\nLet \\(X \\sim \\Exp(1/5)\\). Use R to compute \\(\\E(X)\\).\nWhat pattern do you notice. Explain in terms of the definition of the exponential distribution why this makes sense.\n\n\n\n\n\n\n\nSolution. \nantiD( x * dexp(x, 4) ~ x, lower.bound = 0)(Inf)\n\n[1] 0.25\n\nantiD( x * dexp(x, 10) ~ x, lower.bound = 0)(Inf)\n\n[1] 0.1\n\nantiD( x * dexp(x, 1/5) ~ x, lower.bound = 0)(Inf)\n\n[1] 5\n\n\nIt appears that the mean of an \\(\\Exp(\\lambda)\\)-distribution is \\(1/\\lambda\\). This makes sense. If events have at a rate of \\(30\\) per hour, we would expect to wait \\(1/30\\) of an hour (on average) for the first event to happen.\n\n\n\nExercise 4.7 Use R to plot the pdf and compute the mean and variance of each of the following distributions.\n\n\n\\(\\Beta(2,3)\\)\n\\(\\Beta(20,30)\\)\n\\(\\Gamm(\\texttt{shape} = 2, \\texttt{scale} = 3)\\)\n\\(\\Weibull(\\texttt{shape} = 2, \\texttt{scale} = 3)\\)\n\n\n\n\n\n\n\nSolution. \n# Beta(2,3)\nm <- antiD( x * dbeta(x,2,3) ~ x, lower.bound = 0)(1)\nm\n\n[1] 0.4\n\nantiD( x^2 * dbeta(x,2,3) ~ x, lower.bound = 0 )(1) - m^2\n\n[1] 0.04\n\n\n\n# Beta(20,30)\nm <- antiD( x * dbeta(x,20,30) ~ x, lower.bound = 0)(1)\nm\n\n[1] 0.4\n\nantiD( x^2 * dbeta(x,20,30) ~ x, lower.bound = 0 )(1) - m^2\n\n[1] 0.004705882\n\n\n\n# Gamma(2,scale = 3)\nm <- antiD( x * dgamma(x,2,scale = 3) ~ x, lower.bound = 0)(Inf)\nm\n\n[1] 6\n\nantiD( x^2 * dgamma(x,2,scale = 3) ~ x, lower.bound = 0 )(Inf) - m^2\n\n[1] 18\n\n\n\n# Weibull(2,scale = 3)\nm <- antiD( x * dweibull(x,2,scale = 3) ~ x, lower.bound = 0)(Inf)\nm\n\n[1] 2.658681\n\nantiD( x^2 * dweibull(x,2,scale = 3) ~ x, lower.bound = 0 )(Inf) - m^2\n\n[1] 1.931417\n\n\n\n\n\nExercise 4.8 For each of the following distributions, determine the proportion of the distribution that lies between 0.5 and 1.\n\n\n\\(\\Exp(\\texttt{rate} = 2)\\)\n\\(\\Beta(\\texttt{shape1} = 3, \\texttt{shape2} = 2)\\)\n\\(\\Norm(\\texttt{mean} = 1, \\texttt{sd} = 2)\\)\n\\(\\Weibull(\\texttt{shape} = 2, \\texttt{scale} = 1/2)\\)\n\\(\\Gamm(\\texttt{shape} = 2, \\texttt{scale} = 1/2)\\)\n\n\n\n\n\n\n\nSolution. \npexp(1, 2) - pexp(0.5, 2)\n\n[1] 0.2325442\n\npbeta(1, 3, 2) - pbeta(0.5, 3, 2)\n\n[1] 0.6875\n\npnorm(1, 1, 2) - pnorm(0.5, 1, 2)\n\n[1] 0.09870633\n\npweibull(1, 2, scale = 1/2) - pweibull(0.5, 2, scale = 1/2)\n\n[1] 0.3495638\n\npgamma(1, 2, scale = 1/2)   - pgamma(0.5, 2, scale = 1/2)\n\n[1] 0.329753\n\n\n\n\n\nExercise 4.9  \n\n\nUsing Table 4.5 and the method of moments, fit an exponential distribution to the Twin Falls wind speed data.\n\n\nWind <- \n  read.csv(\"https://rpruim.github.io/Engineering-Statistics/data/stob/TwinfallsWind.csv\")\n\nWhat is the estimated value of the rate parameter?\n\nNow use fitdistr() to fit an exponential distribution using maximum likelihood.\nHow do the two estimates for the rate parameter compare?\nHow well does an exponential distribution fit this data?\n\n\n\n\n\n\n\nSolution. \n\nThe method of moments fit for \\(\\lambda\\) comes from solving \\(\\frac{1}{\\lambda} = \\mean x\\) for \\(\\lambda\\), so \\(\\hat \\lambda = \\frac{1}{\\mean x}\\)\n\n\nlambda.hat <- 1/ mean(~speed, data = Wind)\nlambda.hat\n\n[1] 0.1687696\n\n\n\n\n\n\nfitdistr(Wind$speed, \"exponential\")\n\n      rate    \n  0.168769601 \n (0.001041213)\n\n\n\nThey are the same in this case.\nThis fit is not that great.\n\n\ngf_dhistogram(~speed, data = Wind, fit = \"exponential\")\n\n\n\ngf_qq( ~ speed, data = Wind, distribution = qexp)\n\n\n\n\n\n\n\n\n\nExercise 4.10  \nA Gamma distribution can also be fit using the method of moments.\nBecause there are two parameters (shape and rate or shape and scale),\nyou will need to solve a system of two equations with two unknowns.\n\n\nUsing Table 4.5 and the method of moments, fit a Gamma distribution to the Twin Falls wind speed data. What are the estimated values of the shape and rate parameters?\nHow do the method of moments estimates for the parameters compare to the maximum likelihood estimates from fitdistr()?\n\n\n\n\n\n\n\nSolution. \nWind <- Wind |> mutate(speed2 = ifelse( speed > 0, speed, 0.0025))\nm <- mean(~speed2, data = Wind); m\n\n[1] 5.925242\n\nv <- var(~speed2, data = Wind); v\n\n[1] 13.34629\n\nfitdistr(Wind$speed2, \"gamma\")\n\n      shape         rate    \n  2.495582854   0.421178362 \n (0.020485581) (0.003828652)\n\n\nNow we solve\n\\[\n\\begin{aligned}\n\\alpha \\beta & = \\mean x  = 5.9252423\n\\\\\n\\alpha \\beta^2 & = s^2 = 13.3462932\n\\end{aligned}\n\\]\nDividing the second by the first gives \\(\\hat \\beta = \\sfrac{s^2}{\\mean x}\\). From this we obtain \\(\\hat \\alpha = \\mean{x} / \\hat \\beta = \\sfrac{ \\mean x^2}{s^2}\\).\n\nbeta.hat <- v/m; beta.hat\n\n[1] 2.252447\n\nalpha.hat <- m / beta.hat ; alpha.hat\n\n[1] 2.63058\n\nlambda.hat <- 1/ beta.hat ; lambda.hat\n\n[1] 0.4439616\n\n\nThe fitted values are similar to but not identical to the maximum likelihood estimates.\n\n\n\nExercise 4.11 Sam has found some information about wind speed at a location he is interested in online. Unfortunately, the web site only provides the mean and standard deviation of wind speed.\n\n\n\n\n\n\n\n\nmean:\n10.2 mph\n\n\nstandard deviation:\n5.1 mph\n\n\n\n\n\n\n\nUse this information and the method of moments to estimate the shape and rate parameters of a Gamma distribution.\nIn principal, we could do the same for a Weibull distribution, but the formulas aren’t as easy to work with. Fit a Rayleigh distribution instead (i.e., a Weibull distribution with shape parameter equal to 2).\n\n\n\n\n\n\nSolution. We can recycle some work from the previous problem to quickly obtain the method of moments fit for the Gamma distribution:\n\nm <- 10.2\nv <- 5.1^2 \nbeta.hat <- v/m; beta.hat\n\n[1] 2.55\n\nalpha.hat <- m / beta.hat ; alpha.hat\n\n[1] 4\n\nlambda.hat <- 1/ beta.hat ; lambda.hat\n\n[1] 0.3921569\n\n\nFor the Rayleigh distribution we solve \\(\\hat \\beta \\frac{\\sqrt{\\pi}}{2} = \\mean x\\) for \\(\\hat \\beta\\) and get \\[\n    \\hat\\beta = \\frac{2 \\mean x }{\\sqrt{\\pi}}\n    = \\frac{2 \\cdot 10.2 }{\\sqrt{\\pi}}\n    = 11.5094675\n\\]\n\n\n\nExercise 4.12 In 1964, a study was undertaken to see if IQ at 3 years of age is associated with amount of crying at newborn age. In the study, 38 newborns were made to cry after being tapped on the foot, and the number of distinct cry vocalizations within 20 seconds was counted. The subjects were followed up at 3 years of age and their IQs were measured.  You can load this data using\n\nBaby <- read.csv(\"https://rpruim.github.io/Engineering-Statistics/data/BabyCryIQ.csv\")\nhead(Baby)\n\n\n\n  \n\n\n\nThe `cry.count` variable records the number of distinct cry vocalizations \nwithin 20 seconds.  Choose a family of distributions to fit to this data\nand do the fit using `fitdistr()`. Also include a plot showing \na histogram and your fitted density curve.\n\n\n\nSolution. \nThe distribution is skewed and non-negative, so a gamma or Weibull seems like \na good thing to try.  \n\nfitdistr(Baby$cry.count, \"gamma\")\n\n     shape         rate   \n  12.6338200    0.7329544 \n ( 2.8608824) ( 0.1693120)\n\nfitdistr(Baby$cry.count, \"Weibull\")\n\n     shape        scale   \n   3.5842275   19.1020465 \n ( 0.4245079) ( 0.9177923)\n\ngf_dhistogram( ~ cry.count, data = Baby, binwidth = 1) |>\n  gf_fitdistr( ~ cry.count, data = Baby, \"gamma\", title = \"Gamma\")\n\n\n\ngf_dhistogram(~cry.count, data = Baby, title = \"Weibull\") |>\n  gf_fitdistr(~ cry.count, data = Baby, \"weibull\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4.13 Create normal quantile plots for the ages of patients in the HELPrct data set separated by substance. (Getting separate or overlaid plots using gf_qq() works just like it does for other ggformula plots).\nComment on the plots.\n\n\n\n\n\nSolution. \ngf_qq( ~ age | substance, data = HELPrct)\n\n\n\n\nThe qq-plot for the alcohol group looks good. The other two (especially cocaine) show signs of skew – indicated by the curve to the qq plot.\n\ngf_dens( ~ age | substance, data = HELPrct)\n\n\n\n\n\n\n\nExercise 4.14 Match the normal-quantile plots to the histograms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution. \nY b) V c) Z d) W e) X f) U\n\n\n\n\nExercise 4.15 Show that \\(\\Var(X) = \\E(X^2) - \\E(X)^2\\) by showing that \\[\n    \\int_{-\\infty}^{\\infty} (x - \\mu_X) ^2 f(x) \\; dx\n    =\n    \\int_{-\\infty}^{\\infty} x^2 f(x) \\; dx  -  \\mu_X^2\n\\] whenever \\(f\\) is a pdf and all the integrals involved converge.\n\n\n\nSolution. Some algebra and properties of integrals are all we need:\n\\[\n\\begin{aligned}\n\\int_{-\\infty}^{\\infty} (x - \\mu_X) ^2 f(x) \\; dx\n    &=\n    \\int_{-\\infty}^{\\infty} (x^2 - 2\\mu_X x + \\mu_X^2) f(x) \\; dx\n    \\\\\n    &=\n    \\int_{-\\infty}^{\\infty} x^2 f(x) \\; dx\n    - 2 \\mu_X \\int_{-\\infty}^{\\infty} x f(x) \\; dx\n    + \\mu_X^2 \\int_{-\\infty}^{\\infty} f(x) \\; dx\n    \\\\\n    &=\n    \\Var(X) - 2 \\mu_X^2 + \\mu_X^2\n    \\\\\n    &=\n    \\Var(X) - \\E(X)^2\n\\end{aligned}\n\\]\n\n\n\nExercise 4.16 The heights of 18–22 year olds in the US follow approximately normal distributions within each sex. Estimated means and standard deviations appear in the table below.\n\n\n\n\n\n\n\n\n\n\nmean\nstandard deviation.\n\n\n\n\n\n\nwomen\n64.3 in\n2.6 in\n\n\n\n\nmen\n70.0 in\n2.8 in\n\n\n\n\n\n\nAnswer the following questions without using a computer or calculator (except for basic arithmetic).\n\n\nIf a woman is 68 inches tall, what is her z-score?\nIf a man is 74 inches tall, what is his z-score?\nWhat is more unusual, a woman who is at least 68 inches tall or a man who is at least 74 inches tall?\nBig Joe has decided to open a club for tall people. To join his club, you must be in the tallest 2.5% of people of your sex. How tall must a woman be to join Big Joe’s club?\nHow tall must a man be to join Big Joe’s club?\n\n\n\n\n\n\n\nSolution. \n\n\n\n\n(68 - 64.3)/2.6\n\n[1] 1.423077\n\n\n\n\n\n\n(74 - 70) / 2.8\n\n[1] 1.428571\n\n\n\nIt’s pretty close, but the z-score for the man is slightly larger, so it is slightly more unusual for a man to be that tall.\n\n\n\n64.3 + 2 * 2.6\n\n[1] 69.5\n\n\n\n\n\n\n70 + 2 * 2.8\n\n[1] 75.6\n\n\n\n\n\n\n\nExercise 4.17 Use the information from Exercise 4.16 to answer the following questions.\n\n\nWhat proportion of women are 5’10” or taller?\nWhat proportion of men are 6’4” or taller?\nIf a man is in the 75th percentile for height, how tall is he?\nIf a woman is in the 30th percentile for height, how tall is she?\n\n\n\n\n\n\n\nSolution. \n\n\n\n\n1 - pnorm(70, mean = 64.3, sd = 2.6)\n\n[1] 0.01417865\n\n\n\n\n\n\n1 - pnorm(76, mean = 70, sd = 2.8)\n\n[1] 0.01606229\n\n\n\n\n\n\nqnorm(.75, mean = 70, sd = 2.8)\n\n[1] 71.88857\n\n\n\n\n\n\nqnorm(.30, mean = 64.3, sd = 2.6)\n\n[1] 62.93656\n\n\n\n\n\n\n\n\n\n\nMorgan, Eugene C., Matthew Lackner, Richard M. Vogel, and Laurie G. Baise. 2011. “Probability Distributions for Offshore Wind Speeds.” Energy Conversion and Management 52 (1): 15–26. https://doi.org/10.1016/j.enconman.2010.06.015."
  },
  {
    "objectID": "05-transformations.html#simulations",
    "href": "05-transformations.html#simulations",
    "title": "5  Transformation and Combinations of Random Variables",
    "section": "5.1 Simulations",
    "text": "5.1 Simulations\nIn this section we will make use of the fact that each of our familiar distributions has a function in R that lets us simulate randomly sampling data from that distribution.\n\nX <- runif(10000, 0,1)\nY <- runif(10000, 0,1)\ngf_dhistogram( ~ X , main = \"Sample from Unif(0,1)\", \n               binwidth = .05, center = 0.025)\ngf_dhistogram( ~ Y , main = \"Another sample from Unif(0,1)\", \n               binwidth = .05, center = 0.025)\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 5.1: Two samples from a uniform distribution.\n\n\n\n\nS <- X + Y\nP <- X * Y\ngf_dhistogram( ~ S , main = \"Sum of two iid Unif(0,1) rvs\", \n               binwidth = 0.1, center = 0.05)\ngf_dhistogram( ~ P , main = \"Product of two iid Unif(0,1) rvs\", \n               binwidth = 0.05, center = 0.025)\n\n\n\n\n\n\nFigure 5.2: Sum of two uniform random variables.\n\n\n\n\n\n\n\nFigure 5.3: Product of two uniform random variables.\n\n\n\n\n\n\n\nIndependence Matters\nIt is important that we have created x and y independently. Independent random variables that have the same distribution are called independent identically distributed (iid) random variables. As an illustration of an extreme situation where the variables in our sum are not independent, let’s use the values of x in both roles:\n\n\nS2 <- X + X\nP2 <- X * X \ngf_dhistogram( ~ S2 , main = \"Sum of two non-iid Unif(0,1) rvs\", binwidth = 0.1, center = 0.05)\ngf_dhistogram( ~ P2 , main = \"Product of two non-iid Unif(0,1) rvs\", binwidth = 0.05, center = 0.025)\n\n\n\n\n(a) Sum\n\n\n\n\n\n\n\n(b) Product\n\n\n\nFigure 5.4: Adding and Multiplying two uniform random variables that are not independent.\n\n\nNotice how different the resulting distributions are, especially for the sum.\nSimilar procedures can be used to give an approximate distribution for any combination of random variables that we can simulate."
  },
  {
    "objectID": "05-transformations.html#propagation-of-mean-and-variance",
    "href": "05-transformations.html#propagation-of-mean-and-variance",
    "title": "5  Transformation and Combinations of Random Variables",
    "section": "5.2 Propagation of Mean and Variance",
    "text": "5.2 Propagation of Mean and Variance\nSometimes it is not necessary to know everything about a distribution. Sometimes knowing the mean or variance suffices, and there are several common situations where the mean and variance are easy to calculate\n\n5.2.1 Linear Transformations\nThe easiest of these is a linear transformation of a random variable.\n\nIf \\(X\\) is a random variable with known mean and variance, then\n\\[\n\\begin{aligned}\n    \\E(a X + b) &= a \\E(X) + b \\; \\mbox{, and}\n    \\\\\n        \\Var(a X + b) &= a^2 \\Var (X) \\; .\n\\end{aligned}\n\\]\n\n\nThese are actually pretty easy to prove from the definitions of mean and variance. (Just write down the integrals and do some algebra.) But these results also match our intuition.\n\n\nIf we add or subtract a constant \\(b\\), that increases or decreases every value by the same amount. This will increase the mean by that amount, but does nothing to the variance (since everything is no more or less spread out than it was before). This explains the \\(+b\\) in the first equation and why \\(b\\) does not appear at all in the formula for the variance.\n\n\\[\n\\begin{aligned}\n        \\E(X + b) &= \\int_{-\\infty}^{\\infty} (x+b) f(x) \\; dx  \n              = \\int_{-\\infty}^{\\infty} x f(x) \\; dx  \n                + \\int_{-\\infty}^{\\infty} b f(x) \\; dx  \n              = \\E(X) + b\n              \\\\\n        \\Var(X+b) &= \\int_{-\\infty}^{\\infty} (x + b - (\\mu + b))^2 f(x) \\; dx  \n                    = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\; dx  = \\Var(X)\n\\end{aligned}\n\\]\n\nNow consider multiplying each value by the same amount \\(a\\).\nThis scales all the values by \\(a\\), and hence scales the mean by \\(a\\) as well. This also makes the values more or less spread out (more when \\(|a| > 1\\), less when \\(|a| < 1\\)).\nBut it is the standard deviation – not the variance – that increases or decreases by the factor \\(|a|\\). The variance scales with \\(a^2\\).\n\n\\[\n\\begin{aligned}\n        \\E(aX) &= \\int_{-\\infty}^{\\infty} ax f(x) \\; dx  \n              = a \\int_{-\\infty}^{\\infty} x f(x) \\; dx = a \\E(X)\n              \\\\\n        \\Var(aX) &= \\int_{-\\infty}^{\\infty} (ax - a\\mu)^2 f(x) \\; dx  \n              = a^2 \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x) \\; dx = a^2 \\Var(X)\n\\end{aligned}\n\\]\n\n\n\nExample 5.1 Q. Suppose \\(X\\) has a mean of 5 and a standard deviation of 2. What are the mean and standard deviation of \\(3 X + 4\\)?\nA. \\(\\E(3 X + 4) = 3 \\E(X) + 4 = 3 \\cdot 5 + 4 = 19\\)\n\\(\\Var(3 X + 4) = 3^2 \\Var(X) = 3^2 \\cdot 2^2 = 36\\). So he standard deviation of \\(3X + 4\\) is \\(\\sqrt{36} = 6\\). Notice that \\(6 = 2 \\cdot 3\\).\n\n\n\n\n5.2.2 Sums\nThe most important combination of two random variables is a sum.\n\nTheorem 5.1 Mean and variance of a sum of independent random variables\nLet \\(X\\) and \\(Y\\) be two random variables with known means and variances, then\n\\[\n\\begin{aligned}\n    \\E(X + Y) &= a \\E(X) + \\E(Y) \\; \\mbox{, and}\n    \\\\\n    \\Var(X + Y) &= \\Var (X)  + \\Var(Y) \\; \\mbox{, provided $X$ and $Y$ are independent.}\n\\end{aligned}\n\\]\nThat is,\n\n\nThe expected value of a sum is the sum of the expected values.\nThe variance of a sum is the sum of the variances – provided the variables are independent.\n\n\n\n\n\nThe independence condition for the variance rule is critical. 1\n\nExample 5.2 Q. Suppose \\(X\\) and \\(Y\\) are independent random variables with means 3 and 4 and standard deviations 1 and 2. What are the mean and standard deviation of \\(X + Y\\)?\nA. \\(\\E(X+Y) = 3 + 4\\). \\(\\Var(X + Y) = 1^2 + 2^2 = 5\\), so \\(\\SD(X + Y) = \\sqrt{5} \\approx 2.236\\).\n\n\n\nExample 5.3 Q. Let \\(X \\sim \\Unif(0,1)\\) and \\(Y \\sim \\Unif(0,1)\\) be independent random variables and let \\(S = X+Y\\). What are the mean and variance of \\(S = X + Y\\)?\nA. \\(\\E(S) = \\E(X) + \\E(Y) = \\frac12 + \\frac12 = 1\\). \\(\\Var(S) = \\Var(X) + \\Var(Y) = \\frac1{12} + \\frac1{12} = \\frac16\\).\nNote that this matches the mean and variance of a \\(\\Tri(0,2,1)\\)-distribution, since \\[\n\\frac{ 0 + 2 + 1}{3} = 1 \\;,\n\\] and \\[\n    \\frac{ 0^2 + 2^2 + 1^2 - 0 \\cdot 2 - 0\\cdot 1 - 1\\cdot 2}{18}\n    = \\frac{3}{18} = \\frac16 \\;.\n\\]\nIn fact, it can be shown that \\(S \\sim \\Tri(0,2,1)\\).\n\n\nIf we express the rule for variances in terms of standard deviations we get\n\nTheorem 5.2 The Pythagorean Theorem for standard deviations.\nIf \\(X\\) and \\(Y\\) are independent random variables, then \\[\n    \\SD(X+Y) = \\sqrt{ \\SD(X)^2 + \\SD(Y)^2 }\\; .\n\\] The independence condition plays the role of the right triangle condition in the usual Pythagorean Theorem.\n\n\n\n\n5.2.3 Linear Combinations\nThe results in the preceding sections can be combined and iterated to get results for arbitrary linear combinations of random variables.\n\nTheorem 5.3 Expected value and variance for linear combinations\nLet \\(Y = a_1 X_1 + a_2 X_2 + \\cdots + a_k X_k\\), then\n\\[\n\\begin{aligned}\n    \\E(Y) & =\n    a_1 \\E(X_1) +\n    a_2 \\E(X_1) +\n    \\cdots\n    a_k \\E(X_k)  \n    \\\\[2mm]\n    \\Var(Y) & =\n    a_1^2 \\Var(X_1) +\n    a_2^2 \\Var(X_1) +\n    \\cdots\n    a_k^2 \\Var(X_k) \\; ,\n    \\\\\n    & \\qquad \\mbox{provided $X_1, X_2, \\dots, X_k$ are independent.}  \n\\end{aligned}\n\\]\n\n\n\nExample 5.4 Q. Suppose the means and standard deviations of three independent random variables are as in the table below.\n\n\n\n\nmean\nstandard deviation\n\n\n\n\n\\(X\\)\n100\n15\n\n\n\\(Y\\)\n120\n20\n\n\n\\(Z\\)\n110\n25\n\n\n\nDetermine the mean and standard deviation of \\(X + 2Y - 3Z\\).\nA. The mean is \\(100 + 2 (120) - 3(110) = 10\\).\nThe variance is $122 + 2^2 ^2 + (-3)^2 ^2 = 7450 $, so the standard deviation is \\(\\sqrt{ 7450} = 86.3\\)."
  },
  {
    "objectID": "05-transformations.html#normal-distributions-are-special",
    "href": "05-transformations.html#normal-distributions-are-special",
    "title": "5  Transformation and Combinations of Random Variables",
    "section": "5.3 Normal distributions are special",
    "text": "5.3 Normal distributions are special\nNormal distributions are special because \n\nSpecial Properties of Normal Distributions\n\n\nLinear combinations of independent normal random variables are again normal.\nSums of iid random variables from any distribution are approximately normal provided the number of terms in the sum is large enough.\nThis result follows from what is known as the Central Limit Theorem. The Central Limit Theorem explains why the normal distributions are so important and why so many things have approximately normal distributions.\n\n\n\n\n\nThis means that just knowing the mean and standard deviation tells us everything we need to know about the distribution of the linear combination of normal random variables.\n\nExample 5.5 Let \\(X \\sim \\Norm(10,2)\\) and \\(Y\\sim \\Norm(12,4)\\). If \\(X\\) and \\(Y\\) are independent, then\n\\[\nX + Y \\sim \\Norm(10 + 12, \\sqrt{2^2 + 4^2})\n    = \\Norm(22, 4.47) \\;.\n\\]\n\n\n\nExample 5.6 Let \\(X \\sim \\Norm(10,2)\\) and \\(Y\\sim \\Norm(12,4)\\). If \\(X\\) and \\(Y\\) are independent, then\n\\[\nX - Y \\sim \\Norm(10 - 12, \\sqrt{2^2 + 4^2})\n    = \\Norm(-2, 4.47)\\; .\n\\]\n\n\n\nExample 5.7 Q. Use simulation to illustrate the previous two results.\nA.\n\nX <- rnorm(5000, 10, 2)\nY <- rnorm(5000, 12, 4)\nSum <- X + Y\nDiff <- X - Y\nfitdistr(Sum, \"normal\")\nfitdistr(Diff, \"normal\")\ngf_dhistogram(~ Sum) |> gf_fitdistr(dist = \"norm\") \ngf_dhistogram(~ Diff) |> gf_fitdistr(dist = \"norm\") \ngf_qq(~ Sum) \ngf_qq(~ Diff) \n\n\n\n      mean           sd     \n  21.90981107    4.45151130 \n ( 0.06295388) ( 0.04451511)\n\n\n      mean           sd     \n  -1.96090884    4.46157512 \n ( 0.06309620) ( 0.04461575)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5.8 Let \\(X_i \\simiid \\Unif(0,1)\\). Consider \\(S = \\sum_{i = 1}^{12} X_i\\).\nSince \\(\\E(X_i) = \\frac12\\) and \\(\\Var(X_i) = \\frac{1^2}{12} = \\frac1{12}\\)\n\\[\n\\begin{aligned}\n    \\E(S) &= \\frac12  + \\frac12  + \\cdots \\frac12  = 12 \\cdot \\frac12  = 6\n    \\\\\n    \\Var(S) & = \\frac1{12} + \\frac1{12} + \\cdots \\frac 1{12} = 12 \\cdot \\frac1{12} = 1\n\\end{aligned}\n\\]\nFurthermore, the normal approximation for \\(S\\) is quite good:\n\nX1 <- runif(5000,0,1); X2 <- runif(5000,0,1); X3 <- runif(5000,0,1) \nX4 <- runif(5000,0,1); X5 <- runif(5000,0,1); X6 <- runif(5000,0,1) \nX7 <- runif(5000,0,1); X8 <- runif(5000,0,1); X9 <- runif(5000,0,1) \nX10 <- runif(5000,0,1); X11 <- runif(5000,0,1); X12 <- runif(5000,0,1) \nS <- X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12\nfitdistr(S, \"normal\")\ngf_dhistogram(~S)  |> gf_fitdistr(dist = \"norm\")\ngf_qq(~S)\n\n\n\n      mean           sd     \n  6.033745965   0.994912618 \n (0.014070189) (0.009949126)\n\n\n\n\n\n\n\n\n\n\n\n\nThis means that \\(S - 6 \\approx \\Norm(0,1)\\). This has been used in computer software as a relatively easy way to simulate normal data given a good psuedorandom number generator for \\(\\Unif(0,1)\\)."
  },
  {
    "objectID": "05-transformations.html#estimating-the-mean-of-a-population-by-sampling",
    "href": "05-transformations.html#estimating-the-mean-of-a-population-by-sampling",
    "title": "5  Transformation and Combinations of Random Variables",
    "section": "5.4 Estimating the Mean of a Population by Sampling",
    "text": "5.4 Estimating the Mean of a Population by Sampling\nAs important as data are in statistics, typically we are not interested in our data set, but rather in what we can learn from our data about some larger situation. For example,\n\n\nQuality assurance engineers test a few parts to make a decision about whether the production process is working correctly for all the parts.\nAutomobile manufacturers crash a small number of vehicles to learn how their (other) cars might perform in an accident.\nPublic opinion pollsters survey a (relatively) small number of people in order to learn about the opinions of millions of people.\nIn order to estimate the number of dimes in a large sack of times, you decide to weigh the sack of dimes and divide by the mean weight of a dime. To do this you need to know the mean weight of a dime. You decide to carefully weigh 30 dimes and use those weights to estimate the mean weight of a dime.\n\n\n\nWe have now developed enough background to begin learning how this process works. We begin by introducing some key terms:\n\n\npopulation The collection of individuals, objects, or processes we want to know something about.\nparameter A number that describes (a feature of) a population.\nIn typical applications, parameters are unknown and data are collected for the purpose of estimating parameters.\nsample The collection of individuals, objects, or processes we have data about. Ideally, the sample is a well chosen subset of the population.\nstatistic A number that describes (a feature of) a sample.\nsampling distribution The distribution of a statistic under random sampling.\nThe process of random sampling leads to a random sample, from which a statistic could be computed. Since that number depends on a random process (sampling), it is a random variable. The sampling distribution should not be confused with the distribution of an individual sample (nor with the distribution of the population).\n\n\n\n\nExample 5.9  \n\n\nQuality assurance engineers test a few parts to make a decision about whether the production process is working correctly for all the parts.\n\n\n* population: all parts produced at the plant\n* sample: the parts tested in the quality control protocol\n* parameter: mean strength of all parts produced at the plant\n* statistic: mean strength of the tested parts\n\n\n\nAutomobile manufacturers crash a small number of vehicles to learn how their (other) cars might perform in an accident.\n\n\n* population: all cars (of a certain model) produced\n* sample: the small number of cars that were used in the crash test\n* paramter: average amount of force applied to crash dummy's head in all such crashes (not\n  just the crashes done in the lab)\n* statistic: average amount of force applied to crash dummy's head in the crashes\n  that were tested\n\n\n\nPublic opinion pollsters survey a (relatively) small number of people in order to learn about the opinions of millions of people.\n\n\n* population: all voters\n* sample: people actually contacted\n* parameter: proportion of all voters who will vote for candidate A\n* statistic: proportion of sample who claim they will vote for candidate A\n\n\n\nThe mean weight of a dime can be estimated from the weights of 30 dimes.\n\n\n* population: all dimes in the sack\n* sample: 30 dimes actually weighed\n* parameter: the mean weight of all the dimes in the sack\n* statistic: the mean weight of the 30 dimes actually weighed.\n\n\n\n\n\n\n\nTheorem 5.4 The Central Limit Theorem\nIf \\(X_1, X_1, \\dots, X_n\\) is an iid random sample (of some quantitative variable) from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the sampling distribution of the sample mean or sample sum of a large enough random sample is approximately normally distributed. In fact,\n\n\n\\(\\displaystyle \\mean X \\approx \\Norm(\\mu, \\frac{\\sigma}{\\sqrt{n}})\\)\n\\(\\displaystyle \\sum_{i = 1}^{n} X_i \\approx \\Norm(\\mu, {\\sigma}{\\sqrt{n}})\\)\n\n\n\nThe approximations are better\n\n\nwhen the population distribution is similar to a normal distribution (unimodal, nearly symmetric, etc.), and\nwhen the sample size is larger,\n\n\n\nand are exact when the population distribution is normal.\nThe Central Limit Theorem is illustrated nicely in an applet available from the Rice Virtual Laboratory in Statistics (http://onlinestatbook.com/stat_sim/sampling_dist/index.html).\n\n\nImportant things to note about the Central Limit Theorem\n\n\nThere are three distributions involved: the population, individual sample(s), and the sampling distribution.\nLarge enough is usually not all that large (30–40 is large enough for most quantitative population distributions you are likely to encounter).\nWe could already calculate the expected value and variance of means and sums, the new information in the Central Limit Theorem is about the shape of the resulting sampling distribution.\nThe Central Limit Theorem requires a random sample. In situations where random sampling is not possible, the Central Limit Theorem may still be approximately correct or other more complicated methods may be required.\n\n\n\n\n5.4.1 Estimands, estimates, estimators\nWhen the goal of sampling is to estimate a parameter, it is handy to have the following terminology:\n\n\nestimand A parameter we are trying to estimate. (Sometimes also called the measureand.)\nestimate A statistic calculated from a particular data set and used to estimate the estimand. (Sometimes called a measurement.)\nestimator A random variable obtained by calculating an estimate from a random sample.\nunbiased estimator An estimator for which the expected value is equal to the estimand. So an unbiased estimator is “correct on average”.\n\n\n\nIn this section, our estimand is the mean of the population (\\(mu\\)) and our estimator is \\(\\mean X\\), the mean of a random sample. We will use \\(\\mean x\\) to denote the estimate computed from a particular sample; this is an estimate.\n\n\n5.4.2 If we knew \\(\\sigma\\)\nTypically we will not know \\(\\sigma\\) or \\(\\mu\\). (To know them, one would typically need to know the entire population, but then we would not need to use statistics to estimate \\(\\mu\\) because we would know the exact answer.) But for the moment, let’s pretend we live in a fantasy world where we know \\(\\sigma\\).\n\nExample 5.10 Suppose the standard deviation of the weight of all dimes in our sack of dimes is \\(0.03\\). If we collect a random sample of 25 dimes, then\n\\[\n    \\mean X \\approx \\Norm(\\mu, \\frac{\\sigma}{\\sqrt{n}}) = \\Norm(\\mu, 0.006)\n\\]\nso\n\\[\n    \\mean X - \\mu  \\approx \\Norm(0, \\frac{\\sigma}{\\sqrt{n}}) = \\Norm(0, 0.006) \\;.\n\\]\nThis means that\n\\[\n\\begin{aligned}\n    \\Prob(| \\mean X - \\mu | \\le 0.006) &\\approx 0.68\n    \\\\[4mm]\n    \\Prob(| \\mean X - \\mu | \\le 0.012) &\\approx 0.95\n\\end{aligned}\n\\]\nSo we can be quite confident that our sample mean will be within 0.012 g of the \\(\\mu\\), mean weight of all dimes in the sack.\nExpressed in words, the claim is that 95% of random samples lead to a sample mean that is within 0.012 g of the true mean. Of course, that means 5% of samples lead to a sample mean that is farther away than that. For any given sample, there is no way to know if it is one of the 95% or one of the 5%.\n\n\n\n\n5.4.3 Confidence Intervals (\\(\\sigma\\) known)\nThe typical way of expressing this is with a confidence interval. The key idea is this:\n\nIf \\(\\mean X\\) is close to \\(\\mu\\), then \\(\\mu\\) is close to \\(\\mean X\\).\n\n\nSo an approximate 95% confidence interval is \\[\n\\mean x \\pm 2 SE\n= \\mean x \\pm 2 \\frac{\\sigma}{\\sqrt{n}}\n\\] or more precisely \\[\n\\mean x \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}\n\\] because\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nNotice the switch from \\(\\mean X\\) to \\(\\mean x\\). We used \\(\\mean X\\) when we were considering the random variable formed by taking a random sample and computing the sample mean. \\(\\mean X\\) is a random variable with a distribution. When we are considering a specific data set, we write \\(\\mean x\\) instead.\nThere is a subtly here that often gets people confused about the interpretation of a confidence interval. Although 95% of samples result in 95% confidence intervals that contain the true mean, it is not correct to say that a particular confidence interval has a 95% chance of containing the true mean. Neither the particular confidence interval nor the true mean are random, so no reasonable probability statement can be made about a particular confidence interval computed from a particular data set.\n\n\n5.4.4 Confidence Intervals (\\(\\sigma\\) unknown)\nThe more typical situation is that \\(\\sigma\\) is not known and needs to be estimated from the data. It has been known for a long time that when randomly sampling from a normal population,\n\\[\n\\E\\left( \\sum_{i = 1}^n (X_i - \\mean X)^2 \\right) = (n-1) \\sigma^2\n\\]\nThis means that\n\\[\nS^2 = \\frac{ \\sum_{i = 1}^n (X_i - \\mean X)^2 }{n-1}\n\\] is an unbiased estimator of \\(\\sigma^2\\).2 This explains the reason for the \\(n-1\\) in the denominator of the sample variance.3\nAn obvious, but not quite correct solution to our unknown \\(\\sigma\\) dilemma is to use \\(s\\) in pace of \\(\\sigma\\). In fact this was routinely done until 1908 Student (1908), when William Gosset, publishing under the pseudonym Student, pointed out that when sampling from a \\(\\Norm(\\mu, \\sigma)\\) population,\n\\[\n\\frac{ \\mean X - \\mu}{\\sigma/\\sqrt{n}} \\sim \\Norm(0,1)\n\\mbox{\\;\\ but\\ }\n\\frac{ \\mean X - \\mu}{s/\\sqrt{n}} \\sim \\Tdist(n-1)\n\\] The new family of distributions (called Student’s \\(t\\)-distributions) are very similar to the normal distributions – but “shorter and fatter”. This means that one must go farther into the tails of a \\(t\\)-distribution to capture the central 95%.\n\n\nCode\ngf_dist(\"norm\", main = \"Normal and T-distributions\") |>\n  gf_fun(dt(x, df = 2) ~ x, col = 'gray80') |>\n  gf_fun(dt(x, df = 4) ~ x, col = 'gray60') |>\n  gf_fun(dt(x, df = 8) ~ x, col = 'gray40') |>\n  gf_fun(dt(x, df = 16) ~ x, col = 'gray20')\n\n\n\n\n\nFigure 5.5: Some t-distributions\n\n\n\n\nThe resulting confidence interval has the form \\[\n\\mean x \\pm t_* \\frac{s}{\\sqrt{n}}\n\\]\n\n\n\n5.4.5 Standard Error\nThe Central Limit Theorem tells us that (under certain conditions), the standard deviation of the sampling distribution for the sample mean is \\(\\frac{\\sigma}{\\sqrt{n}}\\). Typically we don’t know \\(\\sigma\\) so we estimate this quantity with \\(\\frac{s}{\\sqrt{n}}\\). To avoid having to say “the estimated standard deviation of the sampling distribution”, we introduce a new term\n\n\nstandard error the estimated standard deviation of a sampling distribution\n\n\n\nWe will typically abbreviate standard error as SE. (Some authors use se.) Statistical software often includes standard errors in output.\nConfidence intervals for the mean can now be expressed as \\[\n\\mean x \\pm t_* SE\n\\] We will see other intervals that make use of the \\(t\\)-distributions. All of them share a common structure:\n\n\\[\n        \\mbox{estimate} \\pm t_* SE\n\\]\n\n\nThe value of \\(t_*\\) needed for a 95% confidence interval is calculated similar to the way we calculated \\(z_*\\), but we need to know the degrees of freedom parameter for the \\(t\\)-distribution (\\(n-1\\) for this situation).\n\nExample 5.11 Suppose a sample of 30 dimes has a mean weight of 2.258 g and a standard deviation of 0.022 g. We can calculate a 95% confidence interval as follows:\n\nx_bar <- 2.258\nt_star <- qt(0.975, df = 29); t_star\n\n[1] 2.04523\n\nSE <- 0.022/ sqrt(30); SE     # standard error\n\n[1] 0.004016632\n\nME <- t_star * SE; ME         # margin of error\n\n[1] 0.008214935\n\nx_bar + c(-1,1) * ME\n\n[1] 2.249785 2.266215\n\n\nIf you have the data (and not just the the summary statistics \\(\\mean x\\) and \\(s\\)), R can automate this entire computation for us with the t.test() function.\n\nt.test( ~ mass, data = Dimes)    \n\n\n    One Sample t-test\n\ndata:  mass\nt = 560.44, df = 29, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 2.249992 2.266474\nsample estimates:\nmean of x \n 2.258233 \n\nconfint(t.test( ~ mass, data = Dimes))  # just the CI without the other stuff\n\n\n\n  \n\n\n\n\n\n\n\n5.4.6 Interpreting Confidence Intervals\nHere is an illustration of 100 confidence intervals computed by sampling from a normal population with mean \\(\\mu = 100\\).\n\n\n\n\n\n\n\n\nNotice that some of the samples have larger means (the dots) and some smaller means. Also some have wider intervals and some narrower (because \\(s\\) varies from sample to sample). But most of the intervals contain the estimand (100). A few do not.\nIn the long-run, 95% of the intervals should “cover” the estimand and 5% should fail to cover. 95% is referred to as the confidence level or coverage rate.\nAs we can see, the estimand is not always contained in the confidence interval. But, in a way that we will be able to make more formal later, a confidence interval is a range of plausible values for the estimand – values that are consistent with the data in a probabilistic sense. The level of confidence is related to how strong the evidence must be for us to declare that a value is not consistent with the data.\n\n\n5.4.7 Other confidence levels\nWe can use other confidence levels by using a different critical value \\(t_*\\).\nSo the general form for our confidence interval is\n\\[\n\\mean x \\pm t_* SE\n\\]\n\nExample 5.12 A 98% confidence interval, for example, requires a larger value of \\(t_*\\). If the sample size is \\(n = 30\\), then we use\n\nqt(0.99, df = 29)\n\n[1] 2.462021\n\n\nNotice the use of 0.99 in this command. We want to find the limits of the central 98% of the standard normal distribution. If the central portion contains 98% of the distribution, then each tail contains 1%.\nWe could also have used the following to calculate \\(t_*\\).\n\nqt(0.01, df = 29)\n\n[1] -2.462021\n\n\n\n\n\nExample 5.13 Q. Compute a 98% confidence interval for the mean weight of a dime based on our dimes data set.\nA. We can do this by hand:\n\nx_bar <- 2.258\nt_star <- qt(0.99, df = 29); t_star\n\n[1] 2.462021\n\nSE <- 0.022/ sqrt(30); SE     # standard error\n\n[1] 0.004016632\n\nME <- t_star * SE; ME         # margin of error\n\n[1] 0.009889034\n\nx_bar + c(-1,1) * ME\n\n[1] 2.248111 2.267889\n\n\nor let R do the work for us:\n\nt.test( ~ mass, data = Dimes, conf.level = 0.98)\n\n\n    One Sample t-test\n\ndata:  mass\nt = 560.44, df = 29, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n98 percent confidence interval:\n 2.248313 2.268154\nsample estimates:\nmean of x \n 2.258233 \n\nconfint(t.test( ~ mass, data = Dimes, conf.level = 0.98))\n\n\n\n  \n\n\n\n\n\n\n\n5.4.8 Robustness\nThe confidence intervals based on the \\(t\\)-distributions assume that the population is normal. The degree to which a statistical procedure works even when some or all of the assumptions used to derive its mathematical properties are not satisfied is referred to as the robustness of the procedure. The \\(t\\)-based confidence intervals are quite robust.\nQuantifying robustness precisely is difficult because how well a procedure works may depend on many factors. The general principles are\n\n\nThe bigger the better.\n  The larger the sample size, the less it mattes what the population\n  distribution is.\nThe more normal the better.\n  The closer the population is to a normal distribution,\n  the smaller the sample sizes may be.\n\n\n\nFor assistance in particular applications, we offer the following rules of thumb.\n\n\nIf the population is normal, the confidence intervals achieve the stated coverage rate for all sample sizes.\n But since small data sets provide very little indication\n of the shape of the population distribution, the normality assumption\n must be justified by something other than the data.\n (Perhaps other larger data sets collected in a similar fashion \n have shown that normality is a good assumption or perhaps there\n is some theoretical reason to accept the normality assumption.)\nFor modestly sized samples (\\(15 \\le n \\le 40\\)), the \\(t\\)-based confidence is acceptable as long as the distribution appears to be unimodal and is not strongly skewed.\nFor large sample size (\\(n \\ge 40\\)), the \\(t\\)-procedure will work acceptably well for most unimodal distributions.\n But keep in mind, if the distribution is strongly skewed, the \n mean might not be the best parameter to estimate.\nBecause both the sample mean and the sample variance are sensitive to outliers, one should proceed with caution when outliers are present.\n Outliers that are due to mistakes and can be corrected, should be.  Outliers that can be verified\n to be incorrect but cannot be corrected should be removed.  It is not\n acceptable to remove an outlier just because you don't want it in your data.\n But sometimes statisticians do \"leave one out analysis\" where they run the \n analysis with and without the outlier.  If the conclusions are the same, then\n the conclusions can be safely drawn.  But if the conclusions are different,\n likely additional data will be needed to resolve the differences.\n\n Don't forget: sometimes the outliers are the interesting part of the story.\n Determining what makes them different from the rest of the data may be \n the most important thing."
  },
  {
    "objectID": "05-transformations.html#exercises",
    "href": "05-transformations.html#exercises",
    "title": "5  Transformation and Combinations of Random Variables",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nExercise 5.1 Gamma distributions\nLet \\(X\\) and \\(Y\\) be independent \\(\\Gamm(\\texttt{shape} = 2, \\texttt{scale} = 3)\\) random variables.\nLet \\(S = X+Y\\) and let \\(D = X - Y\\). Use simulations (with 5000 replications) and quantile-quantile plots to answer the following:\n\n\nFit a normal distribution to \\(S\\) using fitdistr().\nIs the normal distribution a good fit?\nFit a Gamma distribution to \\(S\\) using fitdistr().\nIs the Gamma distribution a good fit?\nFit a normal distribution to \\(D\\) using fitdistr().\nIs the normal distribution a good fit?\nWhy is it not a good idea to fit a Gamma distribution to \\(D\\)?\n\n\n\n\n\n\n\nSolution. \nn <- 5000\nX <- rgamma(n,  shape = 2, scale = 3)\nY <- rgamma(n,  shape = 2, scale = 3)\nS <- X + Y\nD <- X - Y\n\n\nfitdistr(S, \"normal\")\n\n      mean           sd     \n  11.74338061    5.97830327 \n ( 0.08454598) ( 0.05978303)\n\ngf_qq( ~ S)\n\n\n\ngf_density( ~ S)\n\n\n\n\n\nfitdistr(S, \"gamma\")\n\n      shape         rate    \n  3.864842047   0.329108223 \n (0.074200617) (0.006747429)\n\ngf_qq( ~ S, distribution = qgamma, dparams = list(shape = 4.04, rate = 0.333))\n\n\n\ngf_density( ~ S)\n\n\n\n\nThe gamma distribution fits \\(S\\) much better than the normal does. But notice that the shape parameter is larger than the shape parameters for \\(X\\) and \\(Y\\).\n\nfitdistr(D, \"normal\")\n\n      mean          sd    \n  0.07960858   5.74300445 \n (0.08121835) (0.05743004)\n\ngf_qq( ~ D)\n\n\n\ngf_density( ~ D)\n\n\n\n\nThis time the distribution is symmetric, so the problems are not as obvious as for \\(S\\), but the normal quantile plots shows some clear curve (there is lots of data, so very little noise in this plot). The problem is that the shape is not like a normal distribution.\nThe density plot perhaps makes this clearer if you are not familiar with qq-plots.\n\n\n\nExercise 5.2 Normal probabilities\nIf \\(X \\sim \\Norm(110, 15)\\) and \\(Y \\sim \\Norm(100, 20)\\) are independent random variables:\n\n\nWhat is \\(\\Prob(X \\ge 140)\\)?\nWhat is \\(\\Prob(Y \\ge 140)\\)?\nWhat is \\(\\Prob(X \\ge 150)\\)?\nWhat is \\(\\Prob(Y \\ge 150)\\)?\nWhat is \\(\\Prob(X + Y \\ge 250)\\)?\nWhat is \\(\\Prob(X \\ge Y)\\)? (Hint: \\(X \\ge Y \\Leftrightarrow X - Y \\ge 0\\).)\n\n\n\n\n\n\n\nSolution. \n1 - pnorm(140, mean = 110, sd = 15) #  P(X > 140)\n\n[1] 0.02275013\n\n1 - pnorm(140, mean = 100, sd = 20) #  P(Y > 140)\n\n[1] 0.02275013\n\n1 - pnorm(150, mean = 110, sd = 15) #  P(X > 150)\n\n[1] 0.003830381\n\n1 - pnorm(150, mean = 100, sd = 20) #  P(Y > 150)\n\n[1] 0.006209665\n\n\n\\(X + Y \\sim \\Norm(210, 25 )\\) because \\(110 + 100 = 210\\) and \\(15^2 + 20^2 = 25^2\\).\nSo \\(\\Prob(X + Y \\ge 250)\\) is\n\n1 - pnorm(250, 210, 25) \n\n[1] 0.05479929\n\n\n\\(X - Y \\sim \\Norm(10, 25 )\\) because \\(110 - 100 = 210\\) and \\(15^2 + 20^2 = 25^2\\).\nSo \\(\\Prob(X - Y \\ge 0)\\) is\n\n1 - pnorm(0, 10, 25) \n\n[1] 0.6554217\n\n\n\n\n\nExercise 5.3 Linear Combinations\nSuppose \\(X\\) and \\(Y\\) are independent random variables with means and standard deviations as listed below.\n\n\n\n\nmean\nstandard deviation\n\n\n\n\n\\(X\\)\n54\n12\n\n\n\\(Y\\)\n48\n9\n\n\n\nWhat are the mean and standard deviation of each of the following:\n\n\n\\(X + Y\\)\n\\(2X\\)\n\\(2X + 3Y\\)\n\\(2X - 3Y\\)\n\n\n\n\n\n\n\nSolution. \n# part a\nc(mean = 54 + 48, sd= sqrt(12^2 + 9^2))\n\nmean   sd \n 102   15 \n\n# part b\nc(mean = 2 * 54, sd= 2 * 12)\n\nmean   sd \n 108   24 \n\n# part c\nc(mean = 2 * 54 + 3 * 48, sd= sqrt(2^2 * 12^2 + 3^2 * 9^2))\n\n     mean        sd \n252.00000  36.12478 \n\n# part d\nc(mean = 2 * 54 - 3 * 48, sd= sqrt(2^2 * 12^2 + (-3)^2 * 9^2))\n\n     mean        sd \n-36.00000  36.12478 \n\n\n\n\n\nExercise 5.4 Calvin heights\nYou are interested to know the mean height of male Calvin students. Assuming the standard deviation is similar to that of the population at large, we will assume \\(\\sigma = 2.8\\) inches.\n\n\nWhat is the distribution of \\(\\mean X - \\mu\\)? (Hint: start by determining the distribution of \\(\\mean X\\).)\nIf you measure the heights of a sample of 20 students, what is the probability that your mean will be within 1 inch of the actual mean?\nHow large would your sample need to be to make this probability be 95%?\n\n\n\n\n\n\n\nSolution. \n\n\\(\\Norm(0, \\frac{\\sigma}{\\sqrt{n}}) =  \\Norm(0, \\frac{2.8}{\\sqrt{n}})\\)\n\n\n\npnorm(1, mean = 0, sd = 2.8/sqrt(20)) - pnorm(-1, mean = 0, sd = 2.8/sqrt(20))\n\n[1] 0.889777\n\n\n\nWe would need 1 to be equal to \\(2 \\frac{\\sigma}{\\sqrt n}\\). So\n\n\nn <- (2 * (2.8))^2; n\n\n[1] 31.36\n\n# double check:\nn <- round(n); n  # should have an integer\n\n[1] 31\n\npnorm(1, mean = 0, sd = 2.8/sqrt(n)) - pnorm(-1, mean = 0, sd = 2.8/sqrt(n))\n\n[1] 0.9532422\n\n\nThis can also be done by guessing and checking for the value of $n$ (sort of like the \n    guessing game where someone tells you \"higher\" or \"lower\" after each guess until \n    you converge on the number they have selected.\n    \n\nprob <- makeFun(pnorm(1, mean = 0, sd = 2.8/sqrt(n)) - pnorm(-1, mean = 0, sd = 2.8/sqrt(n)) ~ n)\nprob(20)\n\n[1] 0.889777\n\nprob(40)\n\n[1] 0.9761023\n\nprob(30)\n\n[1] 0.9495527\n\nprob(32)\n\n[1] 0.9566482\n\nprob(31)\n\n[1] 0.9532422\n\n\nR can even automate this for us:\n\nuniroot(makeFun(prob(n) - 0.95 ~ n), c(20, 40))  # look for a solution between 20 and 40\n\n$root\n[1] 30.11704\n\n$f.root\n[1] 4.489753e-10\n\n$iter\n[1] 7\n\n$init.it\n[1] NA\n\n$estim.prec\n[1] 6.103516e-05\n\n\nWe should round the answer, of course.\n\n\n\n\n\nExercise 5.5 Confidence interval for a mean\nGive an approximate 95% confidence interval for a population mean \\(\\mu\\) if the sample of size \\(n = 25\\) has mean \\(\\mean x = 8.5\\) and the population standard deviation is \\(\\sigma = 1.7\\).\n\n\n\n\nSolution. \nSE <- 1.7 / sqrt(25) ; SE # standard error\n\n[1] 0.34\n\nME <- 2 * SE; ME          # margin of error\n\n[1] 0.68\n\n8.5 + c(-1,1) * ME        # approx 95<!--  CI -->\n\n[1] 7.82 9.18\n\n\n\n\n\nExercise 5.6 Critical values\nDetermine the critical value \\(t_*\\) for each of the following confidence levels and sample sizes.\n\n\n95% confidence level; \\(n = 4\\)\n95% confidence level; \\(n = 24\\)\n98% confidence level; \\(n = 15\\)\n90% confidence level; \\(n = 20\\)\n99% confidence level; \\(n = 12\\)\n95% confidence level; \\(n = 123\\)\n\n\n\n\n\n\nSolution. Here’s a fancy version that gets all the answers at once. (You didn’t need to know you could do it this way, but R is pretty handy this way.)\n\nanswers <- qt(c(.975, .975, .99, .95, .995, .975), df = c(3,23,14,19,11,122))\nnames(answers) <- letters[1:length(answers)]\nanswers\n\n       a        b        c        d        e        f \n3.182446 2.068658 2.624494 1.729133 3.105807 1.979600 \n\n\n\n\n\nExercise 5.7 Stride rate\nBelow is a normal-quantile plot and some summary information from a sample of the stride rates (strides per second) of healthy men.\n\n\n\n\n\n\n\n  \n\n\n\n\n\nWhat is the standard error of the mean for this sample?\nConstruct a 98% confidence interval for the mean stride rate of healthy men.\nDoes the normal-quantile plot suggest any reasons to worry about the normality assumption?\n\n\n\n\n\n\n\nSolution. \nt.test( ~ strideRate, data = ex07.37, conf.level = 0.98)\n\n\n    One Sample t-test\n\ndata:  strideRate\nt = 51.132, df = 19, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n98 percent confidence interval:\n 0.8795348 0.9714652\nsample estimates:\nmean of x \n   0.9255 \n\n\nThe normal quantile plot is OK but not great. There are some flat spots in the plot indicating ties. This is probably because the number of steps was counted over a short length of time, so the underlying data is probably discrete – leading to the ties. But the distribution is unimodal and not heavily skewed. With a sample size of 20 we can probably proceed cautiously. (In practice, we might want to confirm the results with a method that doesn’t require such a strong normality assumption, but we don’t know any of those methods yet.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE-glass fiber\n\nA random sample of size \\(n = 8\\) E-glass fiber test specimens of a certain type yielded a sample mean interfacial shear yield stress of 30.2 and a sample standard deviation of 3.1. Assuming that the population of interfacial shear yield stress measurements is approximately normal, compute a 95% confidence interval for the true average stress.\n\n\n\n\nSolution. \nSE <- 3.1 / sqrt(8); SE              # standard error\n\n[1] 1.096016\n\nt_star <- qt(0.975, df = 7); t_star  # critical value\n\n[1] 2.364624\n\nme <- t_star * SE; me                # margin of error\n\n[1] 2.591665\n\n30.2 + c(-1,1) * me                  # the 95<!--  confidence interval -->\n\n[1] 27.60834 32.79166\n\n\n\n\n\nExercise 5.8 Polymerization of paper\n\nThe code below will create a data set containing a sample of observations of polymerization degree for some paper specimens. The data have been sorted to assist in typing. (If the data actually occurred in this order, we would probably be doing a different sort of analysis.)\n\nPaper <- \n    tibble(\n      polymer = \n        c(418, 421, 421, 422, 425, 427, 431, 434, \n        437, 439, 446, 447, 448, 453, 454, 463, 465))\n\n\n\nCreate a normal-quantile plot to see if there are any reasons to worry about the assumption that the population is approximately normal.\nCalculate a 95% confidence interval for the mean degree of polymerization for the population of such paper runs. The authors of the paper did this too.\nBased on your confidence interval, is 440 a plausible value for the mean degree of polymerization? Explain.\n\n\n\n\n\n\n\nSolution. \ngf_qq( ~ polymer, data = Paper)\n\n\n\ngf_density( ~ polymer, data = Paper)\n\n\n\nt.test( ~ polymer, data = Paper)\n\n\n    One Sample t-test\n\ndata:  polymer\nt = 119.33, df = 16, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 430.5077 446.0805\nsample estimates:\nmean of x \n 438.2941 \n\n\nThe tails of the sample distribution do not stretch quite as far as we would expect for a normal distribution, but the distribution is unimodal and not heavily skewed, so we are probably still OK.\n\n\n\nExercise 5.9 Alice and Bob\nUsing the same data, Alice constructs a 95% confidence interval and Bob creates a 98% confidence interval. Which interval will be wider? Why?\n\n\n\nSolution. Bob’s interval will be wider because he used a higher level of confidence. (This will cause \\(t_*\\) to be larger. They will both have the same standard error.)\n\n\n\nExercise 5.10 Lab partners\nCharlie and Denise are working on the same physics lab. Charlie leaves lab early and only has a sample size of \\(n = 15\\). Denise stays longer and has a sample size of \\(n = 45\\). Each of them construct a 95% confidence interval from their samples.\n\n\nWhose confidence interval would you expect to be wider?\nUnder what conditions could it be the other way around?\n\n\n\n\n\n\nSolution. We would expect the two standard deviations to be fairly close, but since Denise has quite a bit more data, her standard error will be smaller. This will make her interval narrower. So Charlie’s interval will be wider, unless Charlie gets an unusually small standard deviation and Denise gets an unusually large standard deviation.\n\n\n\nExercise 5.11 Find and article\nFind an article from the engineering or science literature that computes a confidence interval for a mean (be careful, you may see confidence intervals for many other parameters) and also reports the sample mean and standard deviation. Check their computation to see if you both get the same confidence interval. Give a full citation for the article you used.\nGoogle scholar might be a useful tool for this. Or you might ask an engineering or physics professor for an appropriate engineering journal to page through in the library. Since the chances are small that two students will find the same article if working independently, I expect to see lots of different articles used for this problem.\nIf your article looks particularly interesting or contains statistical things that you don’t understand but would like to understand, let me know, and perhaps we can do something later in the semester with your article. It’s easiest to do this if you can give me a URL for locating the paper online.\n\n\n\nSolution. Answers will vary."
  },
  {
    "objectID": "05-transformations.html#review-exercises",
    "href": "05-transformations.html#review-exercises",
    "title": "5  Transformation and Combinations of Random Variables",
    "section": "5.6 {Review Exercises}",
    "text": "5.6 {Review Exercises}\n\nExercise 5.12 Defective parts\nEven when things are running smoothly, 5% of the parts produced by a certain manufacturing process are defective.\n\n\nIf you select 10 parts at random, what is the probability that none of them are defective?\n\n\n\nSuppose you have a quality control procedure for testing parts to see if they are defective, but that the test procedure sometimes makes mistakes:\n\n\nIf a part is good, it will fail the quality control test 10% of the time.\n20% of the defective parts go undetected by the test.\n\n\n\n\n\nWhat percentage of the parts will fail the quality control test?\nIf a part passes the quality control test, what is the probability that the part is defective?\nThe parts that fail inspection are sold as “seconds”. If you purchase a “second”, what is the probability that it is defective?\n\n\n\n\n\n\n\nSolution. \n\n\\(\\Prob(\\mbox{none defective}) = \\Prob(\\mbox{all are good}) = 0.95^{10} = 0.599 = 59.9%\\)\nEven though only \\(5\\)% are defective, nearly \\(14\\)% fail the quality control:\n\n\\[\n\\begin{aligned}\n\\Prob(\\mbox{fail test})\n&= \\Prob(\\mbox{good and fail}) + \\Prob(\\mbox{bad and fail})\n\\\\\n& = \\Prob(\\mbox{good}) \\Prob(\\mbox{fail}\\mid \\mbox{good})\n    + \\Prob(\\mbox{bad}) \\Prob(\\mbox{fail}\\mid \\mbox{bad})\n    \\\\\n&= 0.95 (0.10) + 0.05 (.80) = 0.095 + 0.04 = 0.135 = 13.5\\%\n\\end{aligned}\n\\] c. If a part passes QC, the probability that it is defective drops from 5% to just over 1%:\n\\[\n\\begin{aligned}\n\\Prob(\\mbox{bad} \\mid \\mbox{pass})\n&= \\frac{ \\Prob(\\mbox{bad and pass})}{\\Prob(\\mbox{pass})}\n\\\\\n& =\n\\frac{ 0.05 (0.20) }{ 0.865 }\n=\n0.01156\n\\end{aligned}\n\\]\nThe cost to get this improvement in quality is the cost of the QC test plus the cost of discarding \\(9.5\\)% of good parts in the QC process.\n\n\n\n\\[\n    \\Prob(\\mbox{bad} \\mid \\mbox{fail}) =\n    \\Prob(\\mbox{bad} and  \\mbox{fail}) =\n    \\Prob(\\mbox{fail})  \n    = 0.04 / (0.04 + 0.095)\n    = 0.2962963\n\\]\n\n0.04 / (0.04 + 0.095)\n\n[1] 0.2962963\n\n\n\n\n\n\n\nExercise 5.13 What does the density look like?\nHere is a normal quantile plot from a data set.\n\n\n\n\n\nSketch what a density plot of this same data would look like.\n\n\n\n\nSolution. \n\n\n\n\nThe important feature to get right is the direction of the skew.\n\n\n\nExercise 5.14 Create your own practice problems\nYou should know how to compute confidence intervals for a single quantitative variable both “by hand” and using t.test() if you have data. Here is a template problem you can use to practice both of these.\n\n\nSelect a data set and a quantitative variable in that data set. For example, Length in the KidsFeet data set.\nUse df_stats() to compute some summary statistics.\n\n\ndf_stats ( ~ length, data = KidsFeet)\n\n\n\n  \n\n\n\n\nPick a confidence level. Example: 90% confidence.\nFrom this information, compute a confidence interval\nNow check that you got it right using t.test()\nIt is possible that you chose a variable for which this is not an appropriate procedure. Be sure to check for that, too.\n\n\n\nYou can find other data sets using\n\ndata()\n\nYou won’t run out of examples before you run out of energy for doing these.\n\n\n\n\nSolution. \ndf_stats( ~ length, data = KidsFeet)\n\n\n\n  \n\n\nx_bar <- 24.72\nn <- 39; n\n\n[1] 39\n\ns <- 1.32; s\n\n[1] 1.32\n\nSE <- s / sqrt(n) ; SE\n\n[1] 0.2113692\n\nt_star <- qt(0.95, df = 38) ; t_star\n\n[1] 1.685954\n\nme <- t_star * SE; me\n\n[1] 0.3563588\n\nconfint(t.test( ~length, data = KidsFeet)) \n\n\n\n  \n\n\ngf_qq( ~length, data = KidsFeet)\n\n\n\n\nThe normal-quantile plot does not reveal any causes for concern, but we might be concerned that the distributions for boys and girls are different. Let’s do a graphical check to see if that is a problem.\n\ngf_boxplot(length ~ sex, data = KidsFeet)\n\n\n\ngf_dens( ~ length, color = ~sex, linewidth = 1.5, data = KidsFeet)\n\n\n\n\nIndeed, it appears that the girls’ feet are on average a bit shorter, so perhaps we should create a confidence intervals separately for each group. Now you have two problems. You can get started with\n\ndf_stats(length ~ sex, data = KidsFeet)\n\n\n\n  \n\n\n\n\n\n\nExercise 5.15 Working with a pdf\nThe pdf for a continuous random variable \\(X\\) is\n\\[\nf(x) = \\begin{cases}\n4(x - x^3) & \\mbox{when } 0\\le x \\le 1 \\\\\n0 & \\mbox{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\nDetermine \\(\\Prob(X \\le \\frac{1}{2})\\).\nDetermine the mean and variance of \\(X\\).\n\n\n\n\n\n\n\nSolution. \nlibrary(mosaicCalc)\n  f <- makeFun(4*(x-x^3) ~ x)\n  F <- antiD(f(x) ~ x)\n xF <- antiD(x * f(x) ~ x)\nxxF <- antiD(x^2 * f(x) ~ x)\nF(1) - F(0)                   # should be 1\n\n[1] 1\n\nm <- xF(1) - xF(0); m         # mean\n\n[1] 0.5333333\n\nxxF(1) - xxF(0) - m^2         # variance\n\n[1] 0.04888889\n\n\n\n\n\nExercise 5.16 Working with a kernel\nThe kernel of a continuous distribution is given by \\(k(x) = 4-x^2\\) on the interval \\([-2,2]\\).\n\n\nDetermine the pdf of the distribution.\nCompute the mean and variance of the distribution.\n\n\n\n\n\n\n\nSolution. \nF <- antiD(4 - x^2 ~ x)\nF(2) - F(-2) \n\n[1] 10.66667\n\n\nSo the pdf is\n\\[\n    f(x) = \\frac{4 - x^2}{10.6666667}\n\\]\n\nf <- makeFun((4 - x^2) / C ~ x, C = F(2) - F(-2))\nintegrate(f, -2, 2)\n\n1 with absolute error < 1.1e-14\n\nF <- antiD(f(x) ~ x)\nF(2) - F(-2)\n\n[1] 1\n\nxF <- antiD(x*f(x) ~ x)\nm <- xF(2) - xF(-2)           # mean\nxxF <- antiD(x*x*f(x) ~ x)  \nxxF(2) - xxF(-2) - m^2        # variance\n\n[1] 0.8\n\n\n\n\n\nExercise 5.17 Gamma distributions\nLet \\(X \\sim \\Gamm(\\texttt{shape} = 3, \\texttt{rate} = 2)\\).\n\n\nPlot the pdf of the distribution of \\(X\\).\nDetermine \\(\\Prob(X \\le 1)\\).\nWhat proportion of the distribution is between \\(1\\) and \\(3\\)?\nUse the table to determine the mean, variance, and standard deviation of \\(X\\).\nUse integration to determine the mean, variance, and standard deviation of \\(X\\). (You should get the same answers as above.)\nMake up other problems like this for any of the distributions in the table if you want additional practice.\n\n\n\n\n\n\n\nSolution. \ngf_dist(\"gamma\", params = list(shape = 3, rate = 2))\n\n\n\npgamma(1, shape = 3, rate = 2)\n\n[1] 0.3233236\n\npgamma(3, shape = 3, rate = 2) -  pgamma(1, shape = 3, rate = 2)\n\n[1] 0.6147076\n\nF <- antiD(dgamma(x, shape = 3, rate = 2) ~ x)\nF(Inf) - F(0)    # should be 1.\n\n[1] 1\n\nxF <- antiD(x * dgamma(x, shape = 3, rate = 2) ~ x)\nm <- xF(Inf) - xF(0); m\n\n[1] 1.5\n\nxxF <- antiD(x^2 * dgamma(x, shape = 3, rate = 2) ~ x)\nxxF(Inf) - xxF(0)\n\n[1] 3\n\nxxF(Inf) - xxF(0) - m^2 # variance\n\n[1] 0.75\n\n\n\n\n\nExercise 5.18 Linear combinations\nSuppose \\(X\\) an \\(Y\\) are independent random variables with means and standard deviations as given in the following table.\n\n\n\n\nmean\nstandard deviation\n\n\n\n\n\\(X\\)\n40\n3\n\n\n\\(Y\\)\n50\n4\n\n\n\nDetermine the mean and standard deviation of the following:\n\n\\(X+Y\\)\n\\(X-Y\\)\n\\(\\frac{1}{2} X + 7\\)\n\nReminder: Expected value is another term for mean.\n\n\n\nSolution. \\(\\E(X+Y) = \\E(X) + \\E(Y) = 40 + 50 = 90\\).\n\\(\\Var(X+Y) = \\Var(X) + \\Var(Y) = 3^2 + 4^2 = 5^2\\). So standard deviation = \\(5\\).\n\\(\\E(X-Y) = \\E(X) - \\E(Y) = 40 - 50 = -10\\).\n\\(\\Var(X-Y) = \\Var(X + (-Y)) = \\Var(X) + \\Var(-Y) = 3^2 + 4^2 = 5^2\\). So standard deviation = \\(5\\).\n\\(\\E(\\frac12 X+7) = \\frac12\\E(X) + 7 = 27\\).\n\\(\\Var(\\frac12 X+7) = \\frac14\\Var(X) = \\frac94\\). So standard deviation = \\(\\sqrt{\\frac94}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.19 Terminology review\n\n\nWhat is the difference between a statistic and a parameter?\nWhat is a sampling distribution?\nCreate other similar problems using important terms from this class.\n\n\n\n\n\n\n\nExercise 5.20 Critical flicker frequency\nCritical flicker frequency (called Flicker in the data set below) is the lowest flicker rate at which the human eye can detect that a light source (from a florescent bulb or a computer screen, for example) is flickering. Knowing the cff is important for product manufacturing (since detectable flickering is annoying for the consumer). The command below loads data from from a 1973 study that attempted to determine whether the cff, which varies from person to person, is partially determined by eye color.\n\nFlicker <- read.file(\"http://www.statsci.org/data/general/flicker.txt\")\n\nReading data with read.table()\n\n\nCreate a plot that can be used to visualize the data. What does your plot suggest the answer might be? (Does eye color matter?)\n\n\n\nSolution. Here are two possibilities:\n\ngf_boxplot(Flicker ~ Colour, data = Flicker)\n\n\n\ngf_density( ~ Flicker, fill=~Colour, data = Flicker) |>\n  gf_refine(scale_fill_manual(values = c('steelblue', 'tan4', 'forestgreen')))\n\n\n\n\nThe general trend appears to be that lighter colored eyes (if you consider blue to be lighter than green) appear to be able to detect higher flicker rates, but there is overlap among the groups.\n\n\n\n\nExercise 5.21 Difference between two means\nLet \\(\\mean X_1\\) be the distribution of sample means from a population with mean \\(\\mu_1\\) and standard deviation \\(\\sigma_1\\). Let \\(\\mean X_2\\) be the distribution of sample means from a population with mean \\(\\mu_2\\) and standard deviation \\(\\sigma_2\\). What is the distribution of \\(\\mean X_1 - \\mean X_2\\)? (Hint: start by determining the distributions of \\(\\mean X_1\\) and \\(\\mean X_2\\), then use what we know about differences.)\n\n\n\nExercise 5.22 Difference between two proportions\nSimilar to the previous problem, we can work out the (approximate) sampling distribution for the difference between two proportions. Use the fact that the sampling distribution for a sample proportion is approximately \\(\\Norm(p, \\sqrt{\\frac{p(1-p)}{\\sqrt{n}}})\\) to work out the (approximate) sampling distribution for the difference between two sample proportions.\n\n\n\n\nExercise 5.23 Concrete beams\nBelow is the summary of some data from a study comparing two types of concrete beams by measuring their ultimate load (in kN).\n\n\n\n\n\n\n\n\n\ntype\nsample size\nmean\nstandard deviation\n\n\n\n\nfiberglass grid\n26\n33.4\n2.2\n\n\ncommercial carbon grid\n26\n42.8\n4.3\n\n\n\nCreate a 95% confidence interval for the difference in the mean ultimate load for the two types of beam.\n\n\n\n\nSolution. \nSE <- sqrt( 2.2^2 / 26 + 4.3^2 / 26); SE\n\n[1] 0.9472633\n\ntstar <- qt(0.975, df = 25); tstar   # conservative\n\n[1] 2.059539\n\n42.8 - 33.4 + c(-1,1) * tstar * SE\n\n[1]  7.449075 11.350925\n\n\n\n\n\nExercise 5.24 Fabric extensibility\nThe Devore7::ex9.23 data set has data on the extensibility (%) of two types of fabric, but it is arranged awkwardly. Let’s fix that:\n\nlibrary(tidyr)\nFabric <- \n  Devore7::ex09.23 |> \n  pivot_longer( everything(), names_to = \"quality\", values_to = \"extensibility\",\n                values_drop_na = TRUE) \nhead(Fabric) \n\n\n\n  \n\n\ndf_stats(extensibility ~ quality, data = Fabric)\n\n\n\n  \n\n\n\n\n\nUse the summary above to compute a 95% confidence interval for the difference in mean extensibility for the two types of fabric.\nThen use t.test() to compute the confidence interval. How do the degrees of freedom from t.test() compare with the upper and lower bounds we have for degrees of freedom? Which end of the range is it closer to? Why?\nCreate normal-quantile plots for each group. Does it seem plausible that that the populations are normal? Is it important that this be plausible?\n\n\n\nExtensibility is related to the shape and drape when fabric is used to make clothing.\n\n\n\n\nSolution. \nlibrary(tidyr)\nFabric <- \n  Devore7::ex09.23 |> \n  pivot_longer( everything(), names_to = \"quality\", values_to = \"extensibility\",\n                values_drop_na = TRUE) \nhead(Fabric)\n\n\n\n  \n\n\ndf_stats(extensibility ~ quality, data = Fabric)\n\n\n\n  \n\n\nt.test(extensibility ~ quality, data = Fabric)\n\n\n    Welch Two Sample t-test\n\ndata:  extensibility by quality\nt = -0.38011, df = 10.482, p-value = 0.7115\nalternative hypothesis: true difference in means between group H and group P is not equal to 0\n95 percent confidence interval:\n -0.5403506  0.3820172\nsample estimates:\nmean in group H mean in group P \n       1.508333        1.587500 \n\n\n\n\n\nExercise 5.25 A difference in proportions\nA sample of parts from two factories was inspected. At factory A, 172 out of 213 were rated “highest quality”. At factory B, 184 out of 207 were rated “highest quality”.\n\n\nCompute the proportion of highest quality parts produced in each sample.\nCompute a 95% confidence interval for the difference in the proportions of parts that receive the highest quality rating from the two factories. Do this first without using prop.test(), then repeat with prop.test() and compare the results.\nBased on this data, can we be confident that one factory is better than the other? Explain.\n\n\n\n\n\n\n\nSolution. \nprop.test( c(172, 184), c(184, 207)) |> confint()\n\n\n\n  \n\n\nprop.test( c(172, 184), c(184, 207), correct = FALSE) |> confint()\n\n\n\n  \n\n\n\n\n\n\n\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554."
  },
  {
    "objectID": "06-propagation.html#error-and-uncertainty",
    "href": "06-propagation.html#error-and-uncertainty",
    "title": "6  Propagation of Uncertainty",
    "section": "6.1 Error and Uncertainty",
    "text": "6.1 Error and Uncertainty\nAlthough many people mistakenly conflate the terms error and uncertainty, these are two different, but related concepts.\nThe word “error” in the context of scientific measurement has a rather different meaning from its use in everyday English. It does not mean blunder or goof (although a blunder or goof could increase the amount of experimental error). Instead, error refers to the unavoidable fact that the measurements scientists record are not exactly correct.\nError is easily defined:\n\nDefinition 6.1 \\[\n\\mbox{error} = \\mbox{estimate} - \\mbox{estimand}\n\\] or, equivalently, \\[\n\\mbox{error} = \\mbox{measurement} - \\mbox{measurand}\n\\]\n\n\nThat is, error is the difference between the number we have measured or calculated and the number that calculation is attempting to estimate. In most applications, we do not know the error exactly, because we do not know the estimand. This is where uncertainty comes in.\nUncertainty is a numerical measure summarizing how large the error might be. There are several different types, or definitions, or uncertainty. The different definitions of uncertainty have in common that they are all trying to describe a statistical distribution of errors.\n  Knowing something about this distribution of the errors can tell us how close our estimates tend to be to the estimand.\nWe will use standard uncertainty unless we say otherwise.\n\nDefinition 6.2 Standard uncertainty is the (estimated) standard deviation of the distribution of errors.\n\n\nYou may wonder how we can know the distribution of errors when we cannot know the error. That is a good question, which we will begin to address via an example."
  },
  {
    "objectID": "06-propagation.html#an-example-estimating-the-number-of-dimes-in-a-sack-of-dimes",
    "href": "06-propagation.html#an-example-estimating-the-number-of-dimes-in-a-sack-of-dimes",
    "title": "6  Propagation of Uncertainty",
    "section": "6.2 An Example: Estimating the number of dimes in a sack of dimes",
    "text": "6.2 An Example: Estimating the number of dimes in a sack of dimes\nSuppose you want to estimate the number of dimes in a large sack of dimes. Here is one method you could use:\n\n\nMeasure the weight of all the dimes in the bag by placing them (without the bag) on an appropriately sized scale. (Call this \\(\\hat B\\), our estimate for \\(B\\), the actual weight of the dimes in the bag.)\nMeasure the weight of 30 individual dimes and use those measurements to estimate the mean weight of dimes. (Call this \\(\\hat D\\).)\nCombine these two estimates to compute an estimated number of dimes in the bag. (\\(\\hat N = \\hat B / \\hat D\\).)\n\n\n\nSuppose that the dimes in our our bag together weigh 10.2 kg and the mean weight of our 30 measured dimes is 2.2582333. Then we would estimate the number of dimes to be \\[\n10200 / 2.2582333 = 4516.8051722 \\;.\n\\] But how good is this estimate? Do we expect to be within a small handful of dimes? Might we be off by 100 or 500? Standard uncertainty provides a way to quantify this. But first, we need to calculate uncertainty for the two ingredients in this recipe: the total weight of all the dimes in the bag (\\(\\hat B\\)), and the mean weight of the 30 measured dimes (\\(\\hat D\\)).\n\n6.2.1 Calculating a Standard Uncertainty without Using Data\nWe only measure the bag of dimes once (and might expect that the value observed on the digital read out would be the same if we measured it repeatedly anyway), so the distribution involved in our uncertainty calculation will be based on some assumptions about the workings of our scale. For example, we could model a reading of 10.2 kg with a \\(\\Unif(10.15, 10.25)\\)-distribution.1\nThis model reflects the assumption that if the actual weight is anywhere between 10.15 and 10.25, the reading will be 10.2 and that the actual weight is equally likely to be anywhere within that range.\nIf we interpret the 10.2 reading in this way, then the uncertainty can be calculated as the standard deviation of a \\(\\Unif(10.15, 10.25)\\)-distribution:\n\\[\nu_{\\hat B} = \\frac{b-a}{\\sqrt{12}}\n= \\frac{10.25-10.15}{\\sqrt{12}}\n= \\frac{0.1}{\\sqrt{12}}\n= 0.0288675 kg\n= 28.8675135 g\n\\]\n\n\n6.2.2 Calculating a Standard Uncertainty Using Data\nThe situation for our estimated mean weight of a dime is a little different. We weighed 30 dimes, and calculated the mean mass of one dime from those data. But if we repeated the measurements many times – taking another 30 dimes, calculating the average…taking another 30 dimes, calculating another average…and so on many times…how much variability would there be in the calculated averages? We will learn how to estimate this quantity ourselves soon; for now, we will take it as a given that it is \\(\\frac{s}{\\sqrt{n}}\\), where \\(s\\) is the standard deviation of one sample (the masses of 30 dimes), and \\(n\\) is the sample size (here, 30). So the uncertainty in our mean dime weight is about:        \n\\[\nu_{\\hat D} = \\frac{s}{\\sqrt{n}}\n= \\frac{0.0220699}{\\sqrt{30}}\n= 0.0040294\\;.\n\\] So we would report our estimate for the mean weight of a dime as \\[\n2.2582333 \\pm 0.0040294 \\;.\n\\]\nThis notation looks like a confidence interval, and indeed it is a confidence interval. Since we are using \\(t_* = 1\\), this is approximately a 68% confidence interval:\n\npt(1, df = 29) - pt(-1, df = 29)\n\n[1] 0.674418\n\n\n\n\n6.2.3 Propagating Error by the Delta Method\nNow that we have computed the uncertainties for \\(B\\) and \\(D\\), we need to find a way to combine them to determine the uncertainty for \\(B/D\\). For this we use a linear approximation of the function \\(f(B,D) = B/D\\).\nRecall from calculus that \\[\nf(x,y) \\approx f(a, b)\n+\n\\frac{\\partial f}{\\partial x} (x-a)\n+\n\\frac{\\partial f}{\\partial y} (y-b) \\;,\n\\] where the partial derivatives are evaluated at \\((x,y) = (a,b)\\).\nIf we apply this to our estimators \\(\\hat B\\) and \\(\\hat D\\), we get \\[\nf(\\hat B,\\hat D) \\approx f( B, D )\n+\n\\frac{\\partial f}{\\partial \\hat B} (\\hat B- B)\n+\n\\frac{\\partial f}{\\partial \\hat D} (\\hat D- D)\n\\;.\n\\]           \nFrom this it follows that \\[\n\\begin{aligned}\n    \\E(f(\\hat B,\\hat D)) &\\approx\n    \\E\\left(f( B, D)\\right)\n+\n\\E\\left( \\frac{\\partial f}{\\partial {\\hat B}} (\\hat B- B) \\right)\n+\n\\E\\left( \\frac{\\partial f}{\\partial \\hat D} (\\hat D- D) \\right)\n\\\\\n&\\approx f(B,D) + 0 + 0\n\\\\\n&= f(B,D) \\;.\n\\end{aligned}\n\\]\n(The 0’s come because our estimators are approximately unbiased: \\(\\E(\\hat B) \\approx B\\) and \\(\\E(\\hat D) \\approx D\\).) This says that \\(\\hat B/ \\hat D\\) is a reasonable estimate for \\(B/D\\) – it is approximately unbiased.2\nBut we really want an expression for the uncertainty – the variance (which we will turn into a standard deviation). We use similar logic to the expectation calculation above, and we will need to use the finding (not proven here) that for constants \\(a\\) and \\(b\\) and random variable \\(X\\), \\(Var(aX+b) = a^2 Var(X)\\). Assuming \\(\\hat B\\) and \\(\\hat D\\) are independent,3 a reasonable assumption in this situation, we get\n\\[\n\\begin{aligned}\n    \\\\[5mm]\n\\Var(f(\\hat B,\\hat D))\n&\\approx\n\\Var\\left(f( B, D)\\right)\n+\n\\Var\\left( \\frac{\\partial f}{\\partial \\hat B} (\\hat B - B) \\right)\n+\n\\Var\\left( \\frac{\\partial f}{\\partial \\hat D} (\\hat D - D) \\right)\n\\\\\n&=\n0\n+\n\\left(\\frac{\\partial f}{\\partial \\hat B}\\right)^2 \\Var(\\hat B)\n+\n\\left(\\frac{\\partial f}{\\partial \\hat D}\\right)^2 \\Var(\\hat D)\n\\end{aligned}\n\\]\nwhere again we evaluate the partial derivatives with \\(\\hat{B}\\) and \\(\\hat{D}\\).\nApplying this to \\(f(\\hat B,\\hat D) = \\hat B/\\hat D\\), we get \\[\n\\begin{aligned}\n    \\frac{\\partial f}{\\partial \\hat B} &= \\frac{1}{\\hat D}\n    \\\\\n    \\frac{\\partial f}{\\partial \\hat D} &= \\frac{-\\hat B}{\\hat D^2}\n\\end{aligned}\n\\] So our uncertainty for the estimated number of dimes (remember, we want the standard deviation, which will be the square root of the variance estimate we just derived) is \\[\n\\sqrt{\n\\frac{1}{2.2582333^2} \\cdot 28.8675135^2\n+ \\left(\\frac{1.02\\times 10^{4}}{2.2582333^2}\\right)^2 \\cdot 0.0040294^2}\n=\n15.1117467 \\;,\n\\] and we would report our estimated number of dimes as \\[\n4517 \\pm 15 \\; \\mbox{dimes} \\;.\n\\]\nThe method described in this example is generally referred to as the Delta Method. Often scientists and engineers who are using this method don’t use the hat notation to distinguish between estimates/estimators and estimands. In the box below, we’ve dropped the hats.\n\n\nThe Delta Method for independent estimates\nLet \\(X\\) and \\(Y\\) be independent estimates with uncertainties \\(u_{X}\\) and \\(u_{Y}\\),\nand let \\(W = f(X,Y)\\). Then the uncertainty in the estimate for \\(W\\) can be estimated as \\[\n    u_{W} \\approx\n    \\sqrt{\n\\left(\\frac{\\partial f}{\\partial X}\\right)^2 u_X^2\n+\n\\left(\\frac{\\partial f}{\\partial Y}\\right)^2 u_Y^2\n    }\n\\] where the partial derivatives are evaluated using estimated values of \\(X\\) and \\(Y\\).\nThe Delta Method can be extended to functions of more (or fewer) than two variables by adding (or removing) terms. Slightly more complicated formulas exist to handle situations where the estimators are not independent (but we will not cover those in this course).\nBecause this method is based on using a linear approximation to \\(f\\), it works better when the linear approximation is better. In particular, when \\(\\frac{\\partial^2 f}{\\partial X^2}\\) or \\(\\frac{\\partial^2 f}{\\partial Y^2}\\) are large near the estimated values of \\(X\\) and \\(Y\\), the approximations might not be very good.\n\n\n\n6.2.4 Estimating Uncertainty via Simulations\nWe can also estimate the uncertainty in the estimated number of dimes using simulations.\n\nB <- runif(10000, 10150, 10250)\nSampleMeans <- do(10000) * mean( ~ mass, data = resample(Dimes) )\nhead(SampleMeans ,3)\n\n\n\n  \n\n\nD <- SampleMeans$mean\nN <- B / D\nhistogram( ~ N)\n\n\n\ngf_qq( ~ N)\n\n\n\nsd(N)\n\n[1] 15.06229\n\n\nA few explanatory notes regarding the computation above.\n\n\nresample() samples from a data frame with replacement. That is, some of the rows may appear more than once, others not at all. Resampling is a common way to estimate a sampling distribution from a single sample. resample() can also be used on a “bare” vector (that is, a vector of data points not contained within a data frame, as exemplified below).\n\n\nresample(1:10)    # some items may be chosen more than once\n\n [1]  7  8  6  1 10  9  8  9  6  1\n\nsample(1:10)      # no item may be chosen more than once, so this just shuffles\n\n [1]  5  9  4  1  6  3  7 10  2  8\n\nshuffle(1:10)     # this also shuffles\n\n [1]  8  9  2  5  4  1  6  7 10  3\n\n\n\nWhen do() doesn’t know what to call the result of what it is “doing”, it calls it result. (Sometimes do() can figure out a better name.)\nThe simulated distribution of \\(N = B/D\\) is unimodal, roughly symmetric, and reasonably well approximated by a normal distribution (but clearly not exactly normal).\nThe Delta Method does not guarantee that the distribution will be approximately normal – it only estimates the variance. Sometimes the distribution will be skewed or have heavier or lighter tails than a normal distribution.\nThe results of the Delta Method and simulation are very close. Each is an acceptable method for approximating the same thing, so this is not surprising."
  },
  {
    "objectID": "06-propagation.html#reporting-measurements-and-estimates",
    "href": "06-propagation.html#reporting-measurements-and-estimates",
    "title": "6  Propagation of Uncertainty",
    "section": "6.3 Reporting Measurements and Estimates",
    "text": "6.3 Reporting Measurements and Estimates\n\n6.3.1 What to record, What to report\nWhen you record the results of a measurement for which there is an a priori estimate of uncertainty, the uncertainty should be recorded along with the measurement itself. Similarly, reports of quantities estimated from data should also include estimated uncertainties.\nAs a general guideline, a properly reported scientific estimated quantity includes the following five elements:\n\n\nA number (the estimate)\nUnits (e.g., m or kg or seconds)\nA statement about how it was measured or calculated\nA statement about most likely sources of (the largest components of) error\nAn estimate of the uncertainty\n\n\n\n\nExample 6.1 If you measured the length of a pendulum using a meter stick, you might report the measurement this way:\n\n\nLength \\(= 0.834 \\pm 0.002\\) m\nMeasured with a meter stick from pivot point to the center of the steel weight.\nUncertainty reflects the limited accuracy of measurement with a meter stick.\n\n\n\n\n\nIn plots, the number is given by the scales of the plot, the units are typically included in the axes labels, uncertainties may be represented by “error bars”, and a statement describing the method of measurement or calculation should appear in the plot legend.\n\n\n6.3.2 How many decimal places?\nNumerical values and their uncertainties should be recorded to the proper number of decimal places. Most software either reports too many significant digits or rounds numbers too much. For correct professional presentation of your data, follow these guidelines:\n\n\nThe experimental uncertainty should be rounded to one significant figure unless the leading digit is a 1, in which case, it is generally better to use two digits.\nA measurement should be displayed to the same number of decimal places as the uncertainty on that measurement.\n\n\n\n\nNote carefully the difference between significant figure and decimal place.\nThe following examples will help:\n\nExample 6.2 The timer reports a value of 0.3451 seconds. The uncertainty on the measurement is 0.0038 seconds. By Rule 1, the uncertainty should be reported to one significant figure, so we round it to 0.004 seconds. By Rule 2, the measurement must also be rounded to the third decimal place. Thus, the measurement should be reported as \\(0.345\\pm0.004\\) seconds.\n\n\n\nExample 6.3 The measured value is \\(7.92538 \\cdot 10^4\\), and its uncertainty is \\(2.3872 \\cdot 10^2\\). By Rule 1, the uncertainty should be rounded to one significant figure, so \\(2 \\cdot 10^2\\). By Rule 2, we report the measurement to the same decimal place as the uncertainty, so \\(7.93 \\cdot 10^4\\). Putting it together, the measurement should be reported as \\((7.93\\pm0.02) 10^4\\).\n\n\n\nExample 6.4 The estimated value is \\(89.231\\), and its uncertainty is \\(0.1472\\). By Rule 1, the uncertainty should be rounded to two significant figures, so \\(0.15\\). By Rule 2, we report the estimate to the same decimal place as the uncertainty, so \\(89.23 \\pm 0.15\\).\n\n\n\n\n\n6.3.3 Reporting numbers in a table\nMultiple similar measurements should be reported in a table. The column headings should clearly and concisely indicate the quantity in each column; the column heading must include the units. Uncertainties should be listed in a separate column, located just to the right of the measurement column. (Sometimes, uncertainties are listed in parentheses after the estimate instead; just make sure the header and legend of the table makes it clear what values are being reported, and where.)\n\nExample 6.5 A lab group calculated these numbers for kinetic energy and its uncertainty:\nKinetic Energy | uncertainty |\n0.8682 | 0.059 |\n1.0661 | 0.071 |\n1.0536 | 0.070 |\n1.3881 | 0.058 |\n0.8782 | 0.108 |\nThis should be reported with appropriate rounding as\n\n\n\nKinetic Energy\nuncertainty\n\n\n\n\n0.87\n0.06\n\n\n1.07\n0.07\n\n\n1.05\n0.07\n\n\n1.39\n0.06\n\n\n0.88\n0.11"
  },
  {
    "objectID": "06-propagation.html#sec-propagation-examples",
    "href": "06-propagation.html#sec-propagation-examples",
    "title": "6  Propagation of Uncertainty",
    "section": "6.4 Additional Propagation of Uncertainty Examples",
    "text": "6.4 Additional Propagation of Uncertainty Examples\n\nQ. The side of a square is measured and reported as \\(12.3 \\pm 0.2\\) mm. How should the area be reported?\nA. Our estimate for the area is \\(12.3^2 = 151.29\\). Our transformation is \\(f(x) = x^2\\), so \\(\\Partial{f}{x} = f'(x) = 2x\\).\nApplying the Delta Method, our uncertainty is\n\\[\n    \\sqrt{ (2 (12.3))^2 (0.2)^2 } = 2 (12.3)(0.2) = 4.92\n\\] and we report the area as \\(151 \\pm 5\\).\nIt is worth looking at the relative uncertainty of the linear and area measurements.\n\\[\n\\begin{aligned}\n        \\frac{0.2}{12.3} &= 0.0162602\n        \\\\\n        \\frac{5}{151}\n        &= 0.0325\n\\end{aligned}\n\\]\nSo the relative uncertainty of the area measurement is twice the relative uncertainty of the linear measurement.\n\n\nThe preceding example demonstrates a simplified version of the Delta Method formula when we are dealing with only one estimator.\n\n%s{\\textsf{\\bfseries The Delta Method for one estimator}}\nLet \\(X\\) be an estimator with uncertainty \\(u_{X}\\) and let \\(\\hat W = f(\\hat X)\\). Then the uncertainty in the estimate \\(W\\) can be estimated as\n\\[\n    u_{W} \\approx\n\\left|\\frac{df}{dX}\\right| u_X\n\\] where the derivative is evaluated using the estimated value of \\(X\\), \\(\\hat{X}\\).\n\n\n\nExample 6.6 Q. The sides of a rectangle are measured and reported as \\(12.3 \\pm 0.2\\) mm and \\(6.3 \\pm 0.1\\) mm. How should the area be reported?\nA. Our estimate for the area is \\(12.3 \\cdot 6.3 = 77.49\\).\nNow we need to estimate the uncertainty. Our transformation is \\(f(x,y) = xy\\), so \\(\\frac{\\partial f}{\\partial x} = y\\) and \\(\\frac{\\partial f}{\\partial y} = x\\).\n\\[\n\\sqrt{ 6.3^2 \\cdot 0.2^2 + 12.3^2 \\cdot 0.1^2 } =\n1.7608237\n\\]\nand we should report the area as\n\\[\n77.5 \\pm\n    1.8\n\\]    \n\n\nSometimes it is more convenient to think about relative uncertainty:\n\nDefinition 6.3 (Relative uncertainty) \\[\n\\mbox{relative uncertainty}\n= \\frac{ \\mbox{uncertainty of measurement} }{\\mbox{magnitude of measurement}}\n\\] For example, if we measure a mass to be \\(10.2\\) g with an uncertainty of \\(0.3\\) g, the relative uncertainty is\n\\[\n\\frac{0.3}{10.2}\n= 0.0294118\n= 2.9 %\n\\] Often it is the case that uncertainty grows with the magnitude of the estimate, and relative uncertainty is a way of comparing the uncertainty in large measured values with the uncertainty of small measured values on a more equal basis. Relative uncertainty is also independent of the units used.\n\n\nIn the example above, we get a nice formula if we compute relative uncertainty instead of absolute uncertainty. Let \\(P = XY\\) where \\(X\\) and \\(Y\\) have uncertainties \\(u_X\\) and \\(u_Y\\). Then \\(\\Partial{P}{X} = Y\\) and \\(\\Partial{P}{Y} = X\\), so\n\\[\n\\begin{aligned}\n\\frac{u_P}{P}\n& = \\sqrt{ \\frac{ Y^2 u_X^2 + X^2 u_Y^2}{P^2} }\n\\\\\n& = \\sqrt{ \\frac{ Y^2 u_X^2 + X^2 u_Y^2}{X^2Y^2} }\n\\\\\n& = \\sqrt{ \\frac{ u_X^2}{X^2} + \\frac{u_Y^2}{Y^2} }\n\\\\\n& = \\sqrt{ \\left(\\frac{ u_X}{X}\\right)^2 + \\left(\\frac{u_Y}{Y}\\right)^2 }\n\\end{aligned}\n\\] which gives a Pythagorean identity for the relative uncertainties.\nThe computations are the same for any product.\nSo this Pythagorean identity for relative uncertainties can be applied to estimate uncertainties for quantities such as area (length \\(\\times\\) width), work (force \\(\\times\\) distance), distance (velocity \\(\\times\\) time), etc.\n\nExample 6.7 Q. Use relative uncertainty to estimate the area of the rectangle in Example 6.6.\nA. The relative uncertainties in the length and width are \\[\n    \\frac{0.2}{12.3} = 0.0162602 \\ \\tand \\  \n    \\frac{0.1}{6.3} = 0.015873\n    \\;.\n\\] So the relative uncertainty in the area estimation is \\[\n    \\sqrt{\n    (0.0162602)^2 +  (0.015873)^2 }\n    =\n    0.0227232\n    \\;.\n\\] Now we solve \\[\n    \\frac{u_A}{77.49} = 0.0227232\n\\] to get \\[\n    u_A = (77.49) (0.0227232) =\n    1.7608237\n    \\;.\n\\] Notice that this matches the result from Example 6.6.\n\n\n\nExample 6.8 Q. When two resistors with resistances \\(R_1\\) and \\(R_2\\) are connected in parallel, the combined resistance satisfies \\[\nR = \\frac{R_1 R_2}{R_1 + R_2}\n\\] Suppose the resistances of the two resistors are reported as \\(20 \\pm 0.7\\) ohms and \\(50 \\pm 1.2\\) ohms. How should you report the combined resistance?\nA. Our estimate is \\(\\hat R = \\frac{ 20 \\cdot 50}{ 20 + 50} = 14.2857143\\). To estimate the uncertainty, we need the partial derivatives \\(\\Partial{R}{R_1}\\) and \\(\\partial{R}{R_2}\\).\n\\[\n\\begin{aligned}\n        \\Partial{R}{R_1} &= \\frac{ (R_1+R_2)R_2 - (R_1 R_2) }{(R_1 + R_2)^2}\n        \\\\\n        &= \\left( \\frac{ R_2}{R_1 + R_2}\\right)^2\n        \\\\\n        &= \\left( \\frac{ 50}{20 + 50}\\right)^2  = 0.5102041\n\\end{aligned}\n\\] Similarly, \\[\n\\begin{aligned}\n\\Partial{R}{R_2}\n&= \\left( \\frac{ R_1}{R_1 + R_2}\\right)^2\n\\\\\n&= \\left( \\frac{ 20}{20 + 50}\\right)^2 = 0.0816327\n\\end{aligned}\n\\]\nSo our estimated uncertainty is given by \\[\n    u_R = \\sqrt{ (0.5102041)^2 (0.7)^2 + (0.0816327)^2)(1.2^2) }\n    = 0.3703337 \\;.\n\\] So we report the combined resistance as\n\\[\n14.3 \\pm 0.4\n\\]"
  },
  {
    "objectID": "06-propagation.html#experimental-error-and-its-causes",
    "href": "06-propagation.html#experimental-error-and-its-causes",
    "title": "6  Propagation of Uncertainty",
    "section": "6.5 Experimental Error and Its Causes",
    "text": "6.5 Experimental Error and Its Causes\nThere are many reasons for experimental error, and it is important to identify potential causes for experimental error, to reduce their effects when possible, and to handle them appropriately in any case.\n\n6.5.1 Random error: Same procedure, different results\nEven if measurements are taken by carefully trained scientists using highly precise instruments, repeated measurements of the “same thing” may not always give the same value.\nAll data collection is done in the context of variability, and statistics allows us to interpret our data in this context.\n\n6.5.1.1 The moving target\nOne reason that a measurement may change is that what we are measuring may be changing. Some quantities depend on environmental factors (like temperature and atmospheric pressure, for example) that may change between measurements. This sort of variability can often be reduced by attempting to control factors that might lead to such variability. For example, a delicate experiment might be conducted in a climate controlled chamber. Another solution is to use a model that includes additional variables for these quantities. Often a combination of these two approaches is used.\nIf measurements are made using similar (but not identical) objects, then differences among those objects may lead to variability in measurements. If measurements are made on a sample of living things, the variability from one individual to the next could be quite large. But even in the physical sciences, each run of an experiment may require the use of different ``consumables” that have slightly different properties that affect our measurements.\n\n\n\n\n\n\n\n\n\n\n\n6.5.1.2 Measurement error\nAnother source of error is the measuring process itself. Every measuring device has its limits, as do the humans who are using them. If you repeatedly timed how long it takes for a steel ball to fall from a fixed height, it is quite likely that you would not get exactly the same result each time. The amount of variability would likely increase if several different students were each asked to measure the time as different students might employ slightly different methods, or be more or less skillful in their measuring.\nThe variability from one measurement to another that would exist even if there were no moving target effect is called measurement error. In practice, it can be difficult or impossible to isolate the moving target effect from measurement error, so they may be combined into one source of variability which we will call random error. In physics, the moving target effect is often small – at least in carefully designed experiments – so that random error is dominated by the difficulties of measuring the quantity under study. In other disciplines, the relative magnitudes may be reversed.\nThe best way to estimate the effects of random error is by making repeated measurements and comparing them. The discrepancies (differences between measurements) provide an indication of the amount of random error.\nSometimes additional information can also be used to help us estimate random error. This can be especially important when our ability to take repeated measurements is limited or when limitations of our measurement apparatus make it impossible to directly observe the effects of random error.\n\n\n6.5.1.3 Invisible measurement error\nAlthough there is no theoretical limit to the precision of a numerical quantity like mass, time, velocity, etc., every measurement device has limited precision. Because of this lack of precision in our measurement device, it may well be that repeated measuremets will all look identical.\nBut this does not mean that they are exactly correct.\nFor example, if we measure temperature using a digital thermometer that has a display showing tenths of a degree C, then any temperature between 57.15 and 57.25 will be displayed as 57.2. Any variability within that range will be invisible to our thermometer. Similarly, if we meaure with a ruler with a 1/8 inch scale, we can use interpolation to get not only to the nearest 1/8 inch, but likely to the nearest 1/16 inch or (with some practice) perhaps to the nearest 1/32 inch. But beyond that, we really cannot tell. If more precise measurements are required, a different measuring tool will need to be used.\nIf other sources of variability are small, this kind of measurement error may completely mask them.\n\n\n\n6.5.2 Systematic error: A tendency to over- or under-estimate\nEven if your target were not moving, and even if there were no variability in measurements from time to time, and even if our measurement apparatus were perfectly precise, there is still a chance that a measurement might not give the value we are searching for because the procedure used might tend to give results that over- or under-estimate the quantity being measured. This kind of error may be referred to as either bias or systematic error.\n\nDefinition 6.4 Bias or Systematic error refers to the tendency to either over- or under-estimate a quantity. It is a tendency to “be off in a particular direction”.\n\n\n\n6.5.2.1 Calibration errors\nPerhaps the easiest type of systematic error to understand is calibration error. If a measuring device is not properly calibrated, the resulting measurements may be too large or too small. For example, if a timing device uses an internal clock that runs a bit slow, it will tend to underestimate times. It might be possible to correct for this bias by performing calibration exercises comparing this timing device to another (more accurate) device. If several similar timing devices systematically disagree, but there is no reference to calibrate with, then we have evidence that at least some of the devices are introducing systematic error, but we may not know which ones or how much.\nCalibrating equipment and procedures by using them to measure or compute standard quantities is an important part of quality scientific experimentation because it helps reduce the effects of systematic bias.\n\n\n6.5.2.2 Design flaws\nPoorly designed experiments can also introduce systematic error. For example, imagine an experiment where a steel ball is dropped from a platform at different heights and the time is recorded until the ball hits the ground.\nTo save time, the researchers set the platform at a specified height and drop the ball multiple times before moving the platform to a new height and repeating. Using this method, any error in measuring the height of the platform will affect all the drops from that height in the same way. This introduces an unknown amount of systematic error into the measurements taken at each height. An alternative design in which the heights are done in random order and in which the platform height is reset before each drop would likely have somewhat more random error but would avoid this source of systematic error.\n\n\n6.5.2.3 Dealing with systematic error\nSystematic error is generally more difficult to handle than random error in part because there is often no good way to measure how large systematic error might be or even to detect that it is occurring.\n\n\n\n6.5.3 Relative Magnitudes of Errors from Different Sources\nWe have identified several potential sources of error.\n  It is good to get in the habit of qualitatively determining which sources you expect to contribute relatively larger and smaller amounts of error. Often we can ignore the sources of relatively smaller potential error and focus our attention on the sources of relatively larger potential error."
  },
  {
    "objectID": "06-propagation.html#uncertainty-how-much-error-might-there-be",
    "href": "06-propagation.html#uncertainty-how-much-error-might-there-be",
    "title": "6  Propagation of Uncertainty",
    "section": "6.6 Uncertainty – How Much Error Might There Be?",
    "text": "6.6 Uncertainty – How Much Error Might There Be?\nIn Section 6.4 we did several examples of propagation of uncertainty. But uncertainty cannot be propagated to derived estimates unless we already have estimated uncertainties for the components of the derivation. Where do these uncertainty estimates come from?\nAs we have mentioned before, it is not possible to measure error directly.\n  If we knew the amount of an error exactly, we would correct for it and obtain the exact, correct estimate of the value we were trying to measure.\nSince we do not know the error exactly, we have to try to use our data (and our prior knowledge about the situation) to try to estimate it. In this section we focus our attention on this part of the uncertainty calculation.\n\n6.6.1 Looking at variability in your data\nIf you have repeated measurements, a histogram, boxplot, or density plot of these measurements can provide a visual representations that shows both what is “typical” or “average” and how much variability there is in the data.\n\n\n\n\n\nFigure 6.1: Three graphical representations of the same 25 measurements\n\n\n\n\n\n\n\nFigure 6.2: Three graphical representations of the same 25 measurements\n\n\n\n\n\n\n\nFigure 6.3: Three graphical representations of the same 25 measurements\n\n\n\n\nIn addition, graphical displays of your data may help isolate outliers – values that don’t seem to fit the pattern of the rest of the data. Outliers should not be removed from your data without furhter investigation. You need to know why these values are different from the rest. Was there a mistake in the measurement? Was the value recorded incorrectly (decimal point in the wrong place, wrong units, transposed digits, typo)? Or was there something different going on – so that the potential outlier is really an important, informative data point that is trying to tell you something about the process you are measuring? An outlier might be the key observation in your data set – don’t throw it away without investigating.\n\n\n6.6.2 Estimating uncertainty from your data\nWhenever possible, you should make multiple measurements of the same phenomenon. The variability among these measurements allows us to estimate the uncertainty. If we are using the mean of multiple measurements as our estimate, then the uncertainty is the “standard error of the mean” (which we will derive in the following chapter):\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\] This is how we estimated the uncertainty in our estimate for the mean weight of a dime.\nIn other more complicated study designs, some other standard error formula may be used.\n\n\n6.6.3 Estimating uncertainty in other ways {sec-uncertainty-without-data}\nEstimating uncertainty from data is only possible if\n\n\nYou have multiple measurements from which to estimate the variability, and\nThe measurements actually vary (aren’t masked by imprecise measuring devices, for example).\n\n\n\nFrequently, these conditions are not met, but we still want to quantify the uncertainty.\n\n\n6.6.3.1 The uniform model for measurement (im)precision\nThere are many measuring devises that provide limited precision so that the best we may be able to say is that we know the measured value to be in some interval \\([a,b]\\). This would be the case for measuring devices with digital displays, for example.\nImagine a device that displays 8.03 on its digital display.\nPresumably, this means that the actual measurement can lie anywhere between 8.025 and 8.035. This value has been rounded for digital display, and we have no way of knowing where within that interval the value might be.\nWe don’t want to report the uncertainty as \\(b-a\\) or even \\((b-a)/2\\). These would be overestimates of the “average” amount of error. Here, we aren’t looking for an upper bound on the potential error; we want to estimate something like the average amount of error.\nIn this case we can estiamte a standard uncertainty based on the uniform distribution.\nRecall, a uniform distribution is one in which every value in some interval is “equally likely”. A uniform distribution has standard deviation \\(s = \\frac{b-a}{\\sqrt{12}}\\), so we will use this as our estimate for standard uncertainty as well. Notice that \\(\\frac{2}{\\sqrt{12}} = \\frac{1}{\\sqrt{3}} = 0.5773503\\). This means that adding and subtracting one standard uncertainty from the center of the interval \\[\n\\frac{a+b}{2} \\pm \\frac{b-a}{\\sqrt{12}}\n\\] will cover about 58% of the interval. This is quite a bit less than the 68% covered by the central portion of a normal distribution (within 1 standard deviation of the mean). For a “back of the envelope” computation, to use an uncertainty with a similar amount of “coverage” to the “coverage” that the standard deviation has for the normal distribution, we might choose to use the approximation \\(\\frac{b-a}{\\sqrt{12}} \\approx \\frac{b-a}{3}\\), since this will slightly over-estimate the uncertainty and lead to a central covering probability of \\(2/3 \\approx 68\\)%.\nThis same idea can be used when working with analog scales, but in this case, we typically can see that the value is closer to one end than the other or closer to the center than to the edges. This reduces our uncertainty by a factor of 2. For example, given a ruler marked in mm, if we can tell than a reading is closer to 12.3 mm than it is to 12.4 mm, then we are saying we know the value is in the interval \\([12.3, 12.35]\\), which is only half as wide as the scale of the ruler. We can then use a uniform distribution as above, except that the limits \\(a\\) and \\(b\\) of the distribution are a bit narrower now.\n(On some analog scales, it may be possible to do even better than this.)\n\nExample 6.9 Measuring length on a ruler with a 1 mm scale, we could use \\(\\frac{1/2}{\\sqrt{12}}\\) as the estimated uncertainty of the measurement if the primary source of error is reading the scale. Of course, it may be that lining up the scale with the object being measured introduces more uncertainty than reading the scale does. That may lead us to choose a larger value for our estimated uncertainty.\n\n\n\n\n6.6.3.2 Other models for measurement (im)precision\nOther distributions, most notably the triangle and normal distributions, are sometimes used instead of the uniform distribution to model errors in measurements made with various devices. Each of these models produces a somewhat smaller estimate for the uncertainty than you would get using a uniform distribution. In the case of the normal distribution, typically one considers half the width of the interval to be spanned by 3 standard deviations of the normal distribution (since that would capture 99.7% of the distribution. If \\(a\\) and \\(b\\) are the lower and upper limits of plausible values corresponding to a measurement, the three uncertainty calculations are as follows:\n\n\nTable 6.1: Uncertainties based on 3 different distributions.\n\n\ndistribution\nuncertainty\n\n\n\n\nuniform\n\\(\\displaystyle \\frac{b-a}{2 \\sqrt{3}}\\)\n\n\ntriangle\n\\(\\displaystyle \\frac{b-a}{2 \\sqrt{6}}\\)\n\n\nnormal\n\\(\\displaystyle \\frac{b-a}{2 \\cdot 3}\\)\n\n\n\n\nThis makes the uniform distribution the most conservative and the normal distribution the least conservative.\n\n\n\n\n6.6.3.3 Taking advantage of other data\nSometimes previous experience with a device or protocol may provide us with a good estimate for the uncertainty even before we collect our data. In such cases, we can use these a priori uncertainties both in planning and in analysis, but it is also good to check that the data collected are consistent with the estimated uncertainties."
  },
  {
    "objectID": "06-propagation.html#exercises",
    "href": "06-propagation.html#exercises",
    "title": "6  Propagation of Uncertainty",
    "section": "6.7 Exercises",
    "text": "6.7 Exercises\n\nExercise 6.1 A clinical trial with 30 patients has been performed in which the volume of distribution (the theoretical volume that would be necessary to contain the total amount of an administered drug at the same concentration that it is observed in the blood plasma) of a new anti-diabetes drug was measured for each patient. The sample mean was 10.2 L with a standard deviation of 1.9 L.\n\n\nCalculate a 95% confidence interval for the the mean volume of distribution.\nCalculate a 99% confidence interval for the the mean volume of distribution.\nCalculate the standard uncertainty for the mean volume of distribution.\n\n\n\n\n\n\n\nSolution. \n# 95 <!--  CI -->\nt.star <- qt(.975, df = 29); t.star\n\n[1] 2.04523\n\nME <- t.star *  1.9 / sqrt(30); ME\n\n[1] 0.7094717\n\n1.2 + c(-1,1) * ME\n\n[1] 0.4905283 1.9094717\n\n# 99 <!--  CI -->\nt.star <- qt(.995, df = 29); t.star\n\n[1] 2.756386\n\nME <- t.star *  1.9 / sqrt(30); ME\n\n[1] 0.9561653\n\n10.2 + c(-1,1) * ME\n\n[1]  9.243835 11.156165\n\n# standard uncertainty = SE = s / sqrt(n)\n1.9 / sqrt(30)\n\n[1] 0.346891\n\n\n\n\n\nExercise 6.2 A handbook gives the value of the coefficient of linear thermal expansion of pure copper at 20 degrees C, \\(\\alpha_{20}\\)(Cu), as \\(16.52 \\times 10^{-6} \\ {}^\\circ C^{-1}\\) and simply states that “the error in this value should not exceed \\(0.40 \\times 10^{-6} \\ {}^{\\circ} C^{-1}\\).”\n\n\nBased on this limited information, and assuming a rectangular distribution, compute the standard uncertainty.\nBased on this limited information, and assuming a triangular distribution, compute the standard uncertainty.\nWhy might one prefer one of these over the other?\n\n\n\n\n\n\n\nSolution. \n# rectangualr: sd = width / sqrt(12) = width / (2 * sqrt(3))\n0.4 * 2 / sqrt(12)\n\n[1] 0.2309401\n\n0.4 / sqrt(3)\n\n[1] 0.2309401\n\n# triangular: sd = width / (2 * sqrt(6))\n0.4 * 2 / (2 * sqrt(6))\n\n[1] 0.1632993\n\n0.4 / sqrt(6)\n\n[1] 0.1632993\n\n\nThe uniform distribution is a more conservative assumption. The trianlge distribution should only be used in situations where errors are more likely to be smaller than larger (with the the specified bounds).\n\n\n\nExercise 6.3 The following data are given in in the certificate of a standard solution: C(HCl) = (\\(0.10000 \\pm 0.00010\\)) mol/l. No additional information is given on the type of the uncertainty. (The \\(\\pm\\) part here is not the uncertainty but is supposed to indicate upper and lower bounds on the error.)\n\n\nConvert the uncertainty to standard uncertainty assuming a rectangular distribution.\nConvert the uncertainty to standard uncertainty assuming a triangular distribution.\n\n\n\n\n\n\n\nSolution. \n# rectangular\n0.0001 / sqrt(3)\n\n[1] 5.773503e-05\n\n# trianglular\n0.0001 / sqrt(6)\n\n[1] 4.082483e-05\n\n\n\n\n\nExercise 6.4 The area of a circle is to be calculated from a measured radius. The measurement and its standard uncertainty are reported as \\[\n12.5 \\pm 0.3 \\mbox{m} \\;.\n\\] What should the researchers report as the area?\n\n\n\nSolution. Let \\(f(R) = \\pi R^2\\). Then \\(\\Partial{f}{R} = 2\\pi R\\), so the uncertainty in the area estimation is \\[\n\\sqrt{ (2 \\pi R)^2 (0.3)^2 } = 2 \\pi R \\cdot 0.3  = 23.5619449\n\\mathrm{m}^2\n\\;.\n\\] We can report the area as \\(490 \\pm 20\\).\n\n\n\nExercise 6.5  \nBelow are some computer-computed estimates and uncertainties.  They are far too precise (there are way too\nmany digits reported).  Use standard practice to report each estimate with the correct number of digits.\n\n\n\nestimate\nuncertainty\n\n\n\n\n5.43210\n0.024135\n\n\n1535.68\n12.7342\n\n\n576.3415\n3.453567\n\n\n0.00148932\n0.0000278\n\n\n\n\n\n\nSolution. \n\n\n\nestimate\nuncertainty\n\n\n\n\n5.43\n0.02\n\n\n1536\n13\n\n\n576\n3\n\n\n0.00149\n0.00003\n\n\n\n\n\n\nExercise 6.6  \nA student is calculating the volume of a rectangular tank by measuring \nthe length, width, and height.  These measurements are recorded as \n $L = 2.65 \\pm 0.02$cm, $W = 3.10 \\pm 0.02$cm, and $H = 4.61\\pm 0.05$ cm.\n\n How should the volume be reported?\n\n\n\nSolution. \nIn a product the relative uncertainties add, so\n\nL <- 2.65; W <- 3.10; H <- 4.61\nuL <- 0.02; uW <- 0.02; uH <- 0.05\nV <- L * W * H; V\n\n[1] 37.87115\n\nuV <- sqrt( (uL/L)^2 + (uW/W)^2 + (uH/H)^2) * V; uV\n\n[1] 0.5568714\n\n\nSo we report $37.9 \\pm 0.6$\n\n\n\nExercise 6.7 Estimate (with uncertainty) the amount of gasoline burned by personal cars in a particular year in the US from the following estimates and uncertainties for that year:\n<!--  note: estimates found online for 2011.  uncertainties made  -->\n<!--  up but roughly based on the number of digits reported in online sources. -->\n\n\n\n\n\n\n\n\nquantity\nestimate\nuncertainty\n\n\n\n\ncars per person\n0.80\n0.12\n\n\npopulation (millions of people)\n311.6\n.2\n\n\nfleet fuel efficiency (mpg)\n23.7\n1.7\n\n\naverage distance driven per vehicle (miles)\n12,000\n2,000\n\n\n\nNote: Various estimates for these quantities are available online, but most do not report uncertainty. The uncertainties here reflect the number of significant figures used to report these numbers and the variability between estimates found at different web sites. Also, the methodology for determining these values is not always clear. So treat this as an exercise in propagation of error, but understand that better estimates of fuel consumption (and uncertainty) would be possible with better data.\n\n\n\nSolution. The total fuel consumed is given by \\[\nF = \\frac{kPd}{E}\n\\] where \\(k\\) is the cars per person, \\(P\\) is total population, \\(d\\) is average distance driven per car, and \\(E\\) is the average efficiency.\n\\[\n\\begin{aligned}\n\\Partial{F}{k} &= \\frac{Pd}{E}\n\\\\\n\\Partial{F}{P} &= \\frac{kd}{E}\n\\\\\n\\Partial{F}{d} &= \\frac{kP}{E}\n\\\\\n\\Partial{F}{E} &= \\frac{-kPd}{E^2}\n\\end{aligned}\n\\]\nSo the estimated amount of fuel consumed (in millions of gallons) is \\[\n\\frac{kPd}{E} = \\frac{0.8 \\cdot 311.6 \\cdot 12000 }{23.7}\n= 1.2621772\\times 10^{5}\n\\] and the uncertainty can be computed as follows:\n\nk <- 0.8; u_k <- 0.12\nP <- 311.6; u_P <- 0.2\nd <- 12000; u_d <- 2000\nE <- 23.7; u_E <- 1.7\nvariances <- c(\n            (P*d/E)^2 * u_k^2 , \n            (k*d/E)^2 * u_P^2 , \n            (k*P/E)^2 * u_d^2 , \n            (k * P * d/E^2)^2 * u_E^2\n)\nvariances\n\n[1] 3.584455e+08 6.563051e+03 4.425254e+08 8.196753e+07\n\nu_F <- \n  sqrt( \n            (P*d/E)^2 * u_k^2 + \n            (k*d/E)^2 * u_P^2 + \n            (k*P/E)^2 * u_d^2 + \n    (k * P * d/E^2)^2 * u_E^2\n    )\nu_F\n\n[1] 29714.39\n\nsqrt(sum(variances))\n\n[1] 29714.39\n\n\nSo we might report fuel consumption as 1.3^{5} \\(\\pm\\) 3^{4} millions of gallons or 130 \\(\\pm\\) 30 billions of gallons.\nNote: This problem can also be done in stages. First we estimate the number of (millions of) cars using \\[\nC = k P \\;.\n\\]\n\nC <- k * P; C\n\n[1] 249.28\n\nu_C <- sqrt( P^2 * u_k^2 + k^2 * u_P^2); u_C\n\n[1] 37.39234\n\n\nThe number of (millions of) miles driven is estimated by \\(M = C d\\):\n\nM <- C * d; M\n\n[1] 2991360\n\nu_M <- sqrt( d^2 * u_C^2 + C^2 * u_d^2); u_M\n\n[1] 670746.6\n\n\nFinally, the fuel used is \\(F = M / E\\).\n\nF = M / E\nu_F = sqrt( (1/E)^2 * u_M^2 + (M/E^2)^2 * u_E^2); u_F\n\n[1] 29714.39\n\n\nThis gives the same result (again in millions of gallons of gasoline).\nIt would also be possible to relative uncertainty to do this problem, since we have nice formulas for the relative uncertainty in products and quotients.\n\n\n\nExercise 6.8 A physics student is calculating the speed of a falling object by measuring the time it takes for the object to move between two timing sensors. If she records the time as \\(0.43 \\pm 0.02\\) seconds and the distance as \\(1.637 \\pm 0.006\\) m, how should she report the speed in \\(m/s\\)?\n\n\n\nSolution. \nSee the next problem for the answer.\n\n\n\nExercise 6.9  \n\n\nWork out a formula for the relative uncertainty of \\(Q = X/Y\\) given relative uncertainties for \\(X\\) and \\(Y\\).\nRedo Exercise 6.8 using your new formula.\n\n\n\n\n\n\nSolution. \nLet $Q = X Y^{-1}$.  So \n$\\Partial{Q}{X} = Y^{-1}$ and \n$\\Partial{Q}{Y} = -X Y^{-2}$.  From this we get\n\\[\n\\begin{aligned}\n\\frac{u_Q}{Q}\n& = \\sqrt{ \\frac{ Y^{-2} u_X^2 + X^2 Y^{-4} u_Y^2}{Q^2} }\n\\\\\n& = \\sqrt{ \\frac{ Y^{-2} u_X^2 + X^2 Y^{-4} u_Y^2}{X^2Y^{-2}} }\n\\\\\n& = \\sqrt{ \\frac{ u_X^2}{X^2} + \\frac{u_Y^2}{Y^2} }\n\\\\\n& = \\sqrt{ \\left(\\frac{ u_X}{X}\\right)^2 + \\left(\\frac{u_Y}{Y}\\right)^2 }\n\\end{aligned}\n\\]\nwhich gives a Pythagorean identity for the relative uncertainties just as it did for a product.\n\nV <- 1.637 / 0.43; V\n\n[1] 3.806977\n\nuV <- sqrt( (0.02/.43)^2 + (0.006/1.637)^2 ) * V; uV\n\n[1] 0.1776176\n\n\nSo we report \\(3.81 \\pm 0.18\\)."
  },
  {
    "objectID": "07-simple-linear.html#the-simple-linear-regression-model",
    "href": "07-simple-linear.html#the-simple-linear-regression-model",
    "title": "7  Linear Models",
    "section": "7.1 The Simple Linear Regression Model",
    "text": "7.1 The Simple Linear Regression Model\n\\[\nY = \\beta_0 + \\beta_1 x + \\varepsilon  \\qquad \\mbox{where $\\varepsilon \\sim \\Norm(0,\\sigma)$.}\n\\]\nIn other words:\n\n\nThe mean response for a given predictor value \\(x\\) is given by a linear formula \\[\n\\mbox{mean response} = \\beta_0 + \\beta_1 x\n\\]\nThis can also be written as \\[\n\\E(Y \\mid X = x) = \\beta_0 + \\beta_1 x\n\\]\nThe distribution of all responses for a given predictor value \\(x\\) is normal.\nThe standard deviation of the responses is the same for each predictor value.\n\n\n\nFurthermore, in this model the values of \\(\\varepsilon\\) are independent.\nThere are many different things we might want to do with a linear model, for example:\n\n\nEstimate the coefficients \\(\\beta_0\\) and \\(\\beta_1\\).\nEstimate the value \\(Y\\) associated with a particular value of \\(x\\).\nSay something about how well a line fits the data."
  },
  {
    "objectID": "07-simple-linear.html#fitting-the-simple-linear-model",
    "href": "07-simple-linear.html#fitting-the-simple-linear-model",
    "title": "7  Linear Models",
    "section": "7.2 Fitting the Simple Linear Model",
    "text": "7.2 Fitting the Simple Linear Model\n\n7.2.1 The Least Squares Method\nWe want to determine the best fitting line to the data. The usual method is the method of least squares1, which chooses the line that has the smallest possible sum of squares of residuals, where residuals are defined by\n\\[\n\\mbox{residual} = \\mbox{observed} - \\mbox{predicted}\n\\]\n\nConsider the following small data set.\n\n\n\nSomeData <- tibble(\n  x = c(1,2,4,5,6),\n  y = c(1,4,3,6,5)\n)\nSomeData\n\n\n\n  \n\n\n\n\n\ngf_point( y ~ x, data = SomeData) |>\n  gf_lims(y = c(0, 7), x = c(0, 7))\n\n\n\n\n\n\n\n\nAdd a line to the plot that “fits the data well”. Don’t do any calculations, just add the line.\nNow estimate the residuals for each point relative to your line\nCompute the sum of the squared residuals, \\(SSE\\).\nEstimate the slope and intercept of your line.\n\n\n\n\nFor example, suppose we we select a line that passes through \\((1,2)\\) and \\((6,6)\\). the equation for this line is \\(y = 1.2 + 0.8 x\\), and it looks like a pretty good fit:\n\nf <- makeFun( 1.2 + 0.8 * x ~ x)\ngf_point(y ~ x, data = SomeData) |> \n  gf_lims(x = c(0, 7), y = c(0, 7)) |>\n  gf_fun( f(x) ~ x, col = \"gray50\" )\n\n\n\n\nThe residuals for this function are\n\nresids <- with(SomeData, y - f(x)) ; resids \n\n[1] -1.0  1.2 -1.4  0.8 -1.0\n\n\nand \\(SSE\\) is\n\nsum(resids^2)\n\n[1] 6.04\n\n\nThe following plot provides a way to visualize the sum of the squared residuals (SSE).\n\n\n\n\n\nIf your line is a good fit, then \\(SSE\\) will be small.\nThe best fitting line will have the smallest possible \\(SSE\\).\nThe lm() function will find this best fitting line for us.\n\nmodel1 <- lm( y ~ x, data = SomeData ); model1\n\n\nCall:\nlm(formula = y ~ x, data = SomeData)\n\nCoefficients:\n(Intercept)            x  \n     1.1628       0.7326  \n\n\nThis says that the equation of the best fit line is \\[\n\\hat y = 1.1627907 + 0.7325581 x\n\\]\n\ngf_point(y ~ x, data = SomeData) |>\n  gf_lm()\n\nWarning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\nWe can compute \\(SSE\\) using the resid() function.\n\nSSE <- sum (resid(model1)^2); SSE\n\n[1] 5.569767\n\n\n\n\n\n\n\nAs we see, this is a better fit than our first attempt – at least according to the least squares criterion. It will better than any other attempt – it is the least squares regression line.\n\n\nFor a line with equation \\(y = \\hat\\beta_0 + \\hat\\beta_1 x\\), the residuals are \\[\ne_i = y_i - (\\hat\\beta_0 + \\hat\\beta_1 x)\n\\] and the sum of the squares of the residuals is \\[\nSSE = \\sum e_i^2  = \\sum (y_i - (\\hat\\beta_0 + \\hat\\beta_1 x) )^2\n\\] Simple calculus (which we won’t do here) allows us to compute the best \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) possible.\nThese best values define the least squares regression line. We always compute these values using software, but it is good to note that the least squares line satisfies two very nice properties.\n\n\nThe point \\((\\mean x, \\mean y)\\) is on the line.\n\nThis means that \\(\\mean y = \\hat\\beta_0 + \\hat\\beta_1 \\mean x\\) (and \\(\\hat\\beta_0 = \\mean y - \\hat\\beta_1 \\mean x\\)) #. The slope of the line is \\(\\displaystyle b = r \\frac{s_y}{s_x}\\) where \\(r\\) is the correlation coefficient:  \\[\nr = \\frac{1}{n-1} \\sum \\frac{ x_i - \\mean x }{s_x} \\cdot \\frac{ y_i - \\mean y }{s_y}\n\\]\n\n\nSince we have a point and the slope, it is easy to compute the equation for the line if we know \\(\\mean x\\), \\(s_x\\), \\(\\mean y\\), \\(s_y\\), and \\(r\\).\n\n\n7.2.2 An Example: Estimating OSA\n\nIn a study of eye strain caused by visual display terminals, researchers wanted to be able to estimate ocular surface area (OSA) from palpebral fissure (the horizontal width of the eye opening in cm) because palpebral fissue is easier to measure than OSA.\n\nEyes <- \n  read.table(\n    \"https://rpruim.github.io/Engineering-Statistics/data/PalpebralFissure.txt\", \n    header = TRUE)\nhead(Eyes, 3) \n\n\n\n  \n\n\nx.bar <- mean( ~ palpebral, data = Eyes)  \ny.bar <- mean( ~ OSA, data = Eyes)\ns_x <- sd( ~ palpebral, data = Eyes)\ns_y <- sd( ~ OSA, data = Eyes)\nr <- cor( palpebral ~ OSA, data = Eyes)\nc( x.bar = x.bar, y.bar = y.bar, s_x = s_x, s_y = s_y, r = r )\n\n    x.bar     y.bar       s_x       s_y         r \n1.0513333 2.8403333 0.3798160 1.2083374 0.9681245 \n\nslope <- r * s_y / s_x\nintercept <- y.bar - slope * x.bar\nc(intercept = intercept, slope = slope)\n\n intercept      slope \n-0.3977389  3.0799672 \n\n\n\n\nFortunately, statistical software packages do all this work for us, so the calculations of the preceding example don’t need to be done in practice.\n\nIn a study of eye strain caused by visual display terminals, researchers wanted to be able to estimate ocular surface area (OSA) from palpebral fissure (the horizontal width of the eye opening in cm) because palpebral fissue is easier to measure than OSA.\n\nosa.model <- lm(OSA ~ palpebral, data = Eyes) \nosa.model\n\n\nCall:\nlm(formula = OSA ~ palpebral, data = Eyes)\n\nCoefficients:\n(Intercept)    palpebral  \n    -0.3977       3.0800  \n\n\nlm() stands for linear model. The default output includes the estimates of the coefficients (\\(\\hat\\beta_0\\) and \\(\\hat \\beta_1\\)) based on the data. If that is the only information we want, then we can use\n\ncoef(osa.model)\n\n(Intercept)   palpebral \n -0.3977389   3.0799672 \n\n\nThis means that the equation of the least squares regression line is \\[\n\\hat y = -0.398 + 3.08 x\n\\]\nWe use \\(\\hat y\\) to indicate that this is not an observed value of the response variable but an estimated value (based on the linear equation given).\nR can add a regression line to our scatter plot if we ask it to.\n\n\ngf_point( OSA ~ palpebral, data = Eyes) |> \n  gf_lm()\n\n\n\n\n\n\nWe see that the line does run roughly “through the middle” of the data but that there is some variability above and below the line.\n\n\n\n\n7.2.3 Explanatory and Response Variables Matter\nIt is important that the explanatory variable be the “x” variable and the response variable be the “y” variable when doing regression. If we reverse the roles of OSA and palpebral we do not get the same model. This is because the residuals are measured vertically (in the \\(y\\) direction)."
  },
  {
    "objectID": "07-simple-linear.html#estimating-the-response",
    "href": "07-simple-linear.html#estimating-the-response",
    "title": "7  Linear Models",
    "section": "7.3 Estimating the Response",
    "text": "7.3 Estimating the Response\nWe can use our least squares regression line to estimate the value of the response variable from the value of the explanatory variable.\n\nIf the palpebral width is 1.2 cm, then we would estimate OSA to be\n\\[\n\\hat{`osa`} =\n-0.398 + 3.08 \\cdot 1.2\n= 3.298\n\\]\nR can automate this for us too. The makeFun() function will create a function from our model.\nIf we input a palpebral measurement into this function, the function will return the estimated OSA.\n\nestimated.osa <- makeFun(osa.model)\nestimated.osa(1.2)\n\n       1 \n3.298222 \n\n\nAs it turns out, the 17th measurement in our data set had a palpebral measurement of 1.2 cm.\n\nEyes[17,]\n\n\n\n  \n\n\n\nThe corresponding OSA of 3.76 means that the residual for this observation is \\[\n\\mbox{observed} - \\mbox{predicted} = 3.76 - 3.2982218 =\n0.4617782  \n\\]\n\n\n\n7.3.1 Cautionary Note: Don’t Extrapolate!\nWhile it often makes sense to generate model predictions corresponding to x-values within the range of values measured in the dataset, it is dangerous to extrapolate and make predictions for values outside the range included in the dataset. To assume that the linear relationship observed in the dataset holds for explanatory variable values outside the observed range, we would need a convincing, valid justification, which is usually not available. If we extrapolate anyway, we risk generating erroneous or even nonsense predictions. The problem generally gets worse as we stray further from the observed range of explanatory-variable values."
  },
  {
    "objectID": "07-simple-linear.html#parameter-estimates",
    "href": "07-simple-linear.html#parameter-estimates",
    "title": "7  Linear Models",
    "section": "7.4 Parameter Estimates",
    "text": "7.4 Parameter Estimates\n\n7.4.1 Interpreting the Coefficients\nThe coefficients of the linear model tell us how to construct the linear function that we use to estimate response values, but they can be interesting in their own right as well.\nThe intercept \\(\\beta_0\\) is the mean response value when the explanatory variable is 0. This may or may not be interesting. Often \\(\\beta_0\\) is not interesting because we are not interested in the value of the response variable when the predictor is 0. (That might not even be a possible value for the predictor.) Furthermore, if we do not collect data with values of the explanatory variable near 0, then we will be extrapolating from our data when we talk about the intercept. (Extrapolating is dangerous because we can’t really be sure that the relationships we’ve uncovered with our model really hold for variable values outside the range we measured.)\nThe estimate for \\(\\beta_1\\), on the other hand, is nearly always of interest. The slope coefficient \\(\\beta_1\\) tells us how quickly the response variable changes per unit change in the predictor. This is an interesting value in many more situations. Furthermore, when \\(\\beta_1 = 0\\), then our model does not depend on the predictor at all. So if we construct a confidence interval for \\(\\beta_1\\), and it contains 0, then we do not have sufficient evidence to be convinced that our predictor is of any use in predicting the response.\n\n\n7.4.2 Estimating \\(\\sigma\\)\nThere is one more parameter in our model that we have been mostly ignoring so far: \\(\\sigma\\) (or equivalently \\(\\sigma^2\\)). This is the parameter that describes how tightly things should cluster around the regression line. We can estimate \\(\\sigma^2\\) from our residuals:\n\\[\n\\begin{aligned}\n\\hat\\sigma^2 & = MSE = \\frac{ \\sum_i e_i^2 }{ n -2 }\n\\\\\n\\hat\\sigma & = RMSE = \\sqrt{MSE} = \\sqrt{\\frac{ \\sum_i e_i^2 }{ n -2 } }\n\\end{aligned}\n\\]\nThe acronyms \\(MSE\\) and \\(RMSE\\) stand for Mean Squared Error and Root Mean Squared Error. The numerator in these expressions is the sum of the squares of the residuals \\[\nSSE = \\sum_i e_i^2 \\;.\n\\] This is precisely the quantity that we were minimizing to get our least squares fit. \\[\nMSE = \\frac{SSE}{DFE}\n\\] where \\(DFE = n-2\\) is the degrees of freedom associated with the estimation of \\(\\sigma^2\\) in a simple linear model. We lose two degrees of freedom when we estimate \\(\\beta_0\\) and \\(\\beta_1\\), just like we lost 1 degree of freedom when we had to estimate \\(\\mu\\) in order to compute a sample variance.\n\\(RMSE = \\sqrt{MSE}\\) is listed in the summary output for the linear model as the residual standard error because it is the estimated standard deviation of the error terms in the model.\n\nsummary(osa.model)\n\n\nCall:\nlm(formula = OSA ~ palpebral, data = Eyes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60942 -0.19875 -0.01902  0.21727  0.66378 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.3977     0.1680  -2.367   0.0251 *  \npalpebral     3.0800     0.1506  20.453   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.308 on 28 degrees of freedom\nMultiple R-squared:  0.9373,    Adjusted R-squared:  0.935 \nF-statistic: 418.3 on 1 and 28 DF,  p-value: < 2.2e-16\n\n\nWe will learn about other parts of this summary output shortly. Much is known about the estimator \\(\\sigma^2\\), including\n\n\n\\(\\hat \\sigma^2\\) is unbiased (on average it is \\(\\sigma^2\\)), and\nthe sampling distribution is related to a Chi-Squared distribution with \\(n-2\\) degrees of freedom.\n(Chi-Squared distributions are a special case of Gamma distributions.) More specifically, \\[\n     \\frac{SSE}{\\sigma^2} = \\frac{(n - 2) \\hat \\sigma^2}{\\sigma^2} \\sim \\Chisq(n-2) \\;.\n\\]"
  },
  {
    "objectID": "07-simple-linear.html#checking-assumptions",
    "href": "07-simple-linear.html#checking-assumptions",
    "title": "7  Linear Models",
    "section": "7.5 Checking Assumptions",
    "text": "7.5 Checking Assumptions\n\n7.5.1 What have we assumed?\nIn fitting a linear regression model, we have assumed:\n\n\nA linear relationship between the explanatory and response variables\nThe errors (\\(\\epsilon\\)) are Normally distributed\nIndependence of the errors (in particular, no correlation over time between successive errors for data points collected over time)\nHomoscedasticity of the errors – this means that the variance (spread) of the errors is constant over time, and over the full range of explanatory and predictor variables\n\n\n\n\n\n7.5.2 Don’t Fit a Line If a Line Doesn’t Fit\nThe least squares method can be used to fit a line to any data – even if a line is not a useful representation of the relationship between the variables. When doing regression we should always look at the data to see if a line is a good fit. If it is not, then the simple linear model is not a good choice and we should look for some other model that does a better job of describing the relationship between our two variables.\n\n\n7.5.3 Checking the Residuals\nWe look at the residuals (not just the data scatter plot) because some of our assumptions refer specifically to them. Also, often, it is easier to assess the linear fit by looking at a plot of the residuals than by looking at the natural scatter plot, because on the scale of the residuals, violations of our assumptions are easier to see.\nSo, to verify that our linear regression assumptions are sensible, we can examine the model residuals. Residuals should be checked to see that their distribution looks approximately normal and that thier standard deviation (the spread of the residuals) remains consistent across the range of our data (and across time).\nIn addition, especially if the data were collected over time (measurements made in order during an experiment; data points collected at a series of time points), it is important to verify that the residuals are independent of one another over time. To look for this problem, we can look at a scatter plot of the residuals as a function of time, and suspect a problem if we see series of very large, or very small, residuals all in a row. Another plot that can help us look for non-independence in the residuals is a plot of the autocorrelation function (ACF), obtained using the acf() function in R. This function computes and plots the correlation coefficient R for the residuals at various “lags”. For example, the correlation coefficient for lag 1 is the correlation coefficient between each residual (corresponding to the ith datapoint) and the preceding one (the i-1th data point). Lag 2 is between the ith and i-2th data point, and so on. If the residuals are not independent, then these coefficients will have large absolute values. (Note: the “lag 0” coefficient measures the correlation of the ith residual with itself, so it is always 1. This does NOT indicate any problem with the linear regression model.)\nIn general, we might want to check the following plots of the residuals:\n\n\nResiduals as a function of “fitted values”, or model predictions for the x-values obseved in the actual data set.\nResiduals as a function of the observed values of the explanatory variable (from the actual data set).\nNormal quantile-quantile plot of the residuals (note: in Chapter 4, we made these by hand for any distribution; you may also use the shortcut function qqmath() as illustrated below, to make them for the Normal distribution.)\nResiduals as a function of time (if you know the order in which they were collected, or if the explanatory variable is a time-related one).\nResidual autocorrelation function plot (if the data points were collected over time, or if the explanatory variable is a time-related one).\n\n\n\nFor all the scatter plots, we want to make sure the residuals “look random” – the extent of the spread of the residuals should not vary with time or x or y (``trumpet”-shaped plot). If there is a pattern, it suggests a problem with the homoscedasticity assumption. There should not be long runs of similar residuals, especially over time; if there are, it suggests non-independence of the residuals. There should also be no apparent trends in the plot, linear or non-linear; if there are, it suggests that the relationship between the predictor and response variables was not linear. In an autocorrelation plot, the correlation coefficients (except for lag 0) should not be too large, far exceeding the dotted guide-lines on the plot; if they are, there is probably a problem with the independence assumption.\n\nReturning to our OSA data, we can obtain the residuals using the resid() function and plot them.\n\nosa.hat <- makeFun(osa.model)\npreds <- osa.hat(Eyes$palpebral)\ngf_point( resid(osa.model) ~ preds, data = Eyes,\n        title =\"Residuals versus fitted values\")\n\n\n\ngf_qq( ~ resid(osa.model),  data = Eyes, title = \"Normal QQ plot\")\n\n\n\ngf_point(resid(osa.model) ~ palpebral, data = Eyes,\n        title = \"Residuals versus explanatory variable\")\n\n\n\n\n\nacf(resid(osa.model), ylab = \"Residual ACF\", main = \"\")\n\n\n\n\n\n\nIf the assumptions of the model are correct, there should be no distinct patterns to these scatter plots of the residuals, and the normal-quantile plot should be roughly linear (since the model says that differences between observed responses and the true linear fit should be random noise following a normal distribution with constant standard deviation).\nIn this case things look pretty good.\nWe can save ourselves a little typing if we create these plots using plot() or mplot().\n\nmplot(osa.model, w = 1:2)\n\n[[1]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\nThe “standardized” residuals are the residuals that have been adjusted to have an expected variance (and hence also standard deviation) of 1. Roughly, they are the residuals divided by \\(\\hat \\sigma\\), but there is an additional adjustment that is made as well. This gives us a kind of unitless version of the residual and the normal-quantile plot using standardized residuals does a better job of indicating whether the normality assumption is compatibly with the data. (Remember, the normality assumption is about the errors not the residuals. The standardized residuals should behave roughly like a normal distribution if the errors are normally distributed.) Typically the shape of the normal-quantile plot made with raw residuals and the one made with standarized residuals will be very similar.\n\n\n7.5.4 Outliers in Regression\nOutliers can be very influential in regression, especially in small data sets, and especially if they occur for extreme values of the explanatory variable. Outliers cannot be removed just because we don’t like them, but they should be explored to see what is going on (data entry error? special case? etc.)\nSome researchers will do “leave-one-out” analysis, or “leave some out” analysis where they refit the regression with each data point left out once. If the regression summary changes very little when we do this, this means that the regression line is summarizing information that is shared among all the points relatively equally. But if removing one or a small number of values makes a dramatic change, then we know that that point is exerting a lot of influence over the resulting analysis (a cause for caution).\nThis kind of analysis can be very helpful, especially if you have one or several large potential outliers in your data set, but in this class, we will not generally do it as a matter of course (it’s not a required part of model assessment for coursework)."
  },
  {
    "objectID": "07-simple-linear.html#how-good-are-our-estimates",
    "href": "07-simple-linear.html#how-good-are-our-estimates",
    "title": "7  Linear Models",
    "section": "7.6 How Good Are Our Estimates?",
    "text": "7.6 How Good Are Our Estimates?\nAssuming our diagnostics indicate that fitting a linear model is reasonable for our data, our next question is How good are our estimates? Notice that there are several things we have estimated:\n\n\nThe intercept coefficient \\(\\beta_0\\) [estimate: \\(\\hat \\beta_0\\)]\nThe slope coefficient \\(\\beta_1\\) [estimate: \\(\\hat \\beta_1\\)]\nValues of \\(y\\) for given values of \\(x\\). [estimate: \\(\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x\\)]\n\n\n\nWe would like to be able to compute uncertainties and confidence intervals for these. Fortunately, R makes this straightforward.\n\n7.6.1 Estimating the \\(\\beta\\)s\n\nQ. Returning to the OSA data, compute standard uncertainties and 95% confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\).\nA. The summary() function provides additional information about the model:\n\nsummary(osa.model)\n\n\nCall:\nlm(formula = OSA ~ palpebral, data = Eyes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60942 -0.19875 -0.01902  0.21727  0.66378 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.3977     0.1680  -2.367   0.0251 *  \npalpebral     3.0800     0.1506  20.453   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.308 on 28 degrees of freedom\nMultiple R-squared:  0.9373,    Adjusted R-squared:  0.935 \nF-statistic: 418.3 on 1 and 28 DF,  p-value: < 2.2e-16\n\n\nWe don't know what to do with all of the information displayed here, but we can see\nsome familiar things in the coefficient table.  \nIf we only want the coefficients part of the summary output we can get that using\n\ncoef(summary(osa.model))\n\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -0.3977389  0.1680090 -2.367367 2.506081e-02\npalpebral    3.0799672  0.1505882 20.452918 2.252825e-18\n\n\nFrom this we see the estimates (\\(\\hat \\beta\\)’s) displayed again. Next to each of those is a standard error. That is the standard uncertainty for these estimates. So we could report our estimated coefficients as\n\\[\n    \\beta_0: -0.40 \\pm  -->\n        0.17\n    \\qquad\n    \\beta_1: 3.08 \\pm  \n        0.15\n\\]\nA confidence interval can be computed using \\[\n    \\hat\\beta_i \\pm t_* SE_{\\beta_i}\n\\] because\n\n\nthe sampling distribution for \\(\\hat \\beta_i\\) is normal,\nthe sampling distribution for \\(\\hat \\beta_i\\) is unbiased (the mean is \\(\\beta_i\\)), and\nthe standard deviation of the sampling distribution depends on \\(\\sigma\\) (and some other things), but\nwe don’t know \\(\\sigma\\), so we have to estimate it using \\(RMSE = \\sqrt{MSE}\\).\n\n\n\n\nt.star <- qt(.975, df = 28); t.star    # n-2 degrees of freedom for simple linear regression\n\n[1] 2.048407\n\nt.star * 0.151\n\n[1] 0.3093095\n\n\nSo a 95% confidence interval for $\\beta_1$ is \n\\[\n    3.08 \\pm 0.31\n\\] The degrees of freedom used are \\(DFE = n-2\\), the same as used in the estimate of \\(\\sigma^2\\). (We are using a t-distribution instead of a normal distribution because we don’t know \\(\\sigma\\). The degrees of freedom are those associated with using \\(RMSE = \\sqrt{MSE}\\) as our estimate for \\(\\sigma\\).)\nR\\ can compute confidence intervals for both parameters using the function\n`confint()`:\n\nconfint(osa.model)\n\n                 2.5 %      97.5 %\n(Intercept) -0.7418897 -0.05358811\npalpebral    2.7715014  3.38843310\n\n\nA 68% confidence interval should have a margin of error of approximately 1 standard uncertainty:\n\nconfint(osa.model, level = 0.68, \"palpebral\")\n\n              16 %     84 %\npalpebral 2.927507 3.232428\n\n(3.2325 - 2.9275) / 2   # margin of error\n\n[1] 0.1525\n\ncoef(summary(osa.model))\n\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -0.3977389  0.1680090 -2.367367 2.506081e-02\npalpebral    3.0799672  0.1505882 20.452918 2.252825e-18\n\n\n\n\n\n\n7.6.2 Confidence and Prediction Intervals for the Response Value\nWe can also create interval estimates for the response. R will compute this if we simply ask:\n\nestimated.osa <- makeFun(osa.model)\nestimated.osa(1.2, interval = \"confidence\")\n\n       fit      lwr      upr\n1 3.298222 3.174238 3.422206\n\nestimated.osa(0.8, interval = \"confidence\")\n\n       fit      lwr      upr\n1 2.066235 1.927384 2.205086\n\n\nThese intervals are confidence intervals for the mean response. Sometimes it is desirable to create an interval that will have a 95% chance of containing a new observation – that is, including the anticipated error as well as the mean response. These intervals are called prediction intervals to distinguish them from the usual confidence interval.\n\nestimated.osa <- makeFun(osa.model)\nestimated.osa(1.2, interval = \"prediction\")\n\n       fit      lwr      upr\n1 3.298222 2.655228 3.941216\n\nestimated.osa(0.8, interval = \"prediction\")\n\n       fit      lwr     upr\n1 2.066235 1.420209 2.71226\n\n\nPrediction intervals are typically much wider than confidence intervals.\nWe have to “cast a wider net” to create an interval that is highly likely to contain a new observation (which might be quite a bit above or below the mean).\nThe widths of both types of intervals depend on the value(s) of the explanatory variable(s) from which we are making the estimate. Estimates are more precise near the mean of the predictor variable and become less precise as we move away from there. Extrapolation beyond the observed data range is both less precise, and risky, because we don’t have data to know whether the linear pattern seen in the data extends into that region.\nThe plot below illustrates both confidence (dotted) and prediction (dashed) intervals. Notice how most of the dots are within the prediction bands, but not within the confidence bands.\n\ngf_point(OSA ~ palpebral, data = Eyes) |>\n  gf_lm(interval = \"confidence\") |>\n  gf_lm(interval = \"prediction\")\n\nWarning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n7.6.2.1 A Caution Regarding Prediction Intervals\nPrediction intervals are much more sensitive to the normality assumption than confidence intervals are because the Central Limit Theorem does not help when we are thinking about individual observations (essentially samples of size 1). So if the true distribution of errors is not really normal, then the prediction intervals we compute using the normality assumption will not be accurate.\n\nExercise 7.1 Rainfall\nUse the output below to answer some questions about rainfall volume and runoff volume (both in \\(m^3\\)) for a particular stretch of a Texas highway.\n\n\n\nCall:\nlm(formula = runoff ~ rainfall, data = TexasHighway)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.279 -4.424  1.205  3.145  8.261 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.12830    2.36778  -0.477    0.642    \nrainfall     0.82697    0.03652  22.642  7.9e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.24 on 13 degrees of freedom\nMultiple R-squared:  0.9753,    Adjusted R-squared:  0.9734 \nF-statistic: 512.7 on 1 and 13 DF,  p-value: 7.896e-12\n\n\n\n\nHow many times were rainfall and runoff recorded?\nWhat is the equation for the least squares regression line?\nReport the slope together with its standard uncertainty.\nGive a 95% confidence interval for the slope of this line.\nWhat does this slope tell you about runoff on this stretch of highway?\nWhat is \\(\\hat\\sigma\\)?\n\n\n\n\n\n\n\nSolution. \n\nThe residual degrees of freedom is \\(13\\), so there were \\(13 + 2 = 15\\) observations.\n\\(`runoff` = -1 + 0.83 \\cdot `rainfall`\\)\n\\(0.83 \\pm 0.04\\)\n\n\n\nconfint( rain.model, \"rainfall\" )\n\n             2.5 %    97.5 %\nrainfall 0.7480677 0.9058786\n\n\nWe can compute this from the information displayed:\n\nt.star <- qt( .975, df = 13 ) # 13 df listed for residual standard error\nt.star\n\n[1] 2.160369\n\nSE <- 0.0365\nME <- t.star * SE; SE\n\n[1] 0.0365\n\n0.8270 + c(-1,1) * ME  # CI as an interval\n\n[1] 0.7481465 0.9058535\n\n\nWe should round this using our rounding rules (treating the margin of error like an uncertainty). #. The slope tells us how much additional run-off there is per additional amount of rain that falls. Since both are in the same units (\\(m^3\\)) and since the intercept is essentially 0, we can interpret this slope as a proportion. Roughly 83% of the rain water is being measured as runoff. #. \\(5.24\\)\n\n\n\n\n\nExercise 7.2 Kids’ feet\nThe KidsFeet data set contains variables giving the widths and lengths of feet of some grade school kids.\n\nPerform our usual diagnostics to see whether there are any reasons to be concerned about using a simple linear model in this situation.\nBased on this data, what estimate would you give for the width of a Billy’s foot if Billy’s foot is 24 cm long?\n(Use a 95% confidence level.)\nBased on this data, what estimate would you give for the average width of a kids’ feet that 24 cm long? (Use a 95% confidence level.)\n\n\n\n\n\nSolution. \nfoot.model <- lm(width ~ length, data = KidsFeet)\nmplot(foot.model, w = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nmplot(foot.model, w = 2)\n\n\n\n\nOur diagnostics look pretty good. The residuals look randomly distributed with similar amounts of variability throughout the plot. The normal-quantile plot is nearly linear.\n\nf <- makeFun(foot.model)\nf(24, interval = \"prediction\")\n\n       fit      lwr      upr\n1 8.813022 7.996604 9.629441\n\nf(24, interval = \"confidence\")\n\n       fit      lwr      upr\n1 8.813022 8.665894 8.960151\n\n\nWe can’t estimate Billy’s foot width very accurately (between 8.0 and 9.6 cm), but we can estimate the average foot width for all kids with a foot length of 24 cm more accurately (between 8.67 and 8.96 cm).\n\n\n\nExercise 7.3 Space for bicycles\nSome traffic engineers were interested to study interactions between bicycle and automobile traffic. One part of the study involved comparing the amount of “available space” for a bicyclist (distance in feet from bicycle to centerline of the roadway) and “separation distance” (the average distance between cyclists and passing car, also measured in feet, determined by averaging based on photography over an extend period of time). Data were collected at 10 different sites with bicycle lanes. The data are available in the ex12.21 data set in the Devore7 package.\n\nWrite out an equation for the least squares regression line for predicting separation distance from available space.\nGive an estimate (with uncertainty) for the slope and interpret it.\nA new bicycle lane is planned for a street that has 15 feet of available space. Give an interval estimate for the separation distance on this new street. Should you use a confidence interval or a prediction interval? Why?\nGive a scenario in which you would use the other kind of interval.\n\n\n\n\n\nSolution. \nbike.model <- lm( distance ~ space, data = Devore7::ex12.21 )\ncoef(summary(bike.model))\n\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -2.1824715 1.05668813 -2.065388 7.274847e-02\nspace        0.6603419 0.06747931  9.785841 9.974851e-06\n\nf <- makeFun(bike.model)\nf( 15, interval = \"prediction\" )\n\n       fit      lwr      upr\n1 7.722656 6.313278 9.132035\n\n\nWe would use a confidence interval to estimate the average separation distance for all streets with 15 feet of available space.\n\nf( 15, interval = \"confidence\" )\n\n       fit      lwr      upr\n1 7.722656 7.293168 8.152145\n\n\n\n\n\nExercise 7.4 Biometrics\nSelect only the non-diabetic men from the pheno data set using\n\nlibrary(fastR2)\nMen <- Pheno |> filter(sex==\"M\" & t2d==\"control\")  # note the double == and quotes here\nhead(Men, 3)\n\n\n\n  \n\n\n\nThis data set contains some phenotype information for subjects in a large genetics study. You can find out more about the data set with\n\n?pheno\n\n\n\nUsing this data, fit a linear model that can be used to predict weight from height. What is the equation of the least squares regression line?\nGive a 95% confidence interval for the slope of this regression and interpret it in context. (Hint: what are the units?)\nGive a 95% confidence interval for the mean weight of all non-diabetic men who are 6 feet tall.\nNote the heights are in cm and the weights are in kg, so you will need to convert units to use inches and pounds. (2.54 cm per inch, 2.2 pounds per kg)\nPerform regression diagnostics. Is there any reason to be concerned about this analysis?\n\n\n\n\n\n\n\nSolution. \n\n\n\n\nmodel <- lm( weight ~ height, data = Men )\ncoef(summary(model))\n\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) -69.6988584 12.14450332 -5.739128 1.551023e-08\nheight        0.8706793  0.06994051 12.448856 1.277243e-31\n\n\n\n\n\n–>   –> \n\n\n\n\n# we can ask for just the parameter we want, if we like\nconfint(model, parm = \"height\")\n\n           2.5 %   97.5 %\nheight 0.7333052 1.008053\n\n\nThe slope tells us how much the average weight (in kg) increases per cm of height.\n\n\n\n\nf <- makeFun(model)\n# in kg\nf(6 * 12 * 2.54, interval = \"confidence\") \n\n       fit      lwr      upr\n1 89.53098 87.97557 91.08639\n\n# in pounds\nf(6 * 12 * 2.54, interval = \"confidence\") * 2.2\n\n       fit      lwr      upr\n1 196.9682 193.5463 200.3901\n\n\n\n\n\n\ngf_point(resid(model) ~ fitted(model))\n\n\n\ngf_qq( ~ resid(model))\n\n\n\n\nWe could also have used\n\nmplot(model, which = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nmplot(model, which = 2)\n\n\n\n\nThe residual plot looks fine. There is bit of a bend to the normal-quantile plot, indicating that the distribution of residuals is a bit skewed (to the right – the heaviest men are farther above the mean weight for their height than the lightest men are below).\nIn this particular case, a log transformation of the weights improves the residual distribution. There is still one man whose weight is quite high for his height, but otherwise things look quite good.\n\nmodel2 <- lm( log(weight) ~ height, data = Men)\ncoef(summary(model2))\n\n             Estimate   Std. Error  t value     Pr(>|t|)\n(Intercept) 2.5134829 0.1448876368 17.34781 2.145773e-54\nheight      0.0108067 0.0008344117 12.95128 8.626095e-34\n\ngf_point(resid(model2) ~ fitted(model2))\n\n\n\ngf_qq( ~ resid(model2))\n\n\n\n\nThis model says that \\[\n\\log( `weight` )\n    = 2.51 + 0.0108 \\cdot `height`\n\\] So \\[\n`weight`  \n= 12.3\n        \\cdot (1.011)^{`height`}\n\\]\n\n\n\n\n\nExercise 7.5 Anscobe’s data\nThe anscombe data set contains four pairs of explanatory (x1, x2, x3, and x4) and response (y1, y2, y3, and y4) variables. These data were constructed by Anscombe (Anscombe (1973)).\n\n\nFor each of the four pairs, us R to fit a linear model and compare the results.\nBriefly describe what you notice looking at this output. (You do not have to submit the output itself – let’s save some paper.)\nUse, for example,\n\n\n    model1 <- lm(y1 ~ x1, data = anscombe); summary(model1)\n\n\nFor each model, create a scatterplot that includes the regression line. (Make the plots fairly small and submit them.)\nComment on the results. Why do you think Anscombe invented these data?\n\n\n\n\n\n\nSolution. Anscombe’s data show that it is not sufficient to look only at the numerical summaries produced by regression software. His four data sets produce nearly identical output of and yet show very different fits. An inspection of the residuals (or even simple scatterplots) quickly reveals the various difficulties.\n\n\n\nExercise 7.6 Read and article\nFind an article from the engineering or science literature that uses a simple linear model and report the following information:\n\n\nPrint the first page of the article (with title and abstract) and write a full citation for the article on it. Staple this at the end of your assigment.\nIf the article is available online, provide a URL where it can be found. (You can write that on the printout of the first page of the article, too.)\nHow large was the data set used to fit the linear model? How do you know? (How did the authors communicate this information?)\nWhat are the explanatory and response variables?\nDid the paper give an equation for the least squares regression line (or the coefficients, from which you can determine the regression equation)? If so, report the equation\nDid the paper show a scatter plot of the data? Was the regression line shown on the plot?\nDid the paper provide confidence intervals or uncertainties for the coefficients in the model?\nDid the paper show any diagnostic plots (normal-quantile, residuals plots, etc.)? If not, did the authors say anything in the text about checking that a linear model is appropriate in their situation?\nWhat was the main conclusion of the analysis of the linear model?\nIf there is an indication that the data are available online, let me know where in case we want to use these data for an example.\n\n\n\nGoogle scholar might be a useful tool for locating an article, or a service like JSTOR (available through many academic libraries) also has a large number of scientific articles. Or you might ask an engineering or physics professor for an appropriate engineering journal to page through in the library. Since the chances are small that two students will find the same article if working independently, I expect to see lots of different articles used for this problem.\n\n\n\n\n\n\n\nExercise 7.7 Human waste in Japan\nHigh population density in Japan leads to many resource usage problems, including human waste removal. In a study of a new (in the 1990’s) copression machine for processing sewage sludge, researchers measures the moisture content of the compressed pellets (%) and the machines filtration rate (kg-DS/m/hr). You can load the data using\n\ndata(xmp12.06, package = \"Devore7\")\n\n\n\nWhat are the least squares estimates for the intercept and slope of a line that can be used to estimate the moisture content from the filtration rate? (Give them in our usual manner with both estimate and uncertainty.)\nThe first row of the data is\n\n\nhead(xmp12.06, 1)\n\n\n\n  \n\n\n\nCompute the residual for this observation.\n\nWhat is \\(\\hat \\sigma\\), the estimated value of \\(\\sigma\\)?\nGive a 95% confidence interval for the slope.\nGive a 95% confidence interval for the mean moisture content when the filtration rate is 170 kg-DS/m/hr.\n\n\n\n\n\n\n\nSolution. \ndata(xmp12.06, package = \"Devore7\")\nsewage.model <- lm(moistcon ~ filtrate, data = xmp12.06)\n# a)\ncoef(summary(sewage.model))  # this includes standard errors; could also use summary() \n\n               Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) 72.95854697 0.697528488 104.595795 1.615724e-26\nfiltrate     0.04103377 0.004836781   8.483693 1.051720e-07\n\n# b)\nresid(sewage.model)[1]\n\n         1 \n-0.2000784 \n\n# c)\nsigma(sewage.model)  # can also be read off of the summary() output\n\n[1] 0.6653314\n\n# d)\nconfint(sewage.model)[2, , drop = FALSE]\n\n              2.5 %     97.5 %\nfiltrate 0.03087207 0.05119547\n\n# e) \nestimated.moisture <- makeFun(sewage.model)\nestimated.moisture(170, interval = \"confidence\")\n\n       fit      lwr     upr\n1 79.93429 79.50398 80.3646\n\n\n\n\n\n\n\n\nAnscombe, F. J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17."
  },
  {
    "objectID": "08-nonlinear.html#how-big-is-your-r2",
    "href": "08-nonlinear.html#how-big-is-your-r2",
    "title": "8  Beyond Linear Regression",
    "section": "8.1 How big is your \\(R^2\\)?",
    "text": "8.1 How big is your \\(R^2\\)?\nOne part of regression model diagnostics is to check the fitted model’s \\(R^2\\) value, which gives an indication of the proportion of the variance in the response that has been “explained” by the model.\nA low value (closer to 0) means that data points are spread far around the best fit line; a high one (close to 1) means that data points are clustered very tightly around the line.\nA model with a low \\(R^2\\) value is not necessarily “bad” – it may still provide helpful information about a real relationship between your response and predictor. However, that relationship is very “noisy,” which means that your model will have poor predictive power – it will be unable to make predictions with the accuracy and precision you might hope for.\nOften, the predictive power of a model, and the \\(R^2\\) value, can be improved by adding additional explanatory variables – that is, fitting a model with more than one explanatory variable. It could have two predictors, three, or as many as you can (sensibly) come up with. This kind of model is called multiple regression. Mathematically, it means fitting a model of the form:\n\\[\ny ~ \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 + \\beta_3*x_3 ...\n\\]\nMultiple regression often makes sense when you are studying a complex process where there is most likely “more than one thing going on.” For example, you might consider modelling population growth rates worldwide using a data set including a set of social, economic, health, and political indicators compiled using data from the World Health Organization and partner organizations. The dataset description is available online: http://www.exploredata.net/Downloads/WHO-Data-Set. One idea might be to look for a linear relationship between per-capita income and population growth rate:\n\n# simplify some variable names\nnames(whodat)[10] <- \"PopGrowthRate\"\nnames(whodat)[6] <- \"IncomePerCapita\"\nnames(whodat)[7] <- \"FemaleSchoolEnrollment\"\ngf_point(PopGrowthRate ~ IncomePerCapita, data = whodat)\n\nWarning: Removed 24 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigure 8.1: Population growth rate and per capita income.\n\n\n\n\n\nwho.m1 <- lm(PopGrowthRate ~ IncomePerCapita, data = whodat)\nsummary(who.m1)\n\n\nCall:\nlm(formula = PopGrowthRate ~ IncomePerCapita, data = whodat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.68051 -0.70998 -0.02006  0.70727  2.78944 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      1.657e+00  1.048e-01   15.81  < 2e-16 ***\nIncomePerCapita -2.867e-05  6.218e-06   -4.61 7.72e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.041 on 176 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.1077,    Adjusted R-squared:  0.1027 \nF-statistic: 21.25 on 1 and 176 DF,  p-value: 7.723e-06\n\n\nThe \\(R^2\\) value of this model is very low. But that could be because, unsurprisingly, there are many factors contributing to population growth rate, not just income. For example, what about education? Perhaps more-educated women have fewer children, lowering the population growth rate. So we might want to model population growth rate as a function of both income and education.\nIn R, a multiple regression model can be fitted with a call to lm(). We just add additional predictors to the right hand side of the model formula, separated by \\(+\\) signs. For the WHO example discussed above, for example, we could try:\n\nwho.m2 <- lm(PopGrowthRate ~ IncomePerCapita + FemaleSchoolEnrollment, \n             data = whodat)\nsummary(who.m2)\n\n\nCall:\nlm(formula = PopGrowthRate ~ IncomePerCapita + FemaleSchoolEnrollment, \n    data = whodat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.43374 -0.52979  0.06017  0.56619  2.48052 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             4.095e+00  3.626e-01  11.293  < 2e-16 ***\nIncomePerCapita        -1.102e-05  6.028e-06  -1.827   0.0694 .  \nFemaleSchoolEnrollment -3.105e-02  4.504e-03  -6.894 1.06e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9086 on 167 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.3101,    Adjusted R-squared:  0.3018 \nF-statistic: 37.52 on 2 and 167 DF,  p-value: 3.475e-14\n\n\nWe would need to follow up with our diagnostics to fully assess these two models, but comparison of the \\(R^2\\) values immediately shows that \\(R^2\\) is much higher for the second model. In other words, the multiple regression has succeeded in explaining more of the variance in population growth rates than the simple linear regression with only one predictor."
  },
  {
    "objectID": "08-nonlinear.html#violations-of-linear-regression-assumptions",
    "href": "08-nonlinear.html#violations-of-linear-regression-assumptions",
    "title": "8  Beyond Linear Regression",
    "section": "8.2 Violations of Linear Regression Assumptions",
    "text": "8.2 Violations of Linear Regression Assumptions\nIn the previous chapter, we learned how to carry out regression diagnostics – to check whether or not the assumptions of linear regression analysis were valid for a particular analysis. If the assumptions are violated, then the conclusions (parameter estimates, but especially standard errors) will be incorrect, and the model results can not be trusted.\nFor each type of violation, there are some fixes or modifications we can try in order to fit a valid, trustworthy model to our data and still draw reliable conclusions. In this course, we will focus mainly on type of “fix”: applying transformations to linearize non-linear relationships, and allow us to apply linear regression to the transformed data. This approach is covered in detail in the rest of this chapter.\nBefore beginning our detailed discussion of transformation, we will briefly discuss several other types of “fixes”. The mathematical foundations of these more complex models are essentially beyond the scope of this class, but you should understand when they might be useful (for example, if you see a certain type of pattern in residual diagnostic plots, which technique might help solve the problem?) and be able to implement them in R.\nThe table below provides an overview of various problems you might uncover as you do regression diagnostics, along with possible solutions. Each entry in the table is covered in a bit more detail in the subsequent sections of this chapter.\n\n\n\n\n\n\n\n\nAssumption\nDescription of Problem\nOptions\n\n\n\n\nLinearity\nScatterplot (or residual plots) indicate nonlinear relationship\nTransform explanatory and or response variables. Alternative: fit a non-linear model using the R function nls()\n\n\nIndependence of errors\nACF plot indicates strong dependence of errors over time (or space)\nFit an “autoregressive” model, where this relationship between subsequent or nearby measurements is expected and accounted for. To do this in R, replace lm(y$\\sim$x) with something like gls(y$\\sim$x, correlation = corAR1(form = $\\sim$1)).\n\n\nNormality of errors\nResidual QQ plots indicates departure from normality\nFirst check if other assumptions may also be violated, and try options listed there. If that fails, you may need to add additional predictor variables to your model; or to fit a generalized linear model, a more sophisticated type of regression that we will not cover in this course.\n\n\nEqual Variance (homeoskedasticity)\nVariance of errors is not constant over the full range of response values, or over time\nFirst, make sure that the linearity assumption is not violated. Next, if you have the option of including additional predictors in your model, it may be helpful. Next, transforming the response variable may help. Finally, if none of those options provide a solution, you can fit a model with non-constant error variance. For example, if variance increases with fitted response values, you can replace lm(y$\\sim$x) with something like nls(y$\\sim$x, weights = varPower())"
  },
  {
    "objectID": "08-nonlinear.html#non-normal-errors",
    "href": "08-nonlinear.html#non-normal-errors",
    "title": "8  Beyond Linear Regression",
    "section": "8.3 Non-Normal Errors",
    "text": "8.3 Non-Normal Errors\nSometimes, during diagnostics for a linear regression model, you will find that residual quantile-quantile plots indicate that linear regression residuals are far from normally distributed. In this case, before trying to modify your model in any way, it is useful to check whether any other assumptions of the linear regression have also been violated. If they have, it is worthwhile to try to deal with those problems first, and see if solving them makes the residuals more normal.\nIf non-normal residuals are the only apparent problem with a linear regression model, adding additional explanatory variables might help in some cases. Most of the time, you would have to turn to a more sophisticated regression model called a generalized linear model (GLM). Fitting GLMs is beyond the scope of this class, and you will not be asked to do it."
  },
  {
    "objectID": "08-nonlinear.html#non-independence-of-errors",
    "href": "08-nonlinear.html#non-independence-of-errors",
    "title": "8  Beyond Linear Regression",
    "section": "8.4 Non-Independence of Errors",
    "text": "8.4 Non-Independence of Errors\nSometimes, regression diagnostics (particularly a plot of residuals as a function of time, or an ACF plot) will show that the residuals are not independent. This happens most often when the predictor variable is a temporal or spatial one; data points collected at similar times, or similar locations, are often similar to each other rather than independent.\nWe will consider a simple example using the price of chicken over time (in constant dollars, adjusted for inflation over time). It seems to make sense to try to predict the price of chicken as a function of time (it’s been getting progressively cheaper for the last century or so):  \n\nchickn <- read.csv(\"https://sldr.netlify.app/data/chickn.csv\")\ngf_point(Price ~ Year, data = chickn ) |> gf_lm()\n\nWarning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\nchick.mod <- lm(Price ~ Year, data = chickn)\nsummary(chick.mod)\n\n\nCall:\nlm(formula = Price ~ Year, data = chickn)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.07 -20.23  -4.75  13.20  79.98 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 5792.6918   329.9948   17.55   <2e-16 ***\nYear          -2.9123     0.1685  -17.29   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.48 on 68 degrees of freedom\nMultiple R-squared:  0.8146,    Adjusted R-squared:  0.8119 \nF-statistic: 298.8 on 1 and 68 DF,  p-value: < 2.2e-16\n\n\n\n\n\nHowever, there seems to be a problem with non-independence of the residuals. Price is not independent from year to year; if you know the price was a bit high one year, it’s likely to remain so for the next several years:\n\nacf(resid(chick.mod))\n\n\n\n\nThis is a big problem, because it tends to result in standard error estimates that are artificially small. In other words: we think we have estimated our slope and intercept parameters much more precisely than we really have, and would report falsely narrow confidence intervals. To fix the problem, we can consider replacing our simple linear regression: \\[\ny ~ \\beta_0 + \\beta_1x + \\epsilon\n\\] (where \\(\\epsilon \\sim N(0, \\sigma)\\)) with a model that expects that subsequent residuals to depend on previous ones, so that the residual for the data point collected at time \\(t\\) is: \\[\ne_t = \\rho e_{t-1} + \\epsilon\n\\] (where, still, \\(\\epsilon \\sim N(0, \\sigma)\\); and \\(\\rho\\) is a new parameter indicating how strong the dependence over time is.) This is called an AR(1) process, or an auto-regressive process of order 1. It can be fit easily in R using the function gls() instead of . gls() does “generalized least-squares” fitting, and is found in the package nlme. The function call syntax illustrated in this example will work any time the explanatory variable is the time (or space) one that is causing the non-independence.\n\nlibrary(nlme)\nchick.mod2 <- \n  gls(Price ~ Year, data = chickn, correlation = corAR1(form = ~ 1))\nsummary(chick.mod2)\n\nGeneralized least squares fit by REML\n  Model: Price ~ Year \n  Data: chickn \n       AIC      BIC    logLik\n  557.8043 566.6823 -274.9022\n\nCorrelation Structure: AR(1)\n Formula: ~1 \n Parameter estimate(s):\n      Phi \n0.9483234 \n\nCoefficients:\n               Value Std.Error   t-value p-value\n(Intercept) 4764.172 1600.4333  2.976802  0.0040\nYear          -2.387    0.8171 -2.921499  0.0047\n\n Correlation: \n     (Intr)\nYear -1    \n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.0593757 -0.5673406 -0.1571088  0.3360972  2.0991647 \n\nResidual standard error: 41.39414 \nDegrees of freedom: 70 total; 68 residual\n\n\nIf you plot the residuals of this new model, and plot the ACF, you will see that the correlation coefficients still have high vaules. However, in the new gls() fit, this correlation has now been taken into account in the standard errors (which are larger – compare the coefficient tables to verify it), so it is OK now to trust the model parameter estimates and predictions."
  },
  {
    "objectID": "08-nonlinear.html#heteroscedasticity-non-constant-error-variance",
    "href": "08-nonlinear.html#heteroscedasticity-non-constant-error-variance",
    "title": "8  Beyond Linear Regression",
    "section": "8.5 Heteroscedasticity (Non-constant Error Variance)",
    "text": "8.5 Heteroscedasticity (Non-constant Error Variance)\nSometimes, model diagnostics for a linear regression indicate that variance of errors is not constant over the full range of response values. Often, it is the case that the error variance grows larger as the predicted response value grows larger, resulting in a “trumpet-like” shape in the plot of residuals versus fitted values.\nIf you spot this problem, first, make sure that the linearity assumption is not violated. Next, if you have the option of including additional predictors in your model, it may be helpful. Next, transforming the response variable may help. Specifically, a log or square-root transformation of the response variable may be useful. (See more details and examples later in this chapter, when transformations are discussed in detail.)\nFinally, if none of the previous options provide a solution, you can fit a model that actually expects and accounts for non-constant error variance. We will not cover this topic in any detail, but this brief example is included for your future reference (outside this class). Example: if variance increases with fitted response values, you can fit an appropriate model with the gls function from the nlme package. To do so, replace lm(y ~ x) with something like nls(y ~ x, weights = varPower()).\nHere is a brief example, using the Ornstein dataset from the car package. It gives data on 248 Canadian companies, collected in the mid-1970s. The variable assets gives each company’s assets in millions of dollars, and interconnects gives the number of director and executive positions that are shared with other firms. A scatter plot shows that the richest companies have many of these “interlocks”, so we might model assets as a function of interlocks…however, the residuals have non-constant variance:\n\nlibrary(car)\ngf_point(assets ~ interlocks, data = Ornstein)\n\n\n\nom <- lm(assets ~ interlocks, data = Ornstein)\ngf_point(resid(om) ~ fitted(om))\n\n\n\n\nWe can try to correct for this problem by fitting a model that “expects” this non-constant variance, by using the function gls with the input weights = varPower(). (There are many other ways to model non-constant error variance; this small example gives you just a taste, and for this course, you would not be expected to deal with any cases other than ones like this, where error variance increases with fitted values.)\n\nom2 <- gls(assets ~ interlocks, data = Ornstein, weights = varPower())\n\nAs with the non-independence case, if you plot the residuals for this model, you will see that they DO still have non-constant variance…but again, now it is OK because our model has taken it into account, and computed parameter estimates and standard errors appropriately."
  },
  {
    "objectID": "08-nonlinear.html#non-linear-relationships",
    "href": "08-nonlinear.html#non-linear-relationships",
    "title": "8  Beyond Linear Regression",
    "section": "8.6 Non-linear Relationships",
    "text": "8.6 Non-linear Relationships\nThe rest of this chapter will provide detailed information on how to deal with some non-linear relationships in regression.\nLinear regression assumes a linear relationship between predictor and response variables, but not all relationships between pairs of quantitative variables are linear. There are two common ways to deal with nonlinear relationships:\n\n\nTransform the data before beginning linear regression analysis, so that there is a linear relationship between the (transformed) variables.\nFit a model that explicitly expects, and accounts for, the nonlinear relationship between the two variables."
  },
  {
    "objectID": "08-nonlinear.html#transformations-in-linear-regression",
    "href": "08-nonlinear.html#transformations-in-linear-regression",
    "title": "8  Beyond Linear Regression",
    "section": "8.7 Transformations in Linear Regression",
    "text": "8.7 Transformations in Linear Regression\nThe applicability of linear models can be extended through the use of various transformations of the data.\nThere are several reasons why one might consider a transformation of the predictor or response (or both).\n\n\nTo correspond to a theoretical model.\nSometimes we have a priori information that tells us what kind of non-linear relationship we should anticipate.\nFor example, an experiment to estimate Planck’s constant (\\(\\hbar\\)) using LED lights and a voltage meter is based on the relationship\n\n\\[\n    V_a = \\frac{\\hbar c }{e \\lambda} + k\n\\] where \\(V_a\\) is the activation voltage (the voltage at which the LED just begins to emit light), \\(c\\) is the speed of light, \\(e\\) is the energy of an electron, \\(\\lambda\\) is the frequency of the light emitted, and \\(k\\) is a constant that relates to the energy losses inside the semiconductor’s p-n junction. If we take \\(c\\) and \\(e\\) as known for now (in a fancier version we would work their uncertainties into this, too), we can design an experiment that measures \\(V_a\\) and \\(\\lambda\\) for a number of different colors.\nA little algebra gives us   \n\\[\n    V_a = \\frac{\\hbar c }{e} \\cdot \\frac{1}{\\lambda} + k\n\\] So if we fit a model with \\(V_a\\) as the response and \\(\\frac{1}{\\lambda}\\) as the predictor, then the slope and intercept of the resulting least squares regression line will give us and estimate for \\(\\frac{\\hbar c }{e}\\), from which we can solve for \\(\\hbar\\). (Note: if we know uncertainties for \\(c\\), for \\(e\\), and for the slope, we can compute an estimated uncertainty for \\(\\hbar\\) using our propagation of uncertainty methods.)\nTheory says that a scatter plot of \\(V_a\\) and \\(1/\\lambda\\) should form a straight line, so the the model we would fit would look something like\n\nlm(voltage ~ I(1/wavelength), data = mydata)\n\nWe need to wrap 1/lambda in I() because the arithmetic symbols (+, -, *, /, and ^) have special meanings inside the formula for a model. I() stands for inhibit special interpretation.\nNotice that the intercpet is not directly involved in estimating \\(\\hbar\\), but that we can’t fit the line and obtain our slope without it.\nMany non-linear relationships can be transformed to linearity.  \n@exr-transformations presents several more examples\nand asks you to determine a suitable transformation.\n\nTo obtain a better fit.\nIf a scatterplot or residual plot shows a clearly non-linear pattern to the data, then it would be inappropriate to fit a linear regression (and conclusions drawn from that model would be incorrect and misleading).\nIn the absence of theoretical reasons to expect a particular mathematical relationship between the variables being studied, we may select transformations based on the shape of the relationship as revealed in a scatterplot. Section 8.7.3 provides some guidance for selecting transformations in this situation.\nTo obtain better residual behavior.\nSometimes transformations are used to improve the agreement between the data and the assumptions about the error terms in the model. For example, if the variance in the response appears to increases as the predictor increases, a logarithmic or square root transformation of the response may decrease the disparity in variance.\nSome transformations are used to improve the agreement between the data and the assumptions about the error terms in the model. For example, if data are heteroscedastic – for example, if the variance in the response appears to increase as the predictor increases – a logarithmic or square root transformation of the response may help.\n\n\n\n\nIn practice, all three of these issues are intertwined. A transformation that improves the fit, for example, may or may not have a good theoretical interpretation.\nSimilarly, a transformation performed to achieve homoskedasticity   (equal variance; the opposite is called heteroskedasticity) may result in a fit that does not match the overall shape of the data very well. Despite these potential problems, there are many situations where a relatively simple transformation is all that is needed to greatly improve the model. Here, when we say “improve” the model, we mean that the assumptions of the model are satisfied, and the model fits the data acceptably well.\n\n8.7.1 Three Important Laws\nIn the sciences, relationships between variables based on some scientific theory are often referred to as “laws”. Many of these fall into one of three categories that are easily handled by transforming the data and fitting a linear regression model.\n\n8.7.1.1 Linear Laws\nWe’ve already talked about linear relationships, but it is worth mentioning them again because there are so many situations in which a linear relationship arises.\n\n\n8.7.1.2 Power Laws\nRelationships of the form \\[\ny = A x^p\n\\] are often called power laws. The two parameters are the exponent \\(p\\) and a constant of proportionality \\(A\\). Power laws can be linearized by taking logarithms: \\[\n\\log(y) = \\log(A x^p) = \\log(A) + p \\log(x)\n\\] So if we fit a model of the form\n\nlm(log(y) ~ log(x))\n\nThen \\(\\beta_0 = \\log(A)\\) and \\(\\beta_1 = p\\).\nIf a power law is a good fit for the data then\n\ngf_point( log(y) ~ log(x) )\n\nwill produce a roughly linear plot.\nFitting a power law results in estimates for the parameters \\(\\beta_0 = \\log(A)\\) and \\(\\beta_1 = p\\). Note that we can use logarithms with any base for this transformation. Typically natural logarithms are used (that’s what log() does in R).\nIn some specific applications we might use base 10 logarithms (log10() in R) or base 2 logarithms (log2() in R); this yields the commonly used scale for \\(\\beta_0 = \\log(A)\\), the constant of proportionality.\nSome common situations that are modeled with power laws include drag force vs speed, velocity vs. force, and frequency vs. force.\n\n\n8.7.1.3 Exponential Laws\nRelationships of the form \\[\ny = A B^x = A e^{Cx}\n\\] are often called exponential laws.\nThe two parameters are the base \\(B = e^C\\) and a constant of proportionality \\(A\\). Exponential laws can also be linearized by taking logarithms: \\[\n\\log(y) = \\log(A B^x) = \\log(A) + x \\log(B)\n\\] So if we fit a model of the form\n\nlm(log(y) ~ x)\n\nThen \\(\\beta_0 = \\log(A)\\) and \\(\\beta_1 = \\log(B) = C\\).\nIf an exponential law is a good fit for the data then\n\ngf_point( log(y) ~ x )\n\nwill produce a roughly linear plot.\nFitting an exponential law results in estimates for the parameters \\(\\beta_0 = \\log(A)\\) and \\(\\beta_1 = \\log(B) = C\\). Again, we will generally use natural logarithms. In this course, if you see a log() without an indication of the base of the logartihm, you can assume it is base “e”, a natural logarithm. Similarly, remember that for R, the function log() takes the natural logarithm.\nSome common situations that are modeled with exponential laws include population growth and radioactive decay. Note that exponential growth models are typically only good approximations over a limited range since exponential functions eventually grow quickly, and often some external constraints will limit this growth. For example, a culture of bacteria may grow roughly exponentially for a while, but eventually, limits on space and nourishment will make it impossible for exponential growth to continue.\n\n\n8.7.1.4 Log-log and semi-log plots\nGraphs of \\(\\log(y)\\) vs. \\(\\log(x)\\) (log-log) or \\(\\log(y)\\) vs \\(x\\) (semi-log) can be used to assess whether the power law or exponential law appears to apply in a given situation. If the law were a perfect description of the situation, all the points on the log-log or semi-log plot would fall along a straight line. In practice, the fit won’t be perfect, but the plot is a useful diagnostic. For example, you can compare a plot of \\(y\\) as a function of \\(x\\) with a log-log or semi-log plot, and see which one shows the most linear relationship between the two variables.\nIn the old days, before computers could readily transform the data, special graph paper was produced with semi-log or log-log scales to facilitate this sort of plot.\n   R can easily create plots with transformed scales.\nUse gf_refine() with input scales_*_log10() to your call to gf_point(), as detailed in the example below:\n\nx <- 1:10\ny <- 3 * x^1.5\ngf_point(log(y) ~ log(x))\n\n\n\ngf_point(log(y) ~ x)\n\n\n\ngf_point(y ~ x) |>\n  gf_refine(scale_x_log10(), scale_y_log10())\n\n\n\ngf_point(y ~ x) |>\n  gf_refine(scale_y_log10())\n\n\n\n\nAs expected, the log-log transformation makes things linear. Of course, with real data, the fit won’t be perfect like this.\n\n\n\n8.7.2 Other Models That Can Be Transformed to Linear\nThe three laws above are not the only kinds of relationships that can be transformed to linear.\n\nA chemical engineering text book suggest a law of the form \\[\n\\log( - \\frac{dC}{dt} ) =  \\log(k) + \\alpha \\log(C)\n\\] where \\(C\\) is concentration and \\(t\\) is time.\nThis is equivalent to \\[\n\\begin{aligned}\n- \\frac{dC}{dt}  &=  k \\cdot C^\\alpha\n\\\\\n- \\int C^{-\\alpha} \\; dC  &=  \\int k \\;dt\n\\\\\n- \\frac{1}{1-\\alpha} C^{1-\\alpha} &=  k t + d\n\\\\\n\\frac{1}{\\beta} C^{-\\beta} & = k t + d\n\\\\\nC^{-\\beta} & = \\beta k t + \\beta d\n\\end{aligned}\n\\]\nIf we know \\(\\beta = \\alpha - 1\\) (i.e., if we know \\(\\alpha\\)), then we can fit a linear model using\n\nlm(C^(-1/beta) ~ t)\n\nThe intercept of such a model will be \\(\\beta d\\) and the slope will be \\(\\beta k\\), from which we can easily recover \\(d\\) and \\(k\\).\nAlternatively, if we know \\(d = 0\\) (i.e., if we know that \\(C = 0\\) when \\(t = 0\\)), then we can use \\[\n\\begin{aligned}\n    \\log( C^{-\\beta} )  = -\\beta \\log(C) &= \\log(\\beta k t ) = \\log(\\beta k) + \\log t\n\\\\\n\\log(C) &= - \\frac{\\log(\\beta k)}{\\beta} - \\frac{1}{\\beta} \\log t\n\\end{aligned}\n\\]\nNow if we fit a model of the form\n\nlm(C ~ log(t))\n\nthe intercept will be \\(\\frac{-\\log(\\beta k)}{\\beta}\\) and the slope will be \\(\\frac{-1}{\\beta}\\). From this we can solve for \\(k\\) and \\(\\beta\\).\n\n\n\nContinuing the previous example, we will fit the following data\n\nConcentration <- data.frame(\n  time = c(0, 50, 100, 150, 200, 250, 300),               # minutes\n  concentration = c(50, 38, 30.6, 25.6, 22.2, 19.5, 17.4) # mol/dm^3 * 10^3\n)\ngf_point(concentration ~ time, data = Concentration)\n\n\n\n\nunder the assumption that \\(\\alpha = 2\\), so \\(\\beta = 1\\). In this case, our relationship becomes \\[\n\\frac{1}{C}  = - k t - d \\;.\n\\] We can now fit a model and see how well it does.\n\nconc.model <- lm(1/concentration ~ time, data = Concentration)\nsummary(conc.model)\n\n\nCall:\nlm(formula = 1/concentration ~ time, data = Concentration)\n\nResiduals:\n         1          2          3          4          5          6          7 \n-1.175e-04 -4.144e-05  8.281e-05  2.259e-04 -3.128e-05 -3.398e-05 -8.447e-05 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.012e-02  8.762e-05   229.6 2.97e-11 ***\ntime        1.248e-04  4.860e-07   256.8 1.70e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0001286 on 5 degrees of freedom\nMultiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 \nF-statistic: 6.593e+04 on 1 and 5 DF,  p-value: 1.7e-11\n\nconfint(conc.model)\n\n                   2.5 %       97.5 %\n(Intercept) 0.0198922974 0.0203427516\ntime        0.0001235447 0.0001260434\n\n\nThis provides estimates for the intercept \\(- \\beta d\\) and the slope \\(- \\beta k\\) of our model. We can divide by \\(-\\beta\\) to obtain estimates for \\(d\\) and \\(k\\).\nOf course, we should always look to see whether the fit is a good fit. \n\ngf_point(resid(conc.model) ~ fitted(conc.model))\n\n\n\ngf_qq( ~ resid(conc.model)) |> gf_qqline()\n\nWarning: The following aesthetics were dropped during statistical transformation: sample\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nNotice that these residuals are very small relative to the values for concentration. (We can see this from the vertical scale of the plot and also from the small value for residual standard error in the summary output.) The shape of the residual plot would be more disturbing if the magnitudes were larger and if there were more data.\nAs is, even if there is some systematic problem, it is roughly five orders of magnitude smaller than our concentration measurements, which likely can’t be measured to that degree of accuracy.\nIf we want to show the fit on top of the original data, we must remember to untransform the response, since the model we fitted is a model for \\(1/C\\), but we want to show a model for \\(C\\):\n\ngf_point( concentration ~ time, data = Concentration ) |>\ngf_line( 1/fitted(conc.model) ~ time, data = Concentration)\n\n\n\n\n\n\n\n\n8.7.3 The Ladder of Re-expression\nSometimes we have data for which there is no theory (yet) to suggest the form of a model. In such a case, we may let the data help suggest a model. If we find a model that fits well, we can return to the question of whether there is an explanation for that type of model.\nIn the 1970s, Mosteller and Tukey introduced what they called the ladder of re-expression and bulge rules Tukey (1977),Mosteller:1977:DataAnalysis that can be used to suggest an appropriate transformation to improve the fit when the relationship between two variables (\\(x\\) and \\(y\\) in our examples) is monotonic and has a single bend.\nTheir idea was to apply a power transformation to \\(x\\) or \\(y\\) or both – that is, to work with \\(x^a\\) and \\(y^b\\) for an appropriate choice of \\(a\\) and \\(b\\). Tukey called this ordered list of transformations the ladder of re-expression. The identity transformation has power~\\(1\\). The logarithmic transformation is a special case and is included in the list associated with a power of \\(0\\).\nThe direction of the required transformation can be obtained from Figure 8.2 and Table 8.1, which shows four bulge types, represented by the curves in each of the four quadrants.\nA bulge can potentially be straightened by applying a transformation to one or both variables, moving up or down the ladder as indicated by the arrows. More severe bulges require moving farther up or down the ladder.\n   –> A curve bulging in the same direction as the one in the first quadrant of Figure 8.2, for example, might be straightened by moving up the ladder of transformations for \\(x\\) or \\(y\\) (or both), while a curve like the one in the second quadrant, might be straightened by moving up the ladder for \\(y\\) or down the ladder for \\(x\\).\n\n\n\n\n\n\n\nFigure 8.2: Tukey and Mosteller’s buldging rule\n\n\n\n\n\n\n\n\n\nTable 8.1: Ladder of re-expression\n\n\npower\ntransformation\n\n\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(3\\)\n\\(x \\mapsto x^3\\)\n\n\n\\(2\\)\n\\(x \\mapsto x^2\\)\n\n\n\\(1\\)\n\\(x \\mapsto x\\)\n\n\n\\(\\frac12\\)\n\\(x \\mapsto \\sqrt{x}\\)\n\n\n\\(0\\)\n\\(x \\mapsto \\log(x)\\)\n\n\n\\(-1\\)\n\\(x \\mapsto \\frac1x\\)\n\n\n\\(-2\\)\n\\(x \\mapsto \\frac{1}{x^2}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\n\nThis method focuses primarily on transformations designed to improve the overall fit. The resulting models may or may not have a natural, or obvious, interpretation. These transformations also affect the shape of the distributions of the explanatory and response variables and, more importantly, of the residuals from the linear model\n(see Exercise 8.3). When several different transformations lead to reasonable linear fits, these other factors may lead us to prefer one over another.\n\nExample 8.1 Q. The scatterplot in Figure 8.3 shows a curved relationship between \\(x\\) and \\(y\\). What transformations of \\(x\\) and \\(y\\) improve the linear fit?\n\n\n\n\n\nFigure 8.3: A scatterplot illustrating a non-linear relationship between \\(x\\) and \\(y\\).\n\n\n\n\nA. This type of bulge appears in quadrant IV of Figure 8.2, so we can hope to improve the fit by moving up the ladder for \\(x\\) or down the ladder for \\(y\\). As we see in Figure 8.4, the fit generally improves as we move down and to the right – but not too far, lest we over-correct. A \\(\\log\\)-transformation of the response (\\(a = 1\\), \\(b = 0\\)) seems to be especially good in this case. Not only is the resulting relationship quite linear, but the residuals appear to have a better distribution as well.\n\n\n\n\n\n\n\nFigure 8.4: Using the ladder of re-expression to find a better fit.\n\n\n\n\n\nExample 8.2 Some physics students conducted an experiment in which they dropped steel balls from various heights and recorded the time until the ball hit the floor. We begin by fitting a linear model to this data.\n\nball.model <- lm(time ~ height, data = BallDrop)\nsummary(ball.model)\n\n\nCall:\nlm(formula = time ~ height, data = BallDrop)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0200108 -0.0089383  0.0001623  0.0082016  0.0186519 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.190243   0.004303   44.21   <2e-16 ***\nheight      0.251841   0.005516   45.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01009 on 28 degrees of freedom\nMultiple R-squared:  0.9867,    Adjusted R-squared:  0.9863 \nF-statistic:  2085 on 1 and 28 DF,  p-value: < 2.2e-16\n\ngf_point(time ~ height, data = BallDrop) |> gf_lm()\n\n\n\ngf_point(resid(ball.model) ~ fitted(ball.model))\n\n\n\n\nAt first glance, the large value of \\(r^2\\) and the reasonably good fit in the scatterplot might leave us satisfied that we have found a good model. But a look at the residual plot reveals a clear curvilinear pattern in this data.\nA knowledgeable physics student knows that (ignoring air resistance) the time should be proportional to the square root of the height.\nThis transformation agrees with Tukey’s ladder of re-expression, which suggests moving down the ladder for height or up the ladder for time.\n\nball.modelT <- lm(time ~ sqrt(height), data = BallDrop)\nsummary(ball.modelT)\n\n\nCall:\nlm(formula = time ~ sqrt(height), data = BallDrop)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0087773 -0.0038851  0.0000571  0.0030558  0.0125552 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.016078   0.004084   3.937 0.000498 ***\nsqrt(height) 0.430803   0.004863  88.580  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.005225 on 28 degrees of freedom\nMultiple R-squared:  0.9964,    Adjusted R-squared:  0.9963 \nF-statistic:  7846 on 1 and 28 DF,  p-value: < 2.2e-16\n\ngf_point(time ~ sqrt(height), data = BallDrop) |>\n  gf_lm(time ~ sqrt(height), data = BallDrop) \n\n\n\ngf_point(resid(ball.modelT) ~ fitted(ball.modelT))\n\n\n\n\nThis model does indeed fit better, but the residual plot indicates that there may be some inaccuracy in the measurement of the height.\nIn this experiment, the apparatus was set up once for each height and then several observations were made. So any error in this set-up affected all time measurements for that height in the same way. This could explain why the residuals for each height are clustered the way they are since it violates the assumption that the errors are independent. (See Example 8.3 for a simple attempt to deal with this problem.)\n\n\n\n\n\n\n\nExample 8.3 One simple way to deal with the lack of independence in the previous example is to average all the readings made at each height.\n(This works reasonably well in our example because we have nearly equal numbers of observations at each height.) We pay for this data reduction in a loss of degrees of freedom, but it may be easier to justify that the errors in average times at each height are independent (if we believe that the errors in the height set-up are independent and not systematic).\n\nBallDropAvg <- BallDrop |>\n  group_by(height) |>\n  dplyr::summarize(time = mean(time))\n\nball.modelA <- lm(time ~ sqrt(height), data = BallDropAvg)\nsummary(ball.modelA)\n\n\nCall:\nlm(formula = time ~ sqrt(height), data = BallDropAvg)\n\nResiduals:\n         1          2          3          4          5          6 \n 0.0039552 -0.0040318  0.0028227 -0.0051717  0.0001571  0.0022686 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.016078   0.007404   2.172   0.0956 .  \nsqrt(height) 0.430803   0.008816  48.863 1.05e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.004236 on 4 degrees of freedom\nMultiple R-squared:  0.9983,    Adjusted R-squared:  0.9979 \nF-statistic:  2388 on 1 and 4 DF,  p-value: 1.05e-06\n\ngf_point(time~ height, data = BallDropAvg) |> gf_lm()\n\n\n\ngf_point(resid(ball.modelA) ~ fitted(ball.modelA))\n\n\n\n\nUsing a square root transformation on averaged height measurements in the BallDrop data gives a similar fit but a very different residual plot. The interpretation of this model is also different.\nNotice that the parameter estimates are essentially the same as in the preceding example. The estimate for \\(\\sigma\\) has decreased some. This makes sense since we are now estimating the variability in averaged measurements rather than in individual measurements.\nOf course, we’ve lost a lot of degrees of freedom, and as a result, the standard error for our parameter estimate is about twice as large as before. This might have been different; had the mean values fit especially well, our standard error might have been smaller despite the reduced degrees of freedom.\nOne disadvantage of the data reduction is that it is hard to interpret the residuals (because there are fewer of them).\nAt first glance there appears to be a downward trend in the residuals, but this is largely driven by the fact that the largest residual happened to be for the smallest fit.\n\n\n\nExample 8.4 Q. Rex Boggs of Glenmore State High School in Rockhampton, Queensland, had an interesting hypothesis about the rate at which bar soap is used in the shower. He writes:\n\nI had a hypothesis that the daily weight of my bar of soap [in grams] in my shower wasn’t a linear function, the reason being that the tiny little bar of soap at the end of its life seemed to hang around for just about ever. I wanted to throw it out, but I felt I shouldn’t do so until it became unusable. And that seemed to take weeks.\n\n\nAlso I had recently bought some digital kitchen scales and felt I needed to use them to justify the cost. I hypothesized that the daily weight of a bar of soap might be dependent upon surface area, and hence would be a quadratic function .\n\n\nThe data ends at day 22. On day 23 the soap broke into two pieces and one piece went down the plughole.\n\nThe data indicate that although Rex showered daily, he failed to record the weight for some of the days.\nWhat do the data say in regard to Rex’s hypothesis?\nA. Rex’s assumption that weight should be a (quadratic) function of time does not actually fit his intuition. His intuition corresponds roughly to the differential equation \\[\n\\Partial{t}{W} = k W^{2/3}\\,,\n\\] for some negative constant \\(k\\) since the rate of change should be proportional to the surface area remaining.\n(We are assuming that the bar shrinks in such a way that its shape remains proportionally unaltered.) Solving this equation (by separation of variables) gives    \\[\nW^{1/3} = k t + C\n\\;.\n\\] We can fit untransformed and transformed models (weight^(1/3) ~ day) to this data and compare.\n\nsoap.model1 <- lm(weight ~ day, data = Soap)\nsummary(soap.model1)\n\n\nCall:\nlm(formula = weight ~ day, data = Soap)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2436 -1.2950  0.3078  1.3942  5.5040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 123.1408     1.3822   89.09   <2e-16 ***\nday          -5.5748     0.1068  -52.19   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.949 on 13 degrees of freedom\nMultiple R-squared:  0.9953,    Adjusted R-squared:  0.9949 \nF-statistic:  2724 on 1 and 13 DF,  p-value: < 2.2e-16\n\n\nThe scatterplot in Figure 8.5 (darker line) indicate that the untransformed model is already a good fit.1\n\nsoap.model2 <- lm(I(weight^(1/3)) ~ day, data = Soap)\nsummary(soap.model2)\n\n\nCall:\nlm(formula = I(weight^(1/3)) ~ day, data = Soap)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31107 -0.13666  0.01605  0.15044  0.20095 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.297706   0.083813   63.21  < 2e-16 ***\nday         -0.146980   0.006477  -22.69 7.67e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1788 on 13 degrees of freedom\nMultiple R-squared:  0.9754,    Adjusted R-squared:  0.9735 \nF-statistic:   515 on 1 and 13 DF,  p-value: 7.669e-12\n\n\n\n\n\n\n\nFigure 8.5: Comparing untransformed (darker) and transformed (lighter) fits to soap use data.\n\n\n\n\n\nThe transformed model in this case actually fits worse. The higher value of \\(r^2\\) for the untransformed model is an indication that the untransformed model explains a larger proportion of the variance in soap weights. It is left as an exercise for you to examine diagnostic plots of the model residuals in both cases; you should see that neither one looks markedly better than the other. (There is perhaps an issue with a small amount of non-independence, or correlation over time, of the residuals; we might expect that with data collected over time. However, the dataset is so small that it is hard to tell for sure if the problem is real and worth worrying about.)\nFigure 8.5 shows a scatterplot with both fits.\nThe data do not support Rex’s assumption that a transformation is necessary.\n    The scatterplot and especially the residual plots both show that the residuals are mostly positive near the ends of the data and negative near the center. Part of this is driven by a flattening of the pattern of data points near the end of the measurement period. Perhaps as the soap became very small, Rex used slightly less soap than when the soap was larger. Exercise 8.1 asks you to remove the last few observations and see how that affects the models.\nFinally, since a linear model appears to fit at least reasonably well (but see Exercise 8.1), we can give a confidence interval for \\(\\beta_1\\), the mean amount of soap Rex uses each shower.  –>  –>\n\nconfint(soap.model1)\n\n                 2.5 %     97.5 %\n(Intercept) 120.154672 126.126895\nday          -5.805514  -5.344014"
  },
  {
    "objectID": "08-nonlinear.html#nonlinear-least-squares",
    "href": "08-nonlinear.html#nonlinear-least-squares",
    "title": "8  Beyond Linear Regression",
    "section": "8.8 Nonlinear Least Squares",
    "text": "8.8 Nonlinear Least Squares\nAnother approach to non-linear relationships is called nonlinear least squares or nonlinear regression. In this approach, instead of attempting to transform the relationship until it becomes linear, we fit a nonlinear function by minimizing the the sum of the squared residuals relative to that (paramterized) nonlinear function (form). That is, our model now becomes \\[\ny = f(x) + \\varepsilon\n\\] where \\(f\\) may be any parameterized function.\nThe R function for fitting these models is nls(). This function works much like lm(), but there are some important differences:\n\n\nBecause the model does not have to be linear, we have to use a more verbose description of the model.\nNumerical optimization is used to fit the model, and the algorithm used needs to be given a reasonable starting point for its search. Specifying this starting point simultaneously lets R know what the parameters of the model are. (Each quantity with a starting value is considered a parameter, and the algorithm will adjust all the parameters looking for the best fit – i.e., the smallest MSE (and hence also the smallest SSE and RMSE).\n\n\n\nLet’s illustrate with an example.\n\nExample 8.5 Returning to the ball dropping experiment, let’s fit \\[\n\\begin{aligned}\n\\texttt{time} &= \\alpha_0 + \\alpha_1 \\sqrt{\\texttt{height}}\n\\end{aligned}\n\\tag{8.1}\\]\nusing nonlinear least squares.\n\nnls.model <- nls(time ~ alpha0 + alpha1 * sqrt(height), \n                  data = BallDrop, \n                  start = list(alpha0 = 0, alpha1 = 1))\n\nNotice how the model formula compares with the formula in (@eq:balldrop).\nThe starting point for the algorithm is specified with \n`start = list(alpha0 = 0, alpha1 = 1)`, which also declares \nthe parameters to be fit.\n\nWe can obtain the coefficients of the fitted model with\n\nnls.model\n\nNonlinear regression model\n  model: time ~ alpha0 + alpha1 * sqrt(height)\n   data: BallDrop\n alpha0  alpha1 \n0.01608 0.43080 \n residual sum-of-squares: 0.0007645\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 2.112e-07\n\n\nor\n\ncoef(nls.model)\n\n    alpha0     alpha1 \n0.01607833 0.43080348 \n\n\nA more complete summary can be obtained by\n\nsummary(nls.model)\n\n\nFormula: time ~ alpha0 + alpha1 * sqrt(height)\n\nParameters:\n       Estimate Std. Error t value Pr(>|t|)    \nalpha0 0.016078   0.004084   3.937 0.000498 ***\nalpha1 0.430803   0.004863  88.580  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.005225 on 28 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 2.112e-07\n\n\nWe can restrict our attention to the coefficients table with\n\ncoef(summary(nls.model))\n\n         Estimate  Std. Error   t value     Pr(>|t|)\nalpha0 0.01607833 0.004084015  3.936894 4.975519e-04\nalpha1 0.43080348 0.004863416 88.580433 7.732182e-36\n\n\n\nf <- makeFun(nls.model)\ngf_point(time ~ height, data = BallDrop) |>\n  gf_fun(f(height) ~ height, color = 'gray40')\n\n\n\ngf_point(resid(nls.model) ~ fitted(nls.model))\n\n\n\n\nWe can compare this to the ordinary least squares model by plotting both together on the same plot.\n\nlm.model <- lm(time ~ sqrt(height), data = BallDrop)\ng <- makeFun(lm.model)\ngf_point(time ~ height, data = BallDrop) |>\n  gf_fun(f(height) ~ height, color = 'gray80', size = 1.5) |>\n  gf_fun(g(height) ~ height, color = 'red', size = 0.5, linetype = 2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nIn this particular case, there is very little difference between the two models, but this is not always the case.\n\ncoef(nls.model)\n\n    alpha0     alpha1 \n0.01607833 0.43080348 \n\ncoef(lm.model)\n\n (Intercept) sqrt(height) \n  0.01607833   0.43080348 \n\n\n\n\n\nExample 8.6 Here is example where we fit a different model to the BallDrop data, namely \\[\n    `time` = \\alpha * `height`^p\n\\]\n\npower.model <- nls(time ~ alpha * height^power, data = BallDrop, \n                   start = c(alpha = 1, power = .5))\ncoef(summary(power.model))\n\n       Estimate  Std. Error   t value     Pr(>|t|)\nalpha 0.4472102 0.001342627 333.08590 6.333427e-52\npower 0.4796679 0.005805313  82.62567 5.387914e-35\n\n\nA power law can also be fit using lm() by using a log-log transformation.\n\npower.model2 <- lm(log(time) ~ log(height), data = BallDrop)\ncoef(summary(power.model2))\n\n             Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) -0.807610 0.004330482 -186.49426 7.101233e-45\nlog(height)  0.471911 0.006424548   73.45435 1.431476e-33\n\n\nAgain, the parameter estimates (and uncertainties) are very similar. Recall that to compare our intercept in the second model to the \\(\\alpha\\) value in the first model, we must untransform:\n\nexp(coef(power.model2)[1])\n\n(Intercept) \n  0.4459225 \n\n\nWe can use the delta method to estimate the uncertainty. Since \\(\\frac{d}{dx} e^x = e^x\\) the uncertainty is approximately \\[\n0.4459225 \\cdot 0.0043305\n=\n0.0019311\n\\]\n\n\n\nExample 8.7 In addition to comparing estimated parameters and their uncertainties, we should always look at the residuals of our model. For both the linear regression and the nonlinear least squares models, the assumption is that the error terms are independent, normally distributed, and have a common standard deviation. From the plots below we see\n\n\nThe nonlinear least squares model is a better match for these assumptions than the linear regression model.\nBoth models reveal a lack of independence – at a given height, the residuals move up or down as a cluster as was discussed in the previous section. Neither model is designed to handle this flaw in the design of the experiment.\n\n\n\n\ngf_qq( ~ resid(power.model), main = \"model 1\")\n\n\n\ngf_qq( ~ resid(power.model2))\n\n\n\ngf_point(resid(power.model) ~ fitted(power.model))\n\n\n\ngf_point(resid(power.model2) ~ fitted(power.model2))\n\n\n\n\n\n\nNow let’s take a look at an example where we need the extra flexibilty of the nonlinear least squares approach.\n\nExample 8.8 A professor at Macalester College put hot water in a mug and recorded the temperature as it cooled. Let’s see if we can fit a reasonable model to this data\n\ngf_point(temp ~ time, data = CoolingWater, ylab = \"temp (C)\", xlab = \"time (sec)\")\n\n\n\n\nOur first guess might be some sort of exponential decay\n\ncooling.model1 <- \n  nls(temp ~ A * exp( -k * time), data = CoolingWater, \n      start = list(A = 100, k = 0.1))\nf1 <- makeFun(cooling.model1)\ngf_point(temp ~ time, data = CoolingWater, xlim = c(-50, 300), ylim = c(0, 110), \n        ylab = \"temp (C)\", xlab = \"time (sec)\") |>\ngf_fun(f1(time) ~ time)\n\n\n\n\nThat doesn’t fit very well, and there is a good reason. The model says that eventually the water will freeze because \\[\n\\lim_{t \\to \\infty} A e^{-k t} = 0\n\\] when \\(k >0\\). But clearly our water isn’t going to freeze sitting on a lab table. We can fix this by adding in an offset to account for the ambient temperature:\n\ncooling.model2 <- nls(temp ~ ambient + A * exp(k * (1+time)), data = CoolingWater,\n                      start = list(ambient = 20, A = 80, k = -.1) )\nf2 <- makeFun(cooling.model2)\ngf_point(temp ~ time, data = CoolingWater, xlim = c(-50, 300), ylim = c(0, 110),\n        ylab = \"temp (C)\", xlab = \"time (sec)\") |>\ngf_fun(f1(time) ~ time, linetype = 2, color = \"gray80\") |>\ngf_fun(f2(time) ~ time, color = \"steelblue\")\n\n\n\n\nThis fits much better. Furthermore, this model can be derived from a differential equation \\[\n\\frac{dT}{dt} = -k (T_0 - T_{\\mathrm{ambient}})\n\\;,\n\\] known as Newton’s Law of Cooling.\nLet’s take a look at the residual plot\n\ngf_point(resid(cooling.model2) ~ time, data = CoolingWater) \n\n\n\nplot(cooling.model2, which = 1)\n\n\n\n\nHmm. These plots show a clear pattern and very little noise. The fit doesn’t look as good when viewed this way.\nIt suggests that Newton’s Law of Cooling does not take into account all that is going on here. In particular, there is a considerable amount of evaporation (at least at the beginning when the water is warmer). More complicated models that take this into account can fit even better. For a discussion of a model that includes evaporation, see http://stanwagon.com/public/EvaporationPortmannWagonMiER.pdf.2\n\n\n\n8.8.1 Choosing Between Linear and Non-linear Models\nSo how do we choose between linear and non-linear models? Let’s enumerate some of the differences between them:\n\n\nSome models cannot be expressed as linear models, even after transformations.\nIn this case we only have one option, the non-linear model.\nLinear models can be fit quickly and accurately without numerical optimization algorithms because they satisfy nice linear algebra properties.\nThe use of numerical optimizers in non-linear least squares models makes them subject to potential problems with the optimizers. They may not converge, may converge to the wrong thing, or convergence may depend on choosing an appropriate starting point for the search.\nThe two types of models make different assumptions about the error terms.\nIn particular, when we apply transformations to achieve a linear model, those transformations often affect the distribution of the error terms as well. For example, if we apply a log-log transformation to fit a power law, then the model is\n\\[\n\\begin{aligned}\n     \\log( y ) &= \\beta_0 + \\beta_1 \\log(x) + \\varepsilon\n     \\\\\n     y &= e^{\\beta_0}  x^{\\beta_1} e^\\varepsilon\n     \\\\\n     y &= \\alpha  x^{\\beta_1} e^\\varepsilon\n\\end{aligned}\n\\]\nSo the errors are multiplicative rather than additive and they have a normal distribution after applying the logarithmic transformation. This implies that the relative errors should be about the same magnitude rather than the absolute errors.\nThis is potentially very different from the nonlinear model where the errors are additive:\n\\[\n     y = \\alpha x^\\beta + \\varepsilon\n\\]\nPlots of residuals vs. fits and qq-plots for residuals can help us diagnose whether the assumptions of a model are reasonable for a particular data set.\nLinear models provide an easy way to produce confidence intervals for a mean response or an individual response.\nThe models fit using nls() do not have this capability."
  },
  {
    "objectID": "08-nonlinear.html#exercises",
    "href": "08-nonlinear.html#exercises",
    "title": "8  Beyond Linear Regression",
    "section": "8.9 Exercises",
    "text": "8.9 Exercises\n\nBall drop, revisited\nIn Example 8.5, we applied a square root transformation to the height. Is there another transformation that yields an even better fit?\n\n\n\nExercise 8.1 Soap, revisited\nRemove the last few days from the Soap data set and refit the models in Example 8.4. How much do things change? Do the residuals look better, or is there still some cause for concern?\n\n\n\nExercise 8.2 Transformations\nFor each of the following relationships between a response \\(y\\) and an explanatory variable \\(x\\), if possible find a pair of transformations \\(f\\) and \\(g\\) so that \\(g(y)\\) is a linear function of \\(f(x)\\): \\[\n  g(y) = \\beta_0 + \\beta_1 f(x) \\;.\n\\] For example, if \\(y = a e^{bx}\\), then \\(\\log(y) = \\log(a) + bx\\), so \\(g(y) = \\log(y)\\), \\(f(x) = x\\), \\(\\beta_0= \\log(a)\\), and \\(\\beta_1 = b\\).\n\n\n\n\\(y = a b^x\\).\n\\(y = a x^b\\).\n\\(y = \\frac{1}{a + bx}\\).\n\\(y = \\frac{x}{a + bx}\\).\n\n\n\n\\(y = a x^2 + b x + c\\).\n\\(\\displaystyle y = \\frac{1}{1+e^{a+bx}}\\).\n\\(\\displaystyle y = \\frac{100}{1+e^{a+bx}}\\).\n\n\n\n\n\n\nSolution. \n\n\\(\\log(y) = \\log(a) + x \\log(b)\\), so \\(g(y) = \\log(y)\\), \\(f(x) = x\\), \\(\\beta_0 = \\log(a)\\), and \\(\\beta_1 = \\log(b)\\).\n\\(\\log(y) = \\log(a) + b \\log(x)\\), so \\(g(y) = \\log(y)\\), \\(f(x) = \\log(x)\\), \\(\\beta_0 = \\log(a)\\), and \\(\\beta_1 = b\\).\n\\(\\frac{1}{y} = a + b x\\), so \\(g(y) = \\frac{1}{y}\\), \\(f(x) = x\\), \\(\\beta_0 = a\\), and \\(\\beta_1 = b\\).\n\\(\\frac{1}{y} = \\frac{a}{x} + b\\), so \\(g(y) = \\frac{1}{y}\\), \\(f(x) = \\frac{1}{x}\\), \\(\\beta_0 = b\\), and \\(\\beta_1 = a\\).\nnot possible\n\\(\\frac{1}{y} = 1 + e^{a + bx}\\), so \\(\\log(\\frac{1}{y} - 1) = \\log( \\frac{1-y}{y} ) = {a + bx}\\), so \\(g(y) = \\log(\\frac{1-y}{y})\\), \\(f(x) = {x}\\), \\(\\beta_0 = a\\), and \\(\\beta_1 = b\\).\n\\(\\frac{100}{y} = 1 + e^{a + bx}\\), so \\(\\log(\\frac{100}{y} - 1) = \\log( \\frac{100-y}{y} ) = {a + bx}\\), so \\(g(y) = \\log(\\frac{100-y}{y})\\), \\(f(x) = {x}\\), \\(\\beta_0 = a\\), and \\(\\beta_1 = b\\).\n\n\n\n\n\n\nErrors and transformations  What happens to the role of the error terms (\\(\\varepsilon\\)) when we transform the data? For each transformation from Exercise 8.2, start with the form \\[\n  g(y) = \\beta_0 + \\beta_1 f(x) + \\varepsilon\n\\] and transform back into a form involving the untransformed \\(y\\) and \\(x\\) to see how the error terms are involved in these transformed linear regression models.\nIt is important to remember that when we fit a linear model to transformed data, the usual assumptions of the model are that the errors in the (transformed) linear form are additive and normally distributed. The errors may appear differently in the untransformed relationship.\n\n\n\nExercise 8.3 Tukey bulge and skew\nThe transformations in the ladder of re-expression also affects the shape of a distribution.\n\n\nIf a distribution is symmetric, how does the shape change as we move up the ladder?\nIf a distribution is symmetric, how does the shape change as we move down the ladder?\nIf a distribution is left skewed, in what direction should we move to make the distribution more symmetric?\nIf a distribution is right skewed, in what direction should we move to make the distribution more symmetric?\n\n\n\n\n\n\nSolution. Moving up the ladder will spread the larger values more than the smaller values, resulting in a distribution that is right skewed.\n\n\n\nExercise 8.4 Pendulum\nBy attaching a heavy object to the end of a string, it is easy to construct pendulums of different lengths. Some physics students did this to see how the period (time in seconds until a pendulum returns to the same location) depends on the length (in meters) of the pendulum.\nThe students constructed pendulums of lengths varying from \\(10\\) cm to \\(16\\) m and recorded the period length (averaged over several swings of the pendulum). The resulting data are in the Pendulum data set in the fastR2 package.\n\n\n Fit a power law to this data using a transformation and\n a linear model.  \n How well does the power law fit?  \n What is the estimated power in the power law based on this model?\nFit a power law to this data using a nonlinear model. How well does the power law fit?\nWhat is the estimated power in the power law based on this model?\nCompare residual plots and normal-quantile plots for the residuals for the two models. How do the models compare in this regard?\n\n\n\n\n\n\nSolution. \nAt first glance, the two models might appear equally good.  In each case the \npower is a bit below 2 and the fits look good on top of the raw data.  \n<!--  (Note: the  -->\n<!--  function produced by `makeFun()` does not know how to invert the log -->\n<!--  transformation on the response variable, so we have to do that ourselves.) -->\n\nmodel <- lm(log(period) ~ log(length), data = Pendulum)\nmodel2 <- nls(period ~ A * length^power, data = Pendulum, start = list(A = 1, power = 2))\nf <- makeFun(model)\ng <- makeFun(model2)\ngf_point(period ~ length, data = Pendulum) |>\n  gf_fun(f(x) ~ x, col = 'gray50')\n\n\n\ngf_point(period ~ length, data = Pendulum) |>\n  gf_fun(g(x) ~ x, col = 'red', lty = 2)\n\n\n\ncoef(summary(model))\n\n             Estimate  Std. Error  t value     Pr(>|t|)\n(Intercept) 0.7207055 0.006360981 113.3010 2.027047e-35\nlog(length) 0.4784757 0.003937676 121.5122 3.536212e-36\n\ncoef(summary(model2))\n\n       Estimate Std. Error  t value     Pr(>|t|)\nA     2.0469760 0.02202717 92.92961 2.843896e-33\npower 0.4827226 0.00482899 99.96348 4.610858e-34\n\n\nBut if we look at the residuals, we see that the linear model is clearly \nbetter in this case.  The non-linear model suffers from heteroskedasticity.\n\ngf_point(resid(model) ~ fitted(model)) |> gf_smooth()\n\n`geom_smooth()` using method = 'loess'\n\n\n\n\ngf_point(resid(model2) ~ fitted(model2)) |> gf_smooth()\n\n`geom_smooth()` using method = 'loess'\n\n\n\n\n\nBoth residual distributions are reasonably close to normal, but not perfect.\nIn the ordinary least squares model, the largets few residuals are not as large\nas we would expect.\n\ngf_qq( ~ resid(model))\n\n\n\ngf_qq( ~ resid(model2))\n\n\n\n\nThe residuals in the non-linear model show a clear change \nin variance as the fitted value increases.  This is counteracted by the logarithmic\ntransformation of the explanatory variable.  (In other cases, the non-linaer model\nmight have the preferred residual distribution.)\nThe estimated power based on the linear model is \\(0.478 \\pm 0.004\\).\n\n\n\nExercise 8.5 Vapor pressure\nThe pressure data set contains data on the relation between temperature in degrees Celsius and vapor pressure in millimeters (of mercury). With temperature as the predictor and pressure as the response, use transformations or nonlinear models as needed to obtain a good fit. Make a list of all the models you considered and explain how you chose your best model. What does your model say about the relationship between pressure and temperature?\n\n\n\nSolution. Using Tukey’s buldge rules it is pretty easy to land at something like one of the following\n\ngf_point(log(pressure) ~ temperature, data = pressure)\n\n\n\ngf_point(log(pressure) ~ log(temperature), data = pressure)\n\n\n\n\nNeither of these is pefect (although both are much more linear than the original untransformed relationship). But if you think in degrees Kelvin instead, you might find a much better transformation.\n\ngf_point(log(pressure) ~ (273.15 + temperature), data = pressure)\n\n\n\ngf_point(log(pressure) ~ log(273.15 + temperature), data = pressure)\n\n\n\ngf_point(log(pressure) ~ 1/(273.15 + temperature), data = pressure)\n\n\n\n\nAh, that last one looks quite good. Let’s try that one.\n\nmodel <- lm(log(pressure) ~ I(1/(273.15 + temperature)), data = pressure)\nmplot(model, 1:2)\n\n[[1]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\nThings still aren’t perfect. There’s a bit of a bow in the residual plot, but the size of the residuals is quite small relative to the scale of the log-of-pressure values:\n\nrange( ~ resid(model))\n\n[1] -0.0745625  0.1517124\n\nrange( ~ log(pressure), data = pressure)\n\n[1] -8.517193  6.692084\n\n\nAlso the residual for the fourth observation is quite a bit larger than the rest.\n\n\n\nExercise 8.6 Fertilizing corn\nThe cornnit data set in the package faraway contains data from a study investigating the relationship between corn yield (bushels per acre) and nitrogen (pounds per acre) fertilizer application in Wisconsin. Using nitrogen as the predictor and corn yield as the response, use transformations (if necessary) to obtain a good fit. Make a list of all the models you considered and explain how you chose your best model.\n\n\n\nExercise 8.7 ACT and GPA\nThe data set ACTgpa (in the fastR2 package)  contains the ACT composite scores and GPAs of some randomly selected seniors at a Midwest liberal arts college.\n\n\nGive a 95% confidence interval for the mean ACT score of seniors at this school.\nGive a 95% confidence interval for the mean GPA of seniors at this school.\nUse the data to estimate with 95% confidence the average GPA for all students who score 25 on the ACT.\nSuppose you know a high school student who scored 30 on the ACT. Estimate with 95% confidence his GPA as a senior in college.\nAre there any reasons to be concerned about the analyses you have just done? Explain.\n\n\n\n\n\n\n\nSolution. \n\nWe can build a confidence interval for the mean by fitting a model with only an intercept term.\n\n\ngrades <- ACTgpa\nconfint(lm(ACT ~ 1, data = grades))\n\n               2.5 %   97.5 %\n(Intercept) 24.24932 27.90453\n\n\nBut this isn’t the only way to do it. Here are some other ways.\n\n# here's another way to do it; but you don't need to know about it\nt.test(grades$ACT)\n\n\n    One Sample t-test\n\ndata:  grades$ACT\nt = 29.386, df = 25, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 24.24932 27.90453\nsample estimates:\nmean of x \n 26.07692 \n\n# or you can do it by hand\nx.bar <- mean( ~ ACT, data = grades); x.bar\n\n[1] 26.07692\n\nn <- nrow(grades); n\n\n[1] 26\n\nt.star <- qt(.975, df = n-1); t.star\n\n[1] 2.059539\n\nSE <- sd( ~ ACT, data = grades) / sqrt(n); SE\n\n[1] 0.887387\n\nME <- t.star * SE; ME\n\n[1] 1.827608\n\n\n        So the CI is $26.0769231 \\pm 1.8276077$.  Of course, that is too many digits, we \n        should do some rounding to\n        $26.1 \\pm 1.8$.  \n        \n\n\n\n\nconfint(lm(GPA ~ 1, data = grades))\n\n               2.5 %   97.5 %\n(Intercept) 3.185408 3.578515\n\n# this could also be done the other ways shown above.\n\n\n\n\n\ngrades.model <- lm(GPA ~ ACT, data = grades)\nf <- makeFun(grades.model)\nf(ACT = 25, interval = \"confidence\")\n\n       fit      lwr      upr\n1 3.288243 3.166694 3.409792\n\n\n\n\n\n\nf(ACT = 30, interval = \"prediction\")\n\n       fit      lwr      upr\n1 3.723365 3.100775 4.345955\n\n\n\n\n\n\ngf_point(GPA ~ ACT, data = grades) |>\n  gf_lm(interval = \"confidence\") |>\n  gf_lm(interval = \"prediction\")\n\nWarning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\ngf_point(resid(grades.model) ~ fitted(grades.model)) |> gf_smooth()\n\n`geom_smooth()` using method = 'loess'\n\n\n\n\n\n        There are no major concerns with the regression model. The\n        residuals look pretty good.  (There is perhaps a bit more variability\n        in GPA for the lower ACT scores and if you said you were worried about\n        that, I would not argue.)  \n\n        The prediction intervals are very wide and hardly useful, however.\n        It's pretty hard to give a precise estimate for an individual\n        person -- there's just too much variability from preson to person, \n        even among people with the same ACT score. \n\n\n\n\n\nExercise 8.8 Drag force\nIn the absence of air resistance, a dropped object will continue to accelerate as it falls. But if there is air resistance, the situation is different. The drag force due to air resistance depends on the velocity of an object and operates in the opposite direction of motion. Thus as the object’s velocity increases, so does the drag force until it eventually equals the force due to gravity. At this point the net force is \\(0\\) and the object ceases to accelerate, remaining at a constant velocity called the terminal velocity.\nNow consider the following experiment to determine how terminal velocity depends on the mass (and therefore on the downward force of gravity) of the falling object. A helium balloon is rigged with a small basket and just the right ballast to make it neutrally buoyant. Mass is then added and the terminal velocity is calculated by measuring the time it takes to fall between two sensors once terminal velocity has been reached.\nThe Drag data set (in the fastR2 package) contains the results of such an experiment conducted by some undergraduate physics students. Mass is measured in grams and velocity in meters per second.\n(The distance between the two sensors used for determining terminal velocity is given in the height variable.)\nBy fitting models to this data, determine which of the following “drag laws” matches the data best:\n\n\nDrag is proportional to velocity.\nDrag is proportional to the square of velocity.\nDrag is proportional to the square root of velocity.\nDrag is proportional to the logarithm of velocity.\n\n\n\n\n\n\nSolution. The best of these four models is a model that says drag is proportional to the square of velocity. Given the design of the experiment, it makes the most sense to fit velocity as a function of drag force. Here are several ways we could do the fit:\n\nmodel1 <- lm(velocity^2 ~ force.drag, data = Drag)\nmodel2 <- lm(velocity ~ sqrt(force.drag), data = Drag)\nmodel3 <- lm(log(velocity) ~ log(force.drag), data = Drag)\n\n\ncoef(summary(model1))\n\n               Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) -0.06227051 0.221930683 -0.2805854 7.804746e-01\nforce.drag   0.08399767 0.002320596 36.1965958 3.448978e-32\n\n\n\ncoef(summary(model2))\n\n                    Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept)      -0.03585585 0.054832417 -0.653917 5.169073e-01\nsqrt(force.drag)  0.29097916 0.006806886 42.747762 5.239126e-35\n\n\n\ncoef(summary(model3))\n\n                  Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)     -1.1622713 0.04539588 -25.60301 2.020737e-26\nlog(force.drag)  0.4744661 0.01241003  38.23248 4.103776e-33\n\n\nNote that model1, model2, and model3 are not equivalent, but they all tell roughly the same story.\n\nf1 <- makeFun(model1)\nf2 <- makeFun(model2)\nf3 <- makeFun(model3)\ngf_point(velocity ~ force.drag, data = Drag) |>\n  gf_fun(sqrt(f1(x)) ~ x, alpha = .4) |>\n  gf_fun(f2(x) ~ x, alpha = .4, color = 'red') |>\n  gf_fun(f3(x) ~ x, alpha = .4, color = 'brown')\n\n\n\n\nThe fit for these models reveals some potential errors in the design of this experiment. Separating out the data by the height used to determine velocity suggests that perhaps some of the velocity measurements are not yet at terminal velocity. In both groups, the velocities for the greatest drag forces are not as fast as the pattern of the remaining data would lead us to expect.\n\ngf_point(velocity^2 ~ force.drag, data = Drag, color = ~ height)\n\n\n\nmplot(model1, w = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\ngf_point(log(velocity) ~ log(force.drag), data = Drag, color = ~ height)\n\n\n\ngf_point(velocity ~ force.drag, data = Drag, color = ~ height) |>\n  gf_refine(scale_x_log10(), scale_y_log10())\n\n\n\nmplot(model3, w = 1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nExercise 8.9 Drag force, revisited\nConstruct a plot that reveals a likely systematic problem with the Drag (see Exercise 8.8) data set. Speculate about a potential cause for this.\n\n\n\nSolution. See previous problem.\n\n\n\nExercise 8.10 Drag force, subsetting the data\nExercise 8.9 suggests that some of the data should be removed before analyzing the Drag data set. Redo Exercise 8.8 after removing this data.\n\n\n\nExercise 8.11 Spheres\nThe Spheres data set (in the fastR2 package) contains measurements of the diameter (in meters) and mass (in kilograms) of a set of steel ball bearings. We would expect the mass to be proportional to the cube of the diameter. Fit a model and see if the data reflect this.\n\n\n\nExercise 8.12 More spheres\nThe Spheres data set (in the fastR2 package) contains measurements of the diameter (in meters) and mass (in kilograms) of a set of steel ball bearings. We would expect the mass to be proportional to the cube of the diameter.\nUsing appropriate transformations fit two models: one that predicts mass from diameter and one that predicts diameter from mass.\nHow do the two models compare?\n\n\n\nExercise 8.13 Utilities\nThe Utilities data set has information from utilities bills at a Minnesota residence. Fit a linear model that predicts thermsPerDay from temp.\n\n\nWhat observations should you remove from the data before doing the analysis? Why?\n\nAre any transformations needed?\nHow happy are you with the fit of your model? Are there any reasons for concern?\nInterpret your final model (even if it is with some reservations listed in part c)).\nWhat does it say about the relationship between average monthly temperature and the amount of gas used at this residence? What do the parameters represent?\n\n\n\n\n\n\n\n\n\nTukey, John W. 1977. Exploratory Data Analysis. pub-aw:adr: pub-aw."
  },
  {
    "objectID": "09-hypothesis-testing.html#experimental-design-in-statistics",
    "href": "09-hypothesis-testing.html#experimental-design-in-statistics",
    "title": "9  Hypothesis Testing",
    "section": "9.1 Experimental Design in Statistics",
    "text": "9.1 Experimental Design in Statistics\nBefore we begin to talk about hypothesis testing, let’s review the general process of designing and carrying out a statistical experiment.\n\n\nDetermine the question of interest.\nJust what is it we want to know? It may take some effort to make a vague idea precise. The precise questions may not exactly correspond to our vague questions, and the very exercise of stating the question precisely may modify our question. Sometimes we cannot come up with any way to answer the question we really want to answer, so we have to live with some other question that is not exactly what we wanted but is something we can study and will (we hope) give us some information about our original question.\nDetermine the population.\nJust who or what do we want to know about? For example, are we only interested in one specific person, or women in general, or all women, or all people? Or, are we interested in the energy efficiency of one particular device, or all the machines in a certain factory, or all machines of a certain type, or all machines of a certain class, or all factories in a certain industry?\nSelect measurements.\nWe are going to need some data. We get our data by making some measurements. These might be physical measurements with some device (like a ruler or a scale). But there are other sorts of measurements too, like the answer to a question on a form. Sometimes it is tricky to figure out just what to measure. (How do we measure happiness or intelligence, for example?) Just how we do our measuring will have important consequences for the subsequent statistical analysis. The recorded values of these measurements are called variables (because the values vary from one individual to another).\nDetermine the sample.\nUsually we cannot measure every individual in our population; we have to select some to measure. But how many and which ones? These are important questions that must be answered. Generally speaking, bigger is better, but it is also more expensive. Moreover, no size is large enough if the sample is selected inappropriately.\nFor example, if we wanted to draw conclusions about energy use across a whole industry, we would have to be careful not to sample from just a single factory, or a single type of manufacturing device. If we wanted to draw conclusions about all people, we would have to be careful not to study only male college students. The sample should be a random selection from the whole population (or as close as we can get to that standard).\nMake and record the measurements.\nOnce we have the design figured out, we have to do the legwork of data collection. This can be a time-consuming and tedious process. A study of public opinion may require many thousands of phone calls or personal interviews. In a laboratory setting, each measurement might be the result of a carefully performed laboratory experiment.\nOrganize the data.\nOnce the data have been collected, it is often necessary or useful to organize them. Data are typically stored in spreadsheets or in other formats that are convenient for processing with statistical packages. Very large data sets are often stored in databases.\nPart of the organization of the data may involve producing graphical and numerical summaries of the data. These summaries may give us initial insights into our questions or help us detect errors that may have occurred to this point.\nDraw conclusions from data.\nOnce the data have been collected, organized, and analyzed, we need to reach a conclusion. What is the answer to our scientific question? Is our idea or hypothesis about the way things work incorrect, or do the data support it? How sure are we about these conclusions?\nProduce a report.\nTypically the results of a statistical study are reported in some manner. This may be as a refereed article in an academic journal, as an internal report to a company, or as a solution to a problem on a homework assignment. These reports may themselves be further distilled into press releases, newspaper articles, advertisements, and the like. The mark of a good report is that it provides the essential information about each of the steps of the study.\n\n\n\nAt this point, you may be wondering who the innovative scientist was and what the results of the experiment were. The scientist was R. A. Fisher, who first described this situation as a pedagogical example in his 1925 book on statistical methodology Fisher (1925). Fisher developed statistical methods that are among the most important and widely used methods to this day, and most of his applications were biological."
  },
  {
    "objectID": "09-hypothesis-testing.html#coins-and-cups",
    "href": "09-hypothesis-testing.html#coins-and-cups",
    "title": "9  Hypothesis Testing",
    "section": "9.2 Coins and Cups",
    "text": "9.2 Coins and Cups\nYou might also be curious about how the experiment came out. How many cups of tea were prepared? How many did the woman correctly identify? What was the conclusion?\nFisher never says. In his book he is interested in the method, not the particular results. But let’s suppose we decide to test the lady with ten cups of tea.\nWe’ll flip a coin to decide which way to prepare the cups.\nIf we flip a head, we will pour the milk in first; if tails, we put the tea in first. Then we present the ten cups to the lady and have her state which ones she thinks were prepared each way.\nIt is easy to give her a score (9 out of 10, or 7 out of 10, or whatever it happens to be). It is trickier to figure out what to do with her score. Even if she is just guessing and has no idea, she could get lucky and get quite a few correct – maybe even all 10. But how likely is that?\nHere’s one way we could find out. Suppose I flip a coin ten times and record the pattern of heads and tails. Your job is to guess the sequence of heads and tails. To make it more interesting, we’ll get a lot of other people to guess too and see how everyone does.\n\\(\\vdots\\)\nIf we compare all of the guessers, we will undoubtedly see that some did better and others worse.\nNow let’s suppose the lady gets 9 out of 10 correct. That’s not perfect, but it is better than we would expect for someone who was just guessing. On the other hand, it is not impossible to get 9 out of 10 just by guessing. So here is Fisher’s great idea: Let’s figure out how hard it is to get 9 out of 10 by guessing. If it’s not so hard to do, then perhaps that’s just what happened, so we won’t be too impressed with the lady’s tea tasting ability. On the other hand, if it is really unusual to get 9 out of 10 correct by guessing, then we will have some evidence that she must be able to tell something.\nBut how do we figure out how unusual it is to get 9 out of 10 just by guessing? We’ll learn another method later, but for now, let’s just flip a bunch of coins and keep track. If the lady is just guessing, she might as well be flipping a coin.\nSo here’s the plan. We’ll flip 10 coins. We’ll call the heads correct guesses and the tails incorrect guesses. Then we’ll flip 10 more coins, and 10 more, and 10 more, and . That would get pretty tedious. Fortunately, computers are good at tedious things, so we’ll let the computer do the flipping for us using a tool in the mosaic package.\nThe rflip() function can flip one coin\n\nlibrary(mosaic)\nrflip()\n\n\nFlipping 1 coin [ Prob(Heads) = 0.5 ] ...\n\nH\n\nNumber of Heads: 1 [Proportion Heads: 1]\n\n\nor a number of coins\n\nrflip(10)\n\n\nFlipping 10 coins [ Prob(Heads) = 0.5 ] ...\n\nT H T T H T T T H T\n\nNumber of Heads: 3 [Proportion Heads: 0.3]\n\n\nand show us the results.\nTyping rflip(10) a bunch of times is almost as tedious as flipping all those coins. But it is not too hard to tell R to do() this a bunch of times.\n\ndo(2) * rflip(10)\n\n\n\n  \n\n\n\nLet’s get R to do() it for us 10,000 times and make a table and a histogram of the results.\n\n\n\n\nRandomLadies <- do(10000) * rflip(10)\ngf_histogram( ~ heads, data = RandomLadies, binwidth = 1)  \n\n\n\n\n\ntally( ~ heads, data = RandomLadies)\n\nheads\n   0    1    2    3    4    5    6    7    8    9   10 \n   5  102  467 1203 2048 2470 2035 1140  415  108    7 \n\ntally( ~ heads, data = RandomLadies, format = 'percent')\n\nheads\n    0     1     2     3     4     5     6     7     8     9    10 \n 0.05  1.02  4.67 12.03 20.48 24.70 20.35 11.40  4.15  1.08  0.07 \n\ntally( ~ heads, data = RandomLadies, format = 'proportion')\n\nheads\n     0      1      2      3      4      5      6      7      8      9     10 \n0.0005 0.0102 0.0467 0.1203 0.2048 0.2470 0.2035 0.1140 0.0415 0.0108 0.0007 \n\n\nYou might be surprised to see that the number of correct guesses is exactly 5 (half of the 10 tries) only 25% of the time. But most of the results are quite close to 5 correct. 67% of the results are 4, 5, or 6, for example. And 90% of the results are between 3 and 7 (inclusive). But getting 8 correct is a bit unusual, and getting 9 or 10 correct is even more unusual.\nSo what do we conclude? It is possible that the lady could get 9 or 10 correct just by guessing, but it is not very likely (it only happened in about 1.2% of our simulations). So one of two things must be true:\n\n\nThe lady got unusually “lucky”, or\nThe lady is not just guessing.\n\n\n\nAlthough Fisher did not say how the experiment came out, others have reported that the lady correctly identified all 10 cups! Salsburg (2001)\nThis same reasoning can be applied to answer a wide range of questions that have a similar form. For example, the question of whether dogs can smell cancer could be answered essentially the same way (although it would be a bit more involved than preparing tea and presenting cups to the Lady)."
  },
  {
    "objectID": "09-hypothesis-testing.html#a-general-framework",
    "href": "09-hypothesis-testing.html#a-general-framework",
    "title": "9  Hypothesis Testing",
    "section": "9.3 A General Framework",
    "text": "9.3 A General Framework\nIn statistical hypothesis testing, we can follow the following general procedure. We usually begin with some idea about how the process we are studying should work. For example, we might have a hunch that highway bridges with higher traffic flows are in poorer condition, and therefore merit more frequent repairs. For statistical hypothesis testing, we must translate that “hunch” into a testable null hypothesis, often called \\(H_0\\): one that can be demonstrated to be very unlikely in light of our data. In the case of the bridges, a testable null hypothesis might be that bridge condition does not depend on traffic flow. Then, if our data shows a strong apparent relationship between condition and traffic, we have evidence to reject the null hypothesis, and the data support the idea that there is some relationship between condition and traffic.\nNull hypothesis are usually “boring,” no-result hypotheses: there is no pattern; there is no relationship between the variables of interest; there is no difference between the two samples of interest. If we can reject the null hypothesis in a certain case, we have some evidence – but NOT proof – that there is an interesting pattern in our data, and thus in the population we are trying to draw conclusions about.\nThe alternative to the null hypothesis is called the alternative hypothesis, often called \\(H_1\\). The alternative hypothesis, stated most generally, is usually some form of “the null hypothesis is not true” – so there IS a pattern in the data, or a difference between the samples, etc.\n\n\n[hypothesis] A statement that can be true or false.\n[statistical hypothesis] A hypothesis about a parameter or parameters.\n\n\n\nIn our bridge example, a statistical null hypothesis \\(H_0\\) might be: the true slope of the regression of condition as a function of traffic is 0.\n\nExample 9.1 The following are examples of null hypotheses.\n\n\n\\(H_0: \\mu = 0\\). (The population mean is 0.)\n\\(H_0: \\beta_1=0\\). (The “true” slope is 0 – assuming a model like \\(\\E(Y) = \\beta_0 + \\beta_1 x\\).)\n\n\\(H_0: \\beta_1 = \\beta_2\\) (Two parameters in the model are equal.)\n\\(H_0: \\beta_2 = \\beta_3 = 0\\) (Two parameters in the model are both equal to 0.)\n\n\n\n\n\n\n9.3.1 The Four Step Process\nHypothesis testing generally follows a four-step process.\n\n1. State the null (\\(H_0\\)) and alternative (\\(H_1\\)) hypotheses.\nThe null hypothesis is on trial and innocent until proven guilty. We will render one of two verdicts: Reject the null hypothesis (guilty) or do not reject the null hypothesis (not guilty). *Important! We can never accept or prove either hypothesis – only reject the null (because it doesn’t seem compatible with the data), or fail to reject the null (since it appears plausibly compatible with teh data).\n\n\n2. Compute a test statistic.\nFor a statistical hypothesis test, all the evidence against the null hypothesis must be summarized in a single number called the test statistic. It is a statistic because it is a number computed from the data. It is called a test statistic because we are using it to do hypothesis testing.\n\n\n3. Determine the p-value.\nThe p-value is a probability: Assuming the null hypothesis is true, how likely are we to get at least as much evidence against it as we have in our data (i.e., a test statistic at least as unusual as the one observed) just by random chance?\n\n\n\n4. Interpret the results.\nIf the p-value is small, then one of two things is true:\n\nThe null hypothesis is true and something very unlikely occurred in our sample, or\nThe null hypothesis is false, so it is unsurprising that our observed data yield statistics that seem unlikely based on that (incorrect, untrue) hypothesis.\n\nFor this reason we consider small p-values to provide evidence against the null hypothesis.\n\n\n9.3.2 The Lady Tasting Tea, Revisted\n\nExample 9.2 For the lady tasting tea, this process looks like\n\n1. State Hypotheses\n\n\\(H_0\\): The probability of being correct is \\(0.5\\) (she’s just guessing).\n\\(H_a\\) The probability of being correct is larger than \\(0.5\\) (she can do better than someone who just guesses).\n\n\n\n2. Compute the test statistic\n\n\\(x = 9\\) (of \\(n = 10\\)) were correct.\n\n\n\n3. Determine the p-value\nBased on our empirical method, we estimate a p-value of\n\nprop( ~ (heads >= 9), data = RandomLadies)\n\nprop_TRUE \n   0.0115 \n\n\nThe probabilities for the number of correct guesses can be worked out theoretically as well. The resulting distribution is called a binomial distribution. As with other distributions we have seen, R includes the functions dbinom(), pbinom(), qbinom(), and rbinom().\n\ngf_dist(\"binom\", size = 10, prob = 0.5) # size = # of flips; prob = probability of heads\n\n\n\ndbinom(9, 10, .5) + dbinom(10, 10, .5)\n\n[1] 0.01074219\n\n1 - pbinom(8, 10, .5)                   # Note: P(X >= 9) = 1 - P(X <= 8)\n\n[1] 0.01074219\n\n\n\n\n4. Interpret the p-value.\nHow small is “small”, and how small does a p-value have to be before we reject the null hypothesis? Often, a (usually called \\(\\alpha\\)) of 0.05 is used. Sometimes \\(\\alpha = 0.01\\) is used instead. Basically, this corresponds to a 5% chance of seeing results as extreme as those found in our data, were the null hypothesis really true. If we want to be more conservative about our judgement (not rejecting the null hypothesis unless the evidence in the data is stronger against it), we could use a smaller \\(\\alpha\\) value.\nIt is also common for researchers to simply report the p-values they obtain from their analysis, allowing readers to draw conclusions on their own.\n\n\n\n\n\nExample 9.3 This situation is so common that there is a function to do the calculations for us. We just need to provide the values of \\(x\\) and \\(n\\):\n\nbinom.test(x = 9, n = 10)           # default test is 2-sided\n\n\n\n\ndata:  9 out of 10\nnumber of successes = 9, number of trials = 10, p-value = 0.02148\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5549839 0.9974714\nsample estimates:\nprobability of success \n                   0.9 \n\n\nThe output above doesn’t match what we obtained in Example 9.2 because the alternative hypothesis is different. It accepts both low number of correct identifications (0 or 1) and high numbers (9 or 10) as evidence against the null hypothesis. If we want a one-sided p-value, we just need to ask:\n\nbinom.test(x = 9, n = 10, alternative = \"greater\")        \n\n\n\n\ndata:  9 out of 10\nnumber of successes = 9, number of trials = 10, p-value = 0.01074\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.6058367 1.0000000\nsample estimates:\nprobability of success \n                   0.9 \n\n\n\n\n\n\n\n\n\nApproximate p-value\nTranslation\n\n\n\n\n\\(> 0.10\\)\nNo convincing evidence against the null hypothesis\n\n\n\\(0.05-0.10\\)\nWeak evidence against the null hypothesis\n\n\n\\(0.01-0.05\\)\nSome evidence against the null hypothesis\n\n\n\\(<0.01\\)\nStrong evidence against the null hypothesis\n\n\n\\(<0.001\\)\nVery strong evidence against the null hypothesis"
  },
  {
    "objectID": "09-hypothesis-testing.html#statistical-significance",
    "href": "09-hypothesis-testing.html#statistical-significance",
    "title": "9  Hypothesis Testing",
    "section": "9.4 Statistical Significance",
    "text": "9.4 Statistical Significance\nThe word “significant” has a special meaning in statistics. If we say that a difference or relationship between variables is significant, that means that we have applied a hypothesis test, and have failed to reject the null hypothesis – in other words, we have data that provides some evidence against the null hypothesis, and supporting the alternative.\nSo, a pattern strong enough cause us to reject a null hypothesis of “no difference” or “no pattern” or “no relationship” is called a statistically significant difference, pattern, or relationship. Differences or relationships may fail to be statistically significant if they are small or weak, if they are masked by underlying variability, or if there is too little data. Good studies will collect enough data and work to reduce variability (if that is possible) in order to have a reasonable expectation of detecting differences if they are large enough to be scientifically interesting."
  },
  {
    "objectID": "09-hypothesis-testing.html#t-tests",
    "href": "09-hypothesis-testing.html#t-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.5 T-tests",
    "text": "9.5 T-tests\n\nMany hypothesis tests are conducted based on a \\(t\\)-distribution, and so they are called “t-tests”. This is because, according to the Central Limit Theorem, the sampling distributions of most of our statistics (parameter estimates calculated from data) follow Normal distributions. But just as we did when computing confidence intervals, we’ll always have to use the t-distribution rather than the normal distribution, since we don’t know \\(\\sigma\\) (the true population standard deviation), and since our sample size is finite. These t-tests all use a similar sort of test statistic:\n\\[\nt = \\frac{\\mbox{estimate} - \\mbox{hypothesized value}}{\\mbox{standard error}}\n\\]\nThe numerator tells us that the more the estimate and the hypothesized value differ, the stronger the evidence. The denominator tells us that differences mean more when the standard deviation is small than when the standard deviation is large.\nThe test statistic is converted to a p-value by comparing it to the t-distribution with appropriate degrees of freedom. For linear models, this is the degrees of freedom associated with the residual standard error. If considering some statistic (say, the mean value) for a single variable, the degrees of freedom will be n-1, where n is the sample size.\n\n9.5.1 The 1-sample t-test\nThe 1-sample t-test tests the null hypothesis\n\n\n\\(H_0: \\mu = \\mu_0\\), vs.\n\\(H_a: \\mu \\neq 0\\).\n\n\n\nThat is, it tests whether there is evidence that the mean of some population (\\(\\mu\\)) is different from some hypothesized value (\\(\\mu_0\\)) – often \\(\\mu_0 = 0\\).\n\nExample 9.4 Let’s look at some data on weight loss programs. In this data set, there were two groups. One group received a monetary incentive if they lost weight while following a weight loss program. The controls did not receive a monetary incentive, but followed the same program otherwise. Our null hypothesis is that the control participants would not lose weight – that the true weight loss without incentives would average 0 pounds. Let’s see whether on average the controls lost weight:\n\nlibrary(Stat2Data)\ndata(WeightLossIncentive)\nControls <- WeightLossIncentive |> filter(Group == \"Control\")\ndf_stats( ~ WeightLoss, data = Controls )\n\n\n\n  \n\n\ngf_boxplot( ~ WeightLoss, data = Controls)\n\n\n\n\nThe standard error when doing inference for a mean is \\[\nSE = \\frac{s}{\\sqrt{n}} =\n\\frac{\n    9.1077854\n}{\n    \\sqrt{ 19 }\n}\n=\n2.0894693\n\\]\n\nSE <- 9.108 / sqrt(19); SE\n\n[1] 2.089519\n\n\nIf we want to test our null hypothesis, then we compute a t-statistic:\n\nt <- (3.92 - 0) / SE; t\n\n[1] 1.87603\n\n\nand from this a p-value, which is the tails probability for a t-distribution with 18 degrees of freedom. In other words, the p-value gives the probability of getting a test statistic at least as big as the one we really got, assuming that the test statistic follows a t-distribution with \\(n-1\\) degrees of freedom.\nFirst, we find the probability of observing a test statistic of \\(t= 1.8760303\\) or larger. Then, we have to double this value – it would be at least as unlikely to see a test statistic of \\(-1.8760303\\) or smaller. Our p-value should give the area under the t-distribution curve for x-values smaller than \\(-1.8760303\\) and larger than \\(1.8760303\\); it’s the shaded area in the figure below:\n\n\n\n\n\n\n1 - pt( t, df = 19-1 )\n\n[1] 0.03848197\n\n2 * ( 1 - pt( t, df = 19-1 ) )\n\n[1] 0.07696394\n\n\nOur p-value is \\(0.077\\) (1 or 2 significant digits are sufficient for reporting p-values). This is not compelling evidence that the weight loss program (without incentives) actually leads to a change in weight. A change this large could occur just by chance in nearly 8% of samples.\n\n\n\nExample 9.5 In R, there is also a function to automate this t-test:\n\nt.test( ~ WeightLoss, data = Controls)\n\n\n    One Sample t-test\n\ndata:  WeightLoss\nt = 1.8766, df = 18, p-value = 0.07688\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.4687594  8.3108647\nsample estimates:\nmean of x \n 3.921053 \n\n\nIf we don’t want so much out put, we can ask R to report only the p-value:\n\npval(t.test( ~ WeightLoss, data = Controls))\n\n   p.value \n0.07688493 \n\n\nBy default, t.test() uses a significance level \\(\\alpha = 0.05\\). If we want to specify a different \\(\\alpha\\), we can use the input conf.level as follows:\n\nt.test( ~ WeightLoss, data = Controls, conf.level = 0.01)\n\n\n    One Sample t-test\n\ndata:  WeightLoss\nt = 1.8766, df = 18, p-value = 0.07688\nalternative hypothesis: true mean is not equal to 0\n1 percent confidence interval:\n 3.894498 3.947607\nsample estimates:\nmean of x \n 3.921053 \n\n\n\n\n\n\n9.5.2 The paired 2-sample t-test\nSometimes, a dataset contains paired observations. These might be, for example, two measurements on the same experimental subject before and after some experimental treatment; measurements on the same subject at two times, or in two different situations; (or others). In this case, we can not consider the measurements within a pair to be independent of each other. But what we are really interested in is the magnitude of the difference between the 2 observations in each pair. So this “paired t-test” problem reduces to a one-sample t-test, where the test statistic is constructed using the differences \\(D\\) between the 2 measurements in each pair:\n\\[\nt = \\frac{\\mbox{observed average difference} - \\mbox{hypothesized average difference}}{\\mbox{standard error}}\n\\]\nLet’s consider an example.\n\nExample 9.6 The following table provides the corneal thickness in microns of both eyes of patients who have glaucoma in one eye:\n\n\n\nHealthy\n484\n478\n492\n444\n436\n398\n464\n476\n\n\n\n\nGlaucoma\n488\n478\n480\n426\n440\n410\n458\n460\n\n\nDifference\n4\n0\n-12\n-18\n4\n12\n-6\n-1\n\n\n\nThe corneal thickness is likely to be similar in the two eyes of any single patient, so that the two observations on the same patient cannot be assumed to be independent. But maybe (after accounting for differences between people), there is some difference in corneal thickness that has to do with the presence or absence of glaucoma. First, we can have a look at the data:\n\nGlaucoma <- tibble(\n  subject = 1:8,\n  glaucoma = c(488, 478, 480, 426, 440, 410, 458, 460),\n  healthy = c(484, 478, 492, 444, 436, 398, 464, 476),\n  diff = glaucoma - healthy\n)\n\ngf_point(glaucoma ~ subject, data = Glaucoma, \n         color = ~\"glaucoma\", shape = ~\"glaucoma\", size = 3, alpha = 0.9) |> \ngf_point(healthy ~ subject, data = Glaucoma, \n         color = ~\"healthy\", shape = ~\"healthy\", size = 3, alpha = 0.6) |>\n  gf_refine(\n    scale_color_discrete(name = \"status\"),  \n    scale_shape_discrete(name = \"status\")\n  )\n\n\n\n\nIt looks like healthy corneas are often thicker. To try to quantify this, we consider the difference between each pair of observations, denoted by \\(d_i\\). We wish to test, \\[\nH_0: \\mu = 0 \\qquad \\mbox{ vs } \\qquad H_1: \\mu \\ne 0.\n\\] Under \\(H_0\\), \\[\nT = \\frac{\\bar{D}}{S/\\sqrt{n}} \\sim t_{n-1}.\n\\]\nIn R, we can compute \\(D\\) (the average of \\(d_i\\)) and its standard error to obtain the test statistic \\(t\\) and the corresponding p-value:\n\n\nn <- nrow(Glaucoma); n\n\n[1] 8\n\ndf_stats(~ diff, data = Glaucoma, mean, sd)\n\n\n\n  \n\n\nmeanD <- mean( ~ diff, data = Glaucoma); meanD\n\n[1] -4\n\nSE <- sd( ~ diff, data = Glaucoma) / sqrt(n); SE\n\n[1] 3.798496\n\nt <- (meanD - 0) / SE; t\n\n[1] -1.053048\n\npval <- 2 * pt(t, df=n-1); pval\n\n[1] 0.3273053\n\n\nNote that we used 2 * pt(…) because our test statistic \\(t\\) was negative. If it had been positive, we would have used 2 * (1-pt(…)) instead. The illustration below may help make this clearer – the p-value we are computing corresponds to the shaded areas in the plot.\n\n\n\n\n\nWe can also let R make all the computations for us. Notice that we use the differences between pairs as the input data, rather than the raw data itself.\n\n\ndf_stats(WeightLoss ~ Group, data = WeightLossIncentive)\n\n\n\n  \n\n\nmodel2 <- lm(WeightLoss ~ Group, data = WeightLossIncentive)\ncoef(summary(model2))\n\n                Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)     3.921053   2.122817 1.847099 0.0734505837\nGroupIncentive 11.755418   3.089152 3.805387 0.0005635376\n\n\n\n\n\n\n9.5.3 The (unpaired) 2-sample t-test\nIn some cases, our research sample may contain data from 2 different categories of observational units. For example, in the weight loss data, there were the control participants we considered above; there were also a number of incentivized participants, who received money if they lost weight. We might be interested in considering whether the incentive made a difference. Before we begin, we can plot the data:\n\ngf_boxplot(WeightLoss ~ Group, data = WeightLossIncentive)\n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nIt looks like the incentive group lost more weight. But how can we judge whether the difference between groups is really an effect of the incentive, and not just random variation?\nIn this case, the null hypothesis \\(H_0\\) would be that the average weight loss by control participants (\\(\\mu_c\\)) was the same as the average weight loss by incentivized participants (\\(\\mu_i\\)) – \\(H_0: \\mu_c = \\mu_i\\). The alternative hypothesis would be \\(H_1: \\mu_c \\neq \\mu_i\\) – there was a difference between the two groups.\nBut in this case – with data from 2 different categories – how can we compute the appropriate standard error and test statistic for a t-test? We have to consider the fact that there may be different numbers of data points in the 2 categories. Let \\(n\\) be the sample size within the first category \\(X\\) (control participants in the example), and \\(m\\) be the sample size in the other category \\(Y\\) (incentive participants in the example). How can we compute a standard error for a difference in means between the two groups?\nIn this case, if we assume that the sample variaces are equal between the two categories, then we can define the pooled sample variance \\(s_p\\):\n\\[\ns^2_p = \\frac{(n-1)s_X^2 + (m-1)s_Y^2}{(m+n-2)}\n\\]\nHere, \\(s_Y\\) and \\(s_X\\) are the sample standard deviations within each category. We will not provide proof here, but it is known that in this case, the test statistic is:\n\\[\nT = \\frac{\\bar{X}-\\bar{Y}}{S_p\\sqrt{\\frac{1}{m}+\\frac{1}{n}}}\n\\sim t_{m+n-2}.\n\\]\nAs written in the equation above, this test statistic follows a t-distribution with \\(m+n-2\\) degrees of freedom. We could carry out a two-sample t-test by hand using the test statistic defined above, or we can use the function t.test() and R will do the computations for us. For the weight loss example:\n\nhead(WeightLossIncentive, 4)\n\n\n\n  \n\n\nt.test( ~ WeightLoss, groups = Group, data = WeightLossIncentive,\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  WeightLoss by Group\nt = -3.8054, df = 34, p-value = 0.0005635\nalternative hypothesis: true difference in means between group Control and group Incentive is not equal to 0\n95 percent confidence interval:\n -18.033330  -5.477506\nsample estimates:\n  mean in group Control mean in group Incentive \n               3.921053               15.676471 \n\n\nNote the “var.equal=TRUE” input argument. With this input, R will carry out the t-test assuming equal variance between the two categories. If we want to avoid making this assumption, there is a modified version of the two-sample t-test called the Welch two-sample t-test, which does not assume equal variances. If we omit the “var.equal” input, or set it to “var.equal=FALSE”, then R will do a Welch two-sample t-test for us. (We will not cover in this course how to do a Welch test by hand).\nWe might be able to judge informally whether the equal-variance assumption is valid by looking at the distributions of our variable of interest grouped by category, and computing the sample standard deviations by groups:\n\ngf_dens(~ WeightLoss, color = ~Group, data = WeightLossIncentive)\n\nWarning: Removed 2 rows containing non-finite values (`stat_density()`).\n\n\n\n\nsd( ~ WeightLoss | Group, data = WeightLossIncentive, na.rm = TRUE)\n\n  Control Incentive \n 9.107785  9.413988 \n\n\nWe don’t have an obvious indication of unequal variances. (This assumption can also be tested statistically, although we won’t learn how in this class, and it’s generally not recommended to do such a test prior to running a t-test). If we wanted to do the t-test without the equal variance assumption, in R, we would use:\n\nt.test( ~ WeightLoss, groups = Group, data = WeightLossIncentive)\n\n\n    Welch Two Sample t-test\n\ndata:  WeightLoss by Group\nt = -3.7982, df = 33.276, p-value = 0.0005889\nalternative hypothesis: true difference in means between group Control and group Incentive is not equal to 0\n95 percent confidence interval:\n -18.05026  -5.46058\nsample estimates:\n  mean in group Control mean in group Incentive \n               3.921053               15.676471 \n\n\nIn practice, there is no real disadvantage to simply using Welch’s test all the time. So in general, if you are doing a 2-sample unpaired test, you should always use var.equal=FALSE (or omit the input var.equal, and it will default to FALSE).\n\n\n9.5.4 Testing Model Coefficients\nWe can also use t-tests to test the null hypothesis that there is no relationship between the predictor and explanatory variables in a regression model. If we can’t reject that hypothesis, then we have evidence that there is really some relationship, and that the response can be predicted based upon the explanatory variable.\n\nExample 9.7 Suppose you suspect that drag force should be proportional to the square of velocity. Let’s see if that is consistent with the data collected by some physics students.\nIn this experiment, the students rigged up neutrally buoyant balloon and then loaded it with different amounts of weight and dropped it until and recorded its terminal velocity. At that point the force due to gravity (determined by the mass loaded to the balloon) is equal to the drag force (because there is no acceleration).\nWe’ll fit a power law model \\[\n\\texttt{force.drag} = A \\cdot \\texttt{velocity}^a\n\\] and test the hypothesis that \\(a = 2\\). We can fit this model using a log-log transformation: \\[\n\\log(\\texttt{force.drag}) = \\log(A)  +  a \\log( \\texttt{velocity})\n\\] So \\(a = \\beta_1\\) in our usual linear model notation.\n\nlibrary(fastR2)\ndrag.model <- lm(log(force.drag) ~ log(velocity), data = Drag)\ngf_point(log(force.drag) ~ log(velocity), data = Drag)\n\n\n\n\nThe fit is not perfect, and in fact suggests a systematic problem with the way these data were collected.1 This is even clearer if we look at the residuals.\n\ngf_point(resid(drag.model) ~ fitted(drag.model))\n\n\n\ngf_qq( ~ resid(drag.model))\n\n\n\n\n\nsummary(drag.model)\n\n\nCall:\nlm(formula = log(force.drag) ~ log(velocity), data = Drag)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.34162 -0.14967 -0.04673  0.06663  0.66155 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    2.47366    0.04425   55.91   <2e-16 ***\nlog(velocity)  2.05149    0.05366   38.23   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2451 on 40 degrees of freedom\nMultiple R-squared:  0.9734,    Adjusted R-squared:  0.9727 \nF-statistic:  1462 on 1 and 40 DF,  p-value: < 2.2e-16\n\n\nNone of the p-values produced in this output is what we want.\nThey are testing the hypotheses that \\(\\beta_0 =\\) and that \\(\\beta_1 = 0\\). \\(\\beta_1 = 0\\).\nBut we can easily calculate the p-value we want since we have the standard error and degrees of freedom.\n\nbeta1.hat <- 2.051\nSE <- 0.05366\nt <-  ( beta1.hat - 2 ) / SE; t\n\n[1] 0.9504286\n\n2 * pt( - abs(t), df= 40 )\n\n[1] 0.3476022\n\n\nWith this large a p-value, we cannot reject the null hypothesis that \\(p = 2\\). A large p-value does not prove that \\(p = 2\\), but it does say that our data are consistent with that value.\nOf course, our data may be consistent with many other values of \\(p\\) as well.\n\n\n\nExample 9.8 We could also do the previous example using a nonlinear model\n\ndrag.model2 <- nls(force.drag ~ A * velocity^p, data = Drag, start = list(A = 1,p = 2))\nsummary(drag.model2)\n\n\nFormula: force.drag ~ A * velocity^p\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \nA 12.90528    1.61361   7.998 7.96e-10 ***\np  1.92989    0.09442  20.440  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.48 on 40 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 3.737e-06\n\n\nAgain, the p-values listed are not of interest (they are testing the hypotheses that each coefficient is 0). But we can compute the p-value of interest as follows:\n\nt <- (2 - 1.92989) / 0.0944; t\n\n[1] 0.7426907\n\n2 * pt( - abs(t), df = 40) \n\n[1] 0.4620085\n\n\nAlthough the two p-values are different, the conclusion is the same using either model:\nOur data are consistent with the hypothesis that \\(p = 2\\).\n\n\n\nExample 9.9  \n\ndata(PorschePrice, package = \"Stat2Data\")\nhead(PorschePrice)\n\n\n\n  \n\n\nporsche.model <- lm(Price ~ Mileage, data = PorschePrice)\nsummary(porsche.model)\n\n\nCall:\nlm(formula = Price ~ Mileage, data = PorschePrice)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.3077  -4.0470  -0.3945   3.8374  12.6758 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 71.09045    2.36986    30.0  < 2e-16 ***\nMileage     -0.58940    0.05665   -10.4 3.98e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.17 on 28 degrees of freedom\nMultiple R-squared:  0.7945,    Adjusted R-squared:  0.7872 \nF-statistic: 108.3 on 1 and 28 DF,  p-value: 3.982e-11\n\ngf_point( resid(porsche.model) ~ fitted(porsche.model)) |> gf_smooth()\n\n`geom_smooth()` using method = 'loess'\n\n\n\n\ngf_qq( ~ resid(porsche.model) )\n\n\n\n\nThe model looks reasonable. What are the two hypotheses being tested?\n\n\n\\(H_0: \\beta_0 = 0\\).\nOften this is not an interesting test because often we are not so interested in the intercept \\(\\beta_0\\), and especially not in whether it is 0. In this case, the intercept might be interesting because it tells us the price of a Porsche with no miles. On the other hand, we might not expect a used car, even one with very few miles to fit the same pattern as a new car. There is probably a loss in value that occurs as soon as a car is purchased.\nIn any case, it is clear that the intercept will not be 0; we don’t need a hypothesis test to tell us that. Indeed, the evidence is incredibly strong.\nA confidence interval for the intercept is more interesting since it gives a sort of “starting price” for used Porches.\n\n\nconfint(porsche.model)\n\n                 2.5 %     97.5 %\n(Intercept) 66.2360186 75.9448869\nMileage     -0.7054401 -0.4733618\n\n\n\n\\(H_0: \\beta_1 = 0\\).\nThere is strong evidence against this hypothesis as well. This is also not surprising. If \\(\\beta_1 = 0\\), that would mean that the price of the cars does not depend on the mileage.\nA test of \\(\\beta_1 = 0\\) in a simple linear model is often called the model utility test because it is testing whether the predictor (without any others) is of any use to us or not.\n\n\n\n\n\\(H_0: \\beta_1 = \\beta_{10}\\).\nAlthough the output above doesn’t do all of the work for us, we can test other hypotheses as well. (The notation above is a bit tricky, \\(\\beta_{10}\\) should be read ``\\(\\beta_1\\) null” – it is a hypothesized value for \\(\\beta_1\\).)\nFor example, let’s test \\(\\beta_1 = -1\\). That the hypothesis that the value drops one dollar per mile driven.\nWhile this example is interesting as an exercise, it is quite rare to have a sensible hypothesized value for a regression slope parameter that we want to test. It is much more common to ask, as we did above, “is there a pattern here indicating that the predictor is a useful predictor of the response”?\n\n\nt <- (-0.5894 - (-1) ) / 0.0566; t\n\n[1] 7.254417\n\n2 * pt( - abs(t), df = 28 )\n\n[1] 6.748867e-08\n\n\nThis p-value is small enough to cause us to reject this value for \\(\\beta_1\\)."
  },
  {
    "objectID": "09-hypothesis-testing.html#sec-ci-ht-duality",
    "href": "09-hypothesis-testing.html#sec-ci-ht-duality",
    "title": "9  Hypothesis Testing",
    "section": "9.6 Connection to Confidence Intervals",
    "text": "9.6 Connection to Confidence Intervals\nThere is a natural duality between t-based hypothesis tests and confidence intervals. Since the p-value is computed using tail probabilities of the t-distribution and confidence level describes the central probability, the p-value will be below 0.05 exactly when the hypothesized value is not contained in the 95% confidence interval. (Similar statements can be made for other confidence levels.) \n\nExample 9.10 In the preceding example we rejected the null hypothesis that \\(\\beta_1 = -1\\). In fact, we will reject (at the \\(\\alpha = 0.05\\) level) any hypothesized value not contained in the 95% confidence interval.\n\nt <- (-0.5894 - (- .71)) / 0.0566; t\n\n[1] 2.130742\n\n2 * pt( - abs(t), df = 28)\n\n[1] 0.04203043\n\n\nBut we won’t reject values inside the confidence interval.\n\nt <- (-0.5894 - (- .70)) / 0.0566; t\n\n[1] 1.954064\n\n2 * pt( - abs(t), df = 28 )\n\n[1] 0.06074829\n\n\n\n\n\nExample 9.11 The output below illustrates this duality.\n\ndrag.model <- lm(log(force.drag) ~ log(velocity), data = Drag)\ncoef(summary(drag.model))\n\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)   2.473663 0.04424622 55.90677 1.358299e-39\nlog(velocity) 2.051493 0.05365839 38.23248 4.103776e-33\n\nbeta1.hat <- 2.051\nSE <- 0.05366\nt <-  ( beta1.hat - 2 ) / SE; t\n\n[1] 0.9504286\n\n2 * pt( - abs(t), df= 40 )\n\n[1] 0.3476022\n\nconfint(drag.model)\n\n                 2.5 %   97.5 %\n(Intercept)   2.384238 2.563088\nlog(velocity) 1.943045 2.159941\n\n\nSince the confidence interval for \\(p\\) (i.e., for \\(\\beta_1\\)) includes 2, 2 is a plausible value for the power (i.e., consistent with our data). A 2-sided p-value larger than 0.05 says the same thing at the same level of confidence."
  },
  {
    "objectID": "09-hypothesis-testing.html#exercises",
    "href": "09-hypothesis-testing.html#exercises",
    "title": "9  Hypothesis Testing",
    "section": "9.7 Exercises",
    "text": "9.7 Exercises\n\nExercise 9.1 Geiger counter\nAn experiment was conducted to see if the number of clicks on a Geiger counter in a 7.5 minute interval is related to the distance (in m) between a radioactive source and the detection device according to an inverse square law:\n\\[\n    \\mbox{clicks} = A + \\frac{k}{\\mbox{distance}^{2}}\n\\]\nAnswer the questions below using the following output\n\nmodel <- lm( clicks ~ I(1/(distance^2)), data = Geiger)\nsummary(model)\n\n\nCall:\nlm(formula = clicks ~ I(1/(distance^2)), data = Geiger)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.234 -15.817   4.027  10.899  34.091 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        114.281     10.787   10.59 5.51e-06 ***\nI(1/(distance^2))   31.477      1.001   31.46 1.13e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.51 on 8 degrees of freedom\nMultiple R-squared:  0.992, Adjusted R-squared:  0.991 \nF-statistic: 989.7 on 1 and 8 DF,  p-value: 1.134e-09\n\nplot(model, w = 1:2)\n\n\n\n\n\n\n\n\n\nAre there are any reasons (from what you can tell in the output above) to be concerned about using this model?\nWhat does \\(A\\) tell us about this situation?\nWhat would it mean if \\(A\\) were 0? What if \\(A \\neq 0\\)?\nWhat is the estimate for \\(A\\)? Express this as an estimate \\(\\pm\\) uncertainty using our rules for numbers of digits.\nWhat is the p-value for the test of the null hypothesis that \\(A = 0\\)? What conclusion do we draw from this?\nWhat does \\(k\\) tell us about this situation? In what situations would \\(k\\) be larger or smaller? What would it mean if \\(k\\) were 0?\nExpress the estimate for \\(k\\) as an estimate \\(\\pm\\) uncertainty.\nWhat is the p-value for the test of the null hypothesis that \\(k = 0\\)? What conclusion do we draw from this?\nA standard radioactive substance has a value of \\(k = 29.812\\). Might that be the substance we are using here? Conduct an appropriate hypothesis test to answer this question. Carefully show all four steps.\n\n\n\n\n\n\n\nSolution. \n\nThe normal-quantile plot looks pretty good for a sample of this size. The residual plot is perhaps not quite as “noisy” as we would like (the first few residuals cluster above zero, then next few below), but it is not terrible either. Ideally we would like to have a larger data set to see whether this pattern persists or whether things look more noisy as we “fill-in” with more data.\n\\(A\\) is a measure of background radiation levels, it is the amount of clicks we would get if our test substance were “at infinity”, i.e., so far away (or beyond some shielding) that it does not affect the Geiger counter.\n\\(114 \\pm 11\\)\n5.5^{-6}. This gives strong evidence that there is some background radiation being measured by the Geiger counter.\n\\(k\\) measures the rate at which our test substance is emitting radioactive particles. If \\(k\\) is 0, then our substance is not radioactive (or at least not being detected by the Geiger counter).\n\\(31.5 \\pm 1.0\\)\n1.1^{-9}. We have strong evidence that our substance is decaying and contributing to the Geiger counter clicks.\n\\(H_0: k = 29.812\\); \\(H_a: k \\neq 29.812\\)\n\nt <- (31.5 - 29.812) / 1.0; t     # test statistic\n\n[1] 1.688\n\n2 * (1 - pt(t, df = 8))           # p -value\n\n[1] 0.1298892\n\n\n\nConclusion. With a p-value this large, we cannot reject the null hypothesis. Our data are consistent with the hypothesis that our test substance is the same as the standard substance.\n\n\n\n\n\nExercise 9.2 Gentleman tasting wine\nA gentleman claims he can distinguish between four vintages of a particular wine. His friends, assuming he has probably just had too much of each, decide to test him. They prepare one glass of each vintage and present the gentleman with four unlabeled glasses of wine. What is the probability that the gentleman correctly identifies all four simply by guessing?\n\n\n\nSolution. We can use the product of conditional probabilities:\n\\[\n\\frac14 \\cdot \\frac13 \\cdot \\frac12 \\cdot \\frac11 = \\frac1{24}\n\\] Since the probability of guessing the first glass correctly is \\(\\frac 14\\), the probability of guess the second correctly – assuming the first was correctly guessed – is \\(\\frac13\\); the probability of guessing the third correctly – assuming the first two were correctly guessed – is \\(\\frac 12\\); and if the first three are guessed correctly, the last is guaranteed to be correct.\nAltnernative solution: The wines can be presented in \\(4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\) different orders, so the probability of guessing correctly is \\(1/24\\).\n\n\n\nExercise 9.3 Drag refit\nRedo the drag force analysis after removing observations that appear not to have reached terminal velocity.\nIf you can describe the rows you want to remove logically, the filter() command works well for this. You can also remove rows by row number. For example, the following removes rows 1, 3, 5 and 7:\n\nDrag[ - c(1, 3, 5, 7), ] \n\n\n\n\nSolution. Let’s remove the fastest values at each height setting. Although they are the fastest, it appears that terminal velocity has not yet been reached. At least, these points would fit the overall pattern better if the velocity were larger.\n\ngf_point( force.drag ~ velocity, data = Drag,\n          show.legend = FALSE,\n          color = ~ (velocity < 3.9) & !(velocity > 1 & velocity < 1.5)) |>\n  gf_theme(legend.position = \"top\") \n\n\n\nDrag2 <- Drag |> filter((velocity < 3.9) & !(velocity > 1 & velocity < 1.5))\n\n\ndrag2.model <- lm(log(force.drag) ~ log(velocity), data = Drag2)\nsummary(drag2.model)\n\n\nCall:\nlm(formula = log(force.drag) ~ log(velocity), data = Drag2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31581 -0.10503  0.01792  0.08009  0.25424 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    2.36586    0.02722   86.93   <2e-16 ***\nlog(velocity)  2.11416    0.03679   57.46   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1367 on 28 degrees of freedom\nMultiple R-squared:  0.9916,    Adjusted R-squared:  0.9913 \nF-statistic:  3302 on 1 and 28 DF,  p-value: < 2.2e-16\n\nmplot(drag2.model, w = 1:2)\n\n[[1]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\nThe model is still not as good as we might like, and it seams like the fit is different for the heavier objects than for the lighter ones. This could be due to some flaw in the design of the experiment or because drag force actually behaves differently at low speeds vs. higher speeds. Notice the data suggest an exponent on velocity that is just a tiny bit larger than 2:\n\nconfint(drag2.model)\n\n                 2.5 %   97.5 %\n(Intercept)   2.310110 2.421611\nlog(velocity) 2.038795 2.189534\n\n\nSo our data (for whichever reason, potentially still due to design issues) is not compatible with the hypothesis that this exponent should be 2.\n\n\n\nExercise 9.4 Vegetable oil\nSixteen samples of a certain brand of hydrogenated vegetable oil were tested to determine their melting point. The mean melting point for the 16 samples was 94.32 degrees and the standard deviation was 1.2 degrees.\n\n\nConduct a test of the hypothesis \\(\\mu = 95\\). Follow the four step procedure.\nWill 95 be inside or outside of a 95% confidence interval for the melting point? How do you know?\n\n\n\n\n\n\n\nSolution. \nSE <- 1.2 / sqrt(16); SE\n\n[1] 0.3\n\nt <- (94.32 - 95) / SE; t\n\n[1] -2.266667\n\np_val <- 2 * pt(t, df = 15); p_val    # p-value\n\n[1] 0.03862989\n\n\n95 will be outside the confidence interval because the p-value is\nless than 0.05. As a double check, here is the 95% confidence interval.\n\nt_star <- qt(.975, df = 11); t_star\n\n[1] 2.200985\n\n94.32 + c(-1,1) * t_star * SE\n\n[1] 93.6597 94.9803\n\n\n\n\n\nExercise 9.5 Charpy V-notch impact test\nThe Charpy V-notch impact test is a common way to test the toughness of a material. This test was applied to 42 specimens of a particular alloy at 110 degrees F. The mean amount of transverse lateral expansion was computed to be 73.1 mils with a sample standard deviation of 5.9 mils.\nTo be suitable for a particular application, the true amount of expansion must be less than 75 mils. The alloy will not be used unless their is strong evidence (a p-value below 0.01) that this specification is met.\n\n\nUse a p-value to decide whether this alloy may be used.\nUse a confidence interval to decide whether this alloy may be used.\nAre there advantages to one approach over the other? If you had to present an argument to your boss, which approach would you use?\n\n\n\n\n\n\n\nSolution. \n\nIn this situation it makes sense to do a one-sided test since we will reject the alloy only if the amount of expansion is to high, not if it is too low.\n\n\nSE <- 5.9/sqrt(42); SE\n\n[1] 0.9103898\n\nt <- (73.1 - 75) / SE; t\n\n[1] -2.087018\n\npt(t, df = 41)\n\n[1] 0.02157239\n\n\n\nCorresponding to a 1-sided test with a significance level of \\(\\alpha = 0.01\\), we could make a 98% confidence interval (1% in each tail). The upper end of this would be the same as for a 1-sided 99% confidence interval\n\n\nt_star <- qt(0.99, df = 41)\n73.1 + c(-1, 1) * t_star * SE\n\n[1] 70.89613 75.30387\n\n\n\nIf all we are interested in is a decision, the two methods are equivalent. I confidence interval might be easier to explain to someone less familiar with statistics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFisher, R. A. 1925. Statistical Methods for Research Workers. Oliver & Boyd.\n\n\nSalsburg, D. 2001. The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century. W.H. Freeman, New York."
  },
  {
    "objectID": "10-more-examples.html#sec-hxer",
    "href": "10-more-examples.html#sec-hxer",
    "title": "10  More Examples",
    "section": "10.1 Heat Exchanger Example",
    "text": "10.1 Heat Exchanger Example\nIn this section will discuss several parts of the statistical analysis of a laboratory experiment involving a heat exchanger.\n\n10.1.1 Apparatus and Measurements\nFigure 10.1 shows a diagram illustrating the heat exchanger. Fluids of different temperatures flow in the annulus (\\(\\dot{m}_{3}\\)) and in the inner tube (\\(\\dot{m}_{1}\\)). The entire apparatus is insulated, so we expect little or no heat to be exchanged with the surroundings.\n\n\n\nFigure 10.1: Heat exchanger with statepoints.\n\n\nMass flow rates (\\(\\dot{m}\\)) are controlled via valves. Two mass flow rates (\\(\\dot{m}_{1}\\) and \\(\\dot{m}_{3}\\)) are measured by rotameters.1 Temperatures (\\(T_{1}\\)–\\(T_{4}\\)) are measured by thermocouples.2\n\nEach observation consists of four~(4) temperature measurements and two~(2) mass flow rate measurements. Here is an example data set with one set of measurements at each of 6 experimental conditions:            \n\nlibrary(fastR2)\nHeatX   \n\n\n\n  \n\n\ngf_point(m.hot ~ m.cold, data = HeatX, main = \"Experimental Configurations\", size = 3)\n\n\n\n\n\n\n\n\n\n10.1.2 Standardizing Units\nThe recorded flow rates are in L/min. We will convert them to L/sec and use seconds as our standard unit of time troughout the analyses.\n\nHeatX <- HeatX |>\n  mutate(\n    m.hot = m.hot / 60,\n    m.cold = m.cold / 60\n  )\n\nWe could also convert temperatures to degrees Kelvin, but since temperatures only appear as differences between two temperatures in the expression used here, we can leave them in degrees Celsius.\n\nTable Table 10.1 contains notation and definitions of important quantities involved in the heat exchanger experiment.\n\n\nTable 10.1: Notation for quantities involved in the heat exchanger experiment. Note, some of the physical “constants” are actually temperature dependant. In these cases, a value has been chosen that reflects the temperature range (approx. 15–35 degrees C) seen in the data.\n\n\n\n\n\n\n\n\nSymbol\nDefinition\nUnits\nEstimate\n\n\n\n\n\\(\\dot{Q}\\)\nheat transfer rate\n\\(\\mathrm{W}\\)\n\n\n\n\\(\\dot{m}\\)\nmass flow rate\n\\(\\mathrm{kg}/\\mathrm{s}\\)\n\n\n\n\\(C_{p}\\)\nspecific heat\n\\(\\mathrm{kJ}/(\\mathrm{kg}\\cdot\\mathrm{K})\\)\n\\(4.18 \\pm 0.1\\)\n\n\n\\(T\\)\ntemperature\n\\(\\mathrm{K}\\)\n\n\n\n\\(D\\)\ndiameter of inner tube\n\\(\\mathrm{m}\\)\n\\(0.0143 \\pm 0.0004\\) m\n\n\n\n\n\n(\\(9/16 \\pm 1/64\\) inches)\n\n\n\\(L\\)\nlength of the heat exchanger\n\\(\\mathrm{m}\\)\n\\(1.626 \\pm 0.006\\)\n\n\n\n\n\n(\\(64 \\pm 1/4\\) inches)\n\n\n\\(A\\)\nsurface area of the inner tube (\\(\\pi D L\\))\nm\\(^{2}\\)\n\n\n\n\\(U\\)\nheat transfer coefficient\n\\(\\mathrm{W}/(\\mathrm{m}^{2} \\cdot \\mathrm{K})\\)\n\n\n\n\\(h\\)\nconvective heat transfer coefficient (\\(\\approx 2 U\\))\n\\(\\mathrm{W}/(\\mathrm{m}^2\\cdot \\mathrm{K}\\))\n\n\n\n\\(\\Delta T_{lm}\\)\nlogarithmic mean temperature difference\n\\(\\mathrm{K}\\)\n\n\n\n\\(Nu_{D}\\)\nNusselt number based on \\(D\\)\n–\n\n\n\n\\(Re_{D}\\)\nReynolds number based on \\(D\\)\n–\n\n\n\n\\(Pr\\)\nPrandtl number\n–\n\n\n\n\\(\\mu\\)\ndynamic viscosity\n\\(\\mathrm{kg}/(\\mathrm{m} \\cdot \\mathrm{s})\\)\n\\(0.00102 \\pm 0.00001\\)\n\n\n\\(k\\)\nwater thermal conductivity\n\\(\\mathrm{W}/(\\mathrm{m} \\cdot \\mathrm{K})\\)\n\\(0.598 \\pm 0.004\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.1.3 Calculating the heat transfer, \\(\\dot Q\\)\nThe amount of heat exchanged between the hot and cold water can be determined from the temperature change and the mass flow rate as follows:\n\\[\n    \\dot{Q}_{1} = \\dot{m}_{1} C_{p} (T_{2} - T_{1})\n\\tag{10.1}\\]\n\\[\n    \\dot{Q}_{3} = \\dot{m}_{3} C_{p} (T_{4} - T_{3})\n\\tag{10.2}\\]\nWe can estimate the values of \\(\\dot{Q}\\) from our data by direct caclulation:\n\nC_p <- 4.18\nHeatX2 <- HeatX |>\n  mutate(\n    Q.cold = m.cold * C_p * (T.cold.out - T.cold.in),\n    Q.hot  = m.hot  * C_p * (T.hot.out  - T.hot.in),\n    Q.env  = Q.cold + Q.hot\n  )\nHeatX2\n\n\n\n  \n\n\n\n\n\n10.1.4 Estimating heat exchanged with environment\nIf no heat were exchanged with the environment and all measurements were without error, then our two estimates of \\(\\dot Q\\) would sum to 0. (Heat lost to one fluid is gained by the other.)\nAssuming any loss to (or gain from) the environment is essentially constant for the apparatus over the experimental conditions analysed, we can use our 6 observations to estimate the amount of heat exchanged with the environment:\n\ndf_stats( ~ Q.env, data = HeatX2)\n\n\n\n  \n\n\ngf_point( \"\" ~ Q.env, data = HeatX2, alpha = .6, cex = 3, jitter.data = TRUE)\n\n\n\n\nFrom this we can compute either a p-value for the hypothesis test that the mean difference in heat change is 0 or create a confidence interval for the mean difference in heat change. The information above is enough to do this “by hand” using the standard error formula \\(SE = \\frac{s}{\\sqrt{n}}\\) and a t-distribution with \\(n-1 = 5\\) degrees of freedom.\n\nSE = 0.08274 /sqrt(6); SE\n\n[1] 0.03377846\n\n0.09003 + c(-1,1) * qt(0.975, df = 5) * SE       # 95<!--  CI -->\n\n[1] 0.003199695 0.176860305\n\nt <- (0.08999 - 0) / SE; t\n\n[1] 2.664124\n\n2 * pt( -abs(t), df = 5 )                        # p-value\n\n[1] 0.04466228\n\n\nOr we can let R do all the computations for us:\n\n\n\n\nt.test( ~ Q.env, data = HeatX2)\n\n\n    One Sample t-test\n\ndata:  Q.env\nt = 2.6641, df = 5, p-value = 0.04466\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.003159159 0.176813063\nsample estimates:\n mean of x \n0.08998611 \n\n\nIn our example data, there is modest evidence for exchange with the environment, but the estimated amount of heat gained from the environment is not very precisely estimated. Even at the highest end of the confidence interval, the heat exchanged with the environment is an order of magnitude smaller than the heat exchanged within in the apparatus.\nNotes:\n\n\nThe fact that heat is gained to the system suggests that the cold water was in the outer pipe and hot water in the inner pipe.\nThis analysis assumes that the amount of heat gained/lost is constant over the different set-ups. From this small data set, there is not clear evidence of some other relationship between heat exchanged with the environment and the experimental set up:\n\n\ngf_point(Q.env ~ m.cold, data = HeatX2, color = ~ paste(\"m.hot =\", round(m.hot,2)))\n\n\n\n\n\nThe t-test and interval are based on the assumption that the distribution of deviations between the measured environment heat exchange and the actual is normally distributed. The data set is too small to provide much evidence upon which to judge whether this is a reasonable assumption. The largest value is quite a bit larger than the rest, but even if we remove that observation, our conclusions don’t change dramatically:\n\n\nt.test( ~ Q.env, data = subset(HeatX2, Q.env < max(Q.env)) )\n\n\n    One Sample t-test\n\ndata:  Q.env\nt = 3.4701, df = 4, p-value = 0.02558\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.01183725 0.10659609\nsample estimates:\n mean of x \n0.05921667 \n\n\n\n\nOverall, we conclude that there is likely some heat exchanged with the environment, but the amount of heat exchange with the environment appears to be at most a small factor in this situation.\n\n\n10.1.5 Estimating heat transfer between hot and cold water, \\(\\dot{Q}\\)\nEstimating \\(\\dot{Q}\\) with uncertainty is not possible from this data alone since since \\(\\dot{Q}\\) is not measured directly, and we have only one measurement for each experimental condition (so no way to look at how variable such measurements are without additional information. Furthermore, the design doesn’t provide a method for estimating the uncertainty in \\(\\dot{m}\\) and \\(T\\).\nIf, however, there are external estimates of the uncertainties for temperature and flow rate, and if we can assume these uncertainties are approximately independent, then we can use propagation of uncertainty to estimate the uncertainty in our estimates for heat exchanged. Such uncertainties might come from specifications of the equipment used or be based on past experience of the researcher.\nFor example, suppose that the uncertainties in temperature and flow rate measurements are approximately constant (over the range of temperatures involved): \\(u_T\\), and \\(u_{\\dot{m}}\\). Then the uncertainty in the difference between two independent temperatures is \\(\\sqrt{ u_T^2 + u_T^2 } = \\sqrt{2} u_T\\), and we can estimate the uncertainty in \\(\\dot Q\\) using the delta method.\n\n\nLet \\(\\dot{Q}(\\dot{m}, \\Delta T) = C_p \\dot{m} \\Delta T\\)\n\\(\\displaystyle \\Partial{\\dot{Q}}{\\dot m} = C_p \\Delta T\\) and \\(\\displaystyle \\Partial{\\dot{Q}}{\\Delta T} = C_p \\dot{m}\\), so\n\\(\\displaystyle u_{\\dot Q} \\approx C_p \\sqrt{ (\\Delta T)^2 u^2_{\\dot m} + \\dot{m}^2 u_{\\Delta T}^2 }\\)\n\n\n\nIf we estimate the uncertainty in measured temperatures to be \\(1\\) degree C and the uncertainty in flow rate to be \\(0.5\\) liters per minute (i.e., 0.0083333 L/sec), then we can compute the uncertainties in the \\(\\dot{Q}\\) values as follows\n\nHeatX2<- HeatX2 |>\n  mutate(\n    u.Q.cold = C_p *sqrt( (T.cold.out - T.cold.in)^2 * (0.5/60)^2  + m.cold^2 * 2),  \n    u.Q.hot =  C_p *sqrt( (T.hot.out  - T.hot.in)^2  * (0.5/60)^2  +  m.hot^2 * 2) \n  ) \nHeatX2\n\n\n\n  \n\n\n\nNote: We are using the fact that \\(u_{\\Delta T} = \\sqrt{2} u_T\\).\nWe can also use deltaMethod() in the car package to do this arithmetic for us. In fact, deltaMathod() can even handle cases where the estimates are not independent. To do it’s job, deltaMethod() requires\n\n\nA named vector of estimates,\n\n\nestimates <- HeatX[1, 2:7] |> unlist()  # unlist() turns the data frame into a vector\nestimates\n\n T.cold.in T.cold.out   T.hot.in  T.hot.out     m.cold      m.hot \n14.3000000 18.6000000 38.2000000 33.9000000  0.1666667  0.1666667 \n\n\n\nAn expression for the derived quantity as a quoted string (it will do the derivatives for us!),\n\n\nexprforQ <- \"(T.cold.out - T.cold.in) * C_p * m.cold\"\n\n\nA variance-covariance matrix. In the simple case where things are independent, this is just a matrix with the squared uncertainties on the diagonal and 0 everywhere else.\n\n\nvc <- diag( c(1,1,1,1,.5/60,.5/60)^2)\nvc\n\n     [,1] [,2] [,3] [,4]         [,5]         [,6]\n[1,]    1    0    0    0 0.000000e+00 0.000000e+00\n[2,]    0    1    0    0 0.000000e+00 0.000000e+00\n[3,]    0    0    1    0 0.000000e+00 0.000000e+00\n[4,]    0    0    0    1 0.000000e+00 0.000000e+00\n[5,]    0    0    0    0 6.944444e-05 0.000000e+00\n[6,]    0    0    0    0 0.000000e+00 6.944444e-05\n\n\n\n\nLet’s see how it compares to the (first row of the) results above:\n\nlibrary(car)\ndeltaMethod(estimates, exprforQ, vc)\n\n\n\n  \n\n\n\nThe deltaMethod package adds additional interfaces for deltaMethod() to make this even easier.\n\nlibrary(deltaMethod)\ndeltaMethod(HeatX, exprforQ, \n  uncertainties = c(T.cold.in = 1.0, T.cold.out = 1.0, m.cold = 0.5 / 60))\n\nConverting uncertainties to a covariance matrix assuming independence ...\n\n\n\n\n  \n\n\n\n\n10.1.5.1 Dealing with uncertainties that are not constant\nSometimes the uncertainties of a given variable are different at different values. In that case, we can put the uncertainties into the data frame. For the current example, this is just extra work, but we include it here to show how it works.\n\nHeatX3 <- HeatX |>\n  mutate(\n    u.cold.in = 1, u.cold.out = 1, u.hot.in = 1, u.hot.out = 1, \n    u.m.cold = 0.5/60, u.m.hot = 0.5/60\n  )\nHeatX3\n\n\n\n  \n\n\ndeltaMethod(HeatX3,\n  exprforQ, \n  estimates     = c(\"T.cold.in\", \"T.cold.out\", \"m.cold\"),    # columns with estimates\n  uncertainties = c(\"u.cold.in\", \"u.cold.out\", \"u.m.cold\"))  # and uncertainties\n\nConverting uncertainties to a covariance matrix assuming independence ...\n\n\n\n\n  \n\n\n\nAlthough our uncertainties are the same in each row, this method allows for uncertainties to be specified separately for each row of the data.\n\n\n\n10.1.6 Does the uncertainty in \\(C_p\\) matter?\nIn the analysis above, we treated \\(C_p\\) as a constant (with arbitrary precision).\nBut this number is also known experimentally and has an uncertainty associated with it. Do our results change if we include this uncertainty in our calculations?\n\ndeltaMethod( HeatX |> mutate(C_p = 4.18), exprforQ,      # add in a column for C_p\n  uncertainties = c(T.cold.in = 1.0, T.cold.out = 1.0, m.cold = 0.5/60, C_p = 0.1))\n\nConverting uncertainties to a covariance matrix assuming independence ...\n\n\n\n\n  \n\n\n\nComparing these results to the results above we see that the uncertainty increases, but only by amounts that are barely visible (a few parts per 1,000 – not enough to affect how we report the uncertainty given our rules for reporting digits). This has no meaningful impact on our analysis.\nWe can often simplify propagation of uncertainty calculations by identifying which components of the uncertainty are driving the size of the overall uncertainty. In this case, the imprecision in the estimated value of \\(C_p\\) is unimportant. If we want to improve our uncertainty, we must improve our measurements of temperature and/or flow rate.\n\n\n10.1.7 Using relative uncertainty\nRecall that for products, there is a Pythagorean relationship for relative uncertainties.\n\\[\n\\frac{u_{\\dot Q}}{\\dot Q} \\approx\n\\sqrt{\n(\\frac{u_{\\dot m}}{\\dot{m}})^2\n+\n(\\frac{u_{\\Delta T}}{\\Delta T})^2\n+\n(\\frac{u_{C_p}}{C_p})^2\n}\n\\] This is useful in determining what uncertainties contribute most to the overall uncertainty. For example, using the values in the first row of the data set.\n\n\n\\(\\displaystyle \\frac{u_{\\dot{m}}}{\\dot{m}} = \\frac{0.5/60}{10/60} = 0.05\\)\n\\(\\displaystyle \\frac{u_{\\Delta T}}{\\Delta T} = \\frac{\\sqrt{2}}{4.5} = 0.3142697\\)\n\\(\\displaystyle \\frac{u_{C_p}}{C_p} = \\frac{0.1}{4.18} = 0.0239234\\)\n\n\n\nFrom this we can see that it is the imprecise temperature measurements that are our biggest problem. Even if we eliminated the other uncertainties, we would still have a relative uncertainty of over \\(30\\)%. The details vary a bit from row to row, but the uncertainty in in temperature is our biggest obstacle. In addition to increasing the precision of our temperature sensors, we could also potentially improve things by designing a heat exchanger with more dramatic changes in temperature.\nSuppose, for example, we could estimate temperature with an uncertainty of 0.1 degrees (ten times better than we have been assuming). Then our uncertainties for \\(\\dot{Q}\\) would change pretty dramatically.\n\nHeatX |>\n  mutate(C_p = 4.18) |>  # add in a column for C_p\n  deltaMethod(\n    exprforQ,      \n    uncertainties = c(T.cold.in = 0.1, T.cold.out = 0.1, m.cold = 0.5/60, C_p = 0.1))\n\nConverting uncertainties to a covariance matrix assuming independence ...\n\n\n\n\n  \n\n\n\nBut if we improve the uncertainty in the mass flow rate by a factor of 10, it has only a modest impact on our uncertainty for \\(\\dot{Q}\\).\n\nHeatX |>\n  mutate(C_p = 4.18) |>  # add in a column for C_p\n  deltaMethod(\n    exprforQ,      \n    uncertainties = c(T.cold.in = 1.0, T.cold.out = 1.0, m.cold = 0.5/600, C_p = 0.1))\n\nConverting uncertainties to a covariance matrix assuming independence ...\n\n\n\n\n  \n\n\n\n\n\n10.1.8 Estimating the heat transfer coefficient, \\(U\\)\nNext, we estimate a heat transfer coefficient (\\(U\\)) for both streams,\n\\[\nU = \\frac{\\dot{Q}}{A \\Delta T_{lm}},\n\\]\nwhere\n\\[\n\\Delta T_{lm} \\equiv \\frac{(T_{1} - T_{4}) - (T_{2} - T_{3})}{\\log\\left( \\frac{T_{1} - T_{4}}{T_{2} - T_{3}} \\right)}.\n\\]\nWe can apply the same ideas to estimate the uncertainty in \\(U\\). This time, working it out by hand would be considerably more tedious because of the number of variables involved and the form of the expression to be differentiated. Fortunately, R is happy to take care of those details if we just specify the information needed.\n\nexprforU <- paste(\"C_p * m.cold * (T.cold.out - T.cold.in) /\",\n                  \"( pi * D *  L  * ((T.cold.out - T.hot.in) - (T.cold.in - T.hot.out)) /\",\n                  \" log ( (T.cold.out - T.hot.in) / (T.cold.in - T.hot.out) ) )\")\nHeatX4 <- HeatX |>\n  mutate(D = 0.0143, L = 1.626, C_p = 4.18)\nHeatX4\n\n\n\n  \n\n\ndeltaMethod(\n  HeatX4, \n  exprforU, \n  uncertainties = c(\n    T.cold.in = 1.0, T.cold.out = 1.0, m.cold = 0.5/60, \n    T.hot.in = 1.0, T.hot.out = 1.0, m.hot = 0.5/60, \n    C_p = 0.1, D = 0.0004, L = 0.0006),\n  constants = list(pi = pi)\n)\n\nConverting uncertainties to a covariance matrix assuming independence ...\n\n\n\n\n  \n\n\n\nNote the use of the constants argument here to specify the value of pi.\nWe could also have specified \\(C_p\\) this way if we decided to ignore the uncertainty in that value. But since it is no harder to include that uncertainty, we included it.\n\n\n10.1.9 Estimating the Nusselt number correlation, \\(a\\)\nFinally, we can estimate the parameter \\(a\\) in a Nusselt number correlation for turbulent flow (\\(Re_{D} > 2300\\)):\n\\[\n    a = \\frac{Nu_D}{Re_{D}^{0.8} Pr^{1/3}},\n\\]\nwhere\n\\[\n    Nu_{D} = \\frac{h D}{k} \\approx \\frac{ 2 U D}{k},\n\\]\n\\[\n    Re_{D} = \\frac{4 \\dot{m}}{\\pi D \\mu},\n\\]\nand\n\\[\n    Pr = \\frac{\\mu C_{p}}{k}.\n\\]\nThe uncertainty in the estimates for \\(a\\) can be estimated in a similar manner. For that purpose, we assume that the exponents on \\(Re_{D}\\) and \\(Pr\\) (0.8 and 1/3, respectively) are constant. The values of \\(C_{p}\\), \\(k\\), and \\(\\mu\\) are given in Table 10.1.\nTo complete our uncertainty calculation we must\n\n\nCreate an expression for the quantity we are interested in (as a quoted string).\nCreate a data frame that has all of the components of this expression that have uncertainties.\nCreate a named vector of uncertainties. The names should correspond to the variables names in the data frame, the values are the uncertainties. (Alternatively, the uncertainties can be put inside the data frame and the arguments estimtates and uncertainties can be used to specify which columns are the estimates and which are the corresponding uncertainties.)\nCreate a list that contains the value of any constants (or values for which we are ignoring that there is uncertainty because the uncertainty is so small that it doesn’t affect the analysis).\n\n\n\nThis is left as an exercise for you."
  },
  {
    "objectID": "10-more-examples.html#standard-errors-in-fitdistr-output",
    "href": "10-more-examples.html#standard-errors-in-fitdistr-output",
    "title": "10  More Examples",
    "section": "10.2 Standard Errors in fitdistr() output",
    "text": "10.2 Standard Errors in fitdistr() output\nWe had not yet learned about uncertainty and standard errors when we learned about fitdistr(), but the output from the function includes an estimated standard error.\n\nlibrary(fastR2)\nfitdistr(Jordan8687$points, \"normal\")\n\n      mean          sd    \n  37.0853659    9.8639541 \n ( 1.0892915) ( 0.7702454)\n\n\nThe parenthesized numbers are the estimated standard errors associated with each parameter estimate. In this case, it is easy to compute the standard error for the mean ourselves using \\(SE = s / \\sqrt{n}\\).\n\nsd( ~ points, data = Jordan8687) / sqrt(82)\n\n[1] 1.095995"
  },
  {
    "objectID": "10-more-examples.html#sec-r-squared",
    "href": "10-more-examples.html#sec-r-squared",
    "title": "10  More Examples",
    "section": "10.3 \\(R^2\\)",
    "text": "10.3 \\(R^2\\)\n\nNote: the methods in this section assume a linear model with an intercept term.\n\n\nRecall that linear models are fit by minimizing the sum of the squares of the residuals:% \\[\nRSS = SSE = \\sum_{i = 1}^{n} ( y_i - \\hat y)^2\n\\] This expression reminds us of3 \\[\nSST = \\sum_{i = 1}^{n} ( y_i - \\mean y)^2 \\;,\n\\] the numerator of the variance of \\(y\\). \\(SST\\) is a measure of the total variation in the response variable.\nA little algebra shows that4 \\[\nSSM = SST - SSE =\n\\sum_{i = 1}^{n} ( \\hat y - \\mean y)^2 \\;,\n\\] We can now define \\(R^2\\) by \\[\nR^2 = \\frac{SSM}{SST}  = \\frac{SSM}{ SSM + SSE }\n= \\mbox{proportion of variability in the response explained by the model.}\n\\] \\(R^2\\) is always between 0 and 1 and is reported in the summary output for linear models. It is the square of the correlation coefficient that we saw earlier.\n\nsummary(lm(sat ~ expend, data = SAT))  # how does the average SAT score depend on money spent?\n\n\nCall:\nlm(formula = sat ~ expend, data = SAT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-145.074  -46.821    4.087   40.034  128.489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1089.294     44.390  24.539  < 2e-16 ***\nexpend       -20.892      7.328  -2.851  0.00641 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 69.91 on 48 degrees of freedom\nMultiple R-squared:  0.1448,    Adjusted R-squared:  0.127 \nF-statistic: 8.128 on 1 and 48 DF,  p-value: 0.006408\n\n\nNotice that \\(R^2 < 15%\\) and that the coefficient on expend is negative – indicating that spending more is associated with worse scores on the SAT.\nOne reason for this is that in some states most college bound students take the SAT, but in other states, the ACT is more common so the pool of students taking the SAT is stronger. If we add frac – the fraction of students in a given state who took the SAT.\n\nsummary(lm(sat ~ expend + frac, data = SAT))\n\n\nCall:\nlm(formula = sat ~ expend + frac, data = SAT)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-88.400 -22.884   1.968  19.142  68.755 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 993.8317    21.8332  45.519  < 2e-16 ***\nexpend       12.2865     4.2243   2.909  0.00553 ** \nfrac         -2.8509     0.2151 -13.253  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.46 on 47 degrees of freedom\nMultiple R-squared:  0.8195,    Adjusted R-squared:  0.8118 \nF-statistic: 106.7 on 2 and 47 DF,  p-value: < 2.2e-16\n\n\nNotice how much larger \\(R^2\\) is now, and that the sign of the coefficient on expend is not positive – as we would have expected.\nIt is important to note that adding in the additional variable frac gives a very different impression of the effect of expend on sat. Most of our examples have dealt with one response to one predictor, but in many situations, this is too simplistic and the inclusion of multiple predictors in the model is required to understand their impact on the response."
  },
  {
    "objectID": "10-more-examples.html#exercises",
    "href": "10-more-examples.html#exercises",
    "title": "10  More Examples",
    "section": "10.4 Exercises",
    "text": "10.4 Exercises\n\nExercise 10.1 Give an estimate with uncertainty for the Nusselt number correlation using the data in HeatX used in this chapter."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anscombe, F. J. 1973. “Graphs in Statistical Analysis.”\nThe American Statistician 27 (1): 17.\n\n\nFisher, R. A. 1925. Statistical Methods for Research Workers.\nOliver & Boyd.\n\n\nMorgan, Eugene C., Matthew Lackner, Richard M. Vogel, and Laurie G.\nBaise. 2011. “Probability Distributions for Offshore Wind\nSpeeds.” Energy Conversion and Management 52 (1): 15–26.\nhttps://doi.org/10.1016/j.enconman.2010.06.015.\n\n\nSalsburg, D. 2001. The Lady Tasting Tea: How Statistics\nRevolutionized Science in the Twentieth Century. W.H. Freeman, New\nYork.\n\n\nStudent. 1908. “The Probable Error of a\nMean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. pub-aw:adr:\npub-aw."
  }
]