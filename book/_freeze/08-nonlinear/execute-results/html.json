{
  "hash": "ceb661ae1366a59c4e920b393512acfb",
  "result": {
    "markdown": "# Beyond Linear Regression\n\n\n\n\n\n\n\n\n## How big is your $R^2$?\n\nOne part of regression model diagnostics is to check the fitted model's $R^2$ value, which gives an \nindication of the proportion of the variance in the response that has been \"explained\" by the model.  \nA low value (closer to 0) means that data points are spread far around the best fit line; a high one \n(close to 1) means that data points are clustered very tightly around the line.  \nA model with a low $R^2$ value is not necessarily \"bad\" -- it may still provide helpful information \nabout a real relationship between your response and predictor. \nHowever, that relationship is very \"noisy,\" which means that your model will have poor predictive power -- \nit will be unable to make predictions with the accuracy and precision you might hope for.\n\nOften, the predictive power of a model, and the $R^2$ value, can be improved by adding additional \nexplanatory variables -- that is, fitting a model with more than one explanatory variable. \nIt could have two predictors, three, or as many as you can (sensibly) come up with. \nThis kind of model is called multiple regression. Mathematically, it means fitting a model of the form:\n\n$$\ny ~ \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 + \\beta_3*x_3 ...\n$$\n\nMultiple regression often makes sense when you are studying a complex process where there is most likely \"more than one thing going on.\"  For example, you might consider modelling population growth rates worldwide using a data set including a set of social, economic, health, and political indicators compiled using data from the World Health Organization and partner organizations. The dataset description is available online: <http://www.exploredata.net/Downloads/WHO-Data-Set>.  One idea might be to look for a linear relationship between per-capita income and population growth rate:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simplify some variable names\nnames(whodat)[10] <- \"PopGrowthRate\"\nnames(whodat)[6] <- \"IncomePerCapita\"\nnames(whodat)[7] <- \"FemaleSchoolEnrollment\"\ngf_point(PopGrowthRate ~ IncomePerCapita, data = whodat)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 24 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![Population growth rate and per capita income.](08-nonlinear_files/figure-html/fig-who-plot1-1.png){#fig-who-plot1 width=432}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwho.m1 <- lm(PopGrowthRate ~ IncomePerCapita, data = whodat)\nsummary(who.m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = PopGrowthRate ~ IncomePerCapita, data = whodat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.68051 -0.70998 -0.02006  0.70727  2.78944 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      1.657e+00  1.048e-01   15.81  < 2e-16 ***\nIncomePerCapita -2.867e-05  6.218e-06   -4.61 7.72e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.041 on 176 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.1077,\tAdjusted R-squared:  0.1027 \nF-statistic: 21.25 on 1 and 176 DF,  p-value: 7.723e-06\n```\n:::\n:::\n\n\n\n\nThe $R^2$ value of this model is very low. But that could be because, unsurprisingly, there are *many* factors contributing to population growth rate, not *just* income.  For example, what about education? Perhaps more-educated women have fewer children, lowering the population growth rate.  So we might want to model population growth rate as a function of *both* income and education.\n\nIn R, a multiple regression model can be fitted with a call to `lm()`.  We just\nadd additional predictors to the right hand side of the model formula, separated\nby $+$ signs. For the WHO example discussed above, for example, we could try:\n\n::: {.cell}\n\n```{.r .cell-code}\nwho.m2 <- lm(PopGrowthRate ~ IncomePerCapita + FemaleSchoolEnrollment, \n             data = whodat)\nsummary(who.m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = PopGrowthRate ~ IncomePerCapita + FemaleSchoolEnrollment, \n    data = whodat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.43374 -0.52979  0.06017  0.56619  2.48052 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             4.095e+00  3.626e-01  11.293  < 2e-16 ***\nIncomePerCapita        -1.102e-05  6.028e-06  -1.827   0.0694 .  \nFemaleSchoolEnrollment -3.105e-02  4.504e-03  -6.894 1.06e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9086 on 167 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.3101,\tAdjusted R-squared:  0.3018 \nF-statistic: 37.52 on 2 and 167 DF,  p-value: 3.475e-14\n```\n:::\n:::\n\n \n\nWe would need to follow up with our diagnostics to fully assess these two\nmodels, but comparison of the $R^2$ values immediately shows that $R^2$ is much\nhigher for the second model. In other words, the multiple regression has\nsucceeded in explaining more of the variance in population growth rates than the\nsimple linear regression with only one predictor.\n\n\n## Violations of Linear Regression Assumptions\n\nIn the previous chapter, we learned how to carry out regression diagnostics -- to check whether or not the assumptions of linear regression analysis were valid for a particular analysis.  If the assumptions are violated, then the conclusions (parameter estimates, but especially standard errors) will be incorrect, and the model results can not be trusted.  \n\nFor each type of violation, there are some fixes or modifications we can try in order to fit a valid, trustworthy model to our data and still draw reliable conclusions.  In this course, we will focus mainly on type of \"fix\":  applying transformations to linearize non-linear relationships, and allow us to apply linear regression to the transformed data.  This approach is covered in detail in the rest of this chapter.\n\nBefore beginning our detailed discussion of transformation, we will briefly discuss several other types of \"fixes\".  The mathematical foundations of these more complex models are essentially beyond the scope of this class, but you should understand when they might be useful (for example, if you see a certain type of pattern in residual diagnostic plots, which technique might help solve the problem?) and be able to implement them in R.\n\nThe table below provides an overview of various problems you might uncover as you do regression diagnostics, along with possible solutions.  Each entry in the table is covered in a bit more detail in the subsequent sections of this chapter.\n\n|\tAssumption     | Description of Problem | Options |\n|----------------|------------------------|--------------------------------------------------------------------------------|\n|\tLinearity      | Scatterplot (or residual plots) indicate nonlinear relationship | Transform explanatory and or response variables. Alternative: fit a non-linear model using the R function `nls()` | \n|\tIndependence of errors | ACF plot indicates strong dependence of errors over time (or space) | Fit an \"autoregressive\" model, where this relationship between subsequent or nearby measurements is expected and accounted for.  To do this in R, replace `lm(y$\\sim$x)` with something like `gls(y$\\sim$x, correlation = corAR1(form = $\\sim$1))`.  |\n| Normality of errors | Residual QQ plots indicates departure from normality | First check if other assumptions may also be violated, and try options listed there. If that fails, you may need to add additional predictor variables to your model; or to fit a generalized linear model, a more sophisticated type of regression that we will not cover in this course. |\n| Equal Variance (homeoskedasticity) | Variance of errors is not constant over the full range of response values, or over time | First, make sure that the linearity assumption is not violated. Next, if you have the option of including additional predictors in your model, it may be helpful.  Next, transforming the response variable may help. Finally, if none of those options provide a solution, you can fit a model with non-constant error variance. For example, if variance increases with fitted response values, you can replace `lm(y$\\sim$x)` with something like `nls(y$\\sim$x, weights = varPower())` |  \n\n## Non-Normal Errors\n\nSometimes, during diagnostics for a linear regression model, you will find that residual quantile-quantile plots indicate that linear regression residuals are far from normally distributed.  In this case, before trying to modify your model in any way, it is useful to check whether any *other* assumptions of the linear regression have *also* been violated.  If they have, it is worthwhile to try to deal with those problems first, and see if solving them makes the residuals more normal.\n\nIf non-normal residuals are the only apparent problem with a linear regression model, adding additional explanatory variables *might* help in some cases.  Most of the time, you would have to turn to a more sophisticated regression model called a generalized linear model (GLM).  Fitting GLMs is beyond the scope of this class, and you will not be asked to do it.\n\n## Non-Independence of Errors\n\nSometimes, regression diagnostics (particularly a plot of residuals as a function of time, or an ACF plot) will show that the residuals are not independent.  This happens most often when the predictor variable is a temporal or spatial one; data points collected at similar times, or similar locations, are often similar to each other rather than independent.\n\nWe will consider a simple example using the price of chicken over time (in constant dollars, adjusted for inflation over time).  It seems to make sense to try to predict the price of chicken as a function of time (it's been getting progressively cheaper for the last century or so):\n<!--  # chickn <- read.csv(\"http://www.calvin.edu/~sld33/data/chickn.csv\", header = TRUE) -->\n<!-- % # chickn <- read.csv(\"https://github.com/selva86/datasets/blob/master/chicken.csv\") |> -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchickn <- read.csv(\"https://sldr.netlify.app/data/chickn.csv\")\ngf_point(Price ~ Year, data = chickn ) |> gf_lm()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n```\n:::\n\n```{.r .cell-code}\nchick.mod <- lm(Price ~ Year, data = chickn)\nsummary(chick.mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Year, data = chickn)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.07 -20.23  -4.75  13.20  79.98 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 5792.6918   329.9948   17.55   <2e-16 ***\nYear          -2.9123     0.1685  -17.29   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.48 on 68 degrees of freedom\nMultiple R-squared:  0.8146,\tAdjusted R-squared:  0.8119 \nF-statistic: 298.8 on 1 and 68 DF,  p-value: < 2.2e-16\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/chickn-1.png){width=432}\n:::\n:::\n\n \n\nHowever, there seems to be a problem with non-independence of the residuals.  Price is not independent from year to year; if you know the price was a bit high one year, it's likely to remain so for the next several years:\n\n::: {.cell}\n\n```{.r .cell-code}\nacf(resid(chick.mod))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/chick-acf-1.png){width=432}\n:::\n:::\n\n\n\nThis is a big problem, because it tends to result in standard error estimates that are artificially small. In other words: we think we have estimated our slope and intercept parameters *much* more precisely than we really have, and would report falsely narrow confidence intervals.\nTo fix the problem, we can consider replacing our simple linear regression:\n$$\ny ~ \\beta_0 + \\beta_1x + \\epsilon\n$$\n(where $\\epsilon \\sim N(0, \\sigma)$) with a model that expects that subsequent residuals to depend on previous ones, so that the residual for the data point collected at time $t$ is:\n$$\ne_t = \\rho e_{t-1} + \\epsilon\n$$\n(where, still, $\\epsilon \\sim N(0, \\sigma)$; and $\\rho$ is a new parameter indicating how strong the dependence over time is.)  This is called an AR(1) process, or an auto-regressive process of order 1.  It can be fit easily in R using the function `gls()` instead of \\text{lm()}. `gls()` does \"generalized least-squares\" fitting, and is found in the package `nlme`.  The function call syntax illustrated in this example will work any time the explanatory variable is the time (or space) one that is causing the non-independence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nlme)\nchick.mod2 <- \n  gls(Price ~ Year, data = chickn, correlation = corAR1(form = ~ 1))\nsummary(chick.mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized least squares fit by REML\n  Model: Price ~ Year \n  Data: chickn \n       AIC      BIC    logLik\n  557.8043 566.6823 -274.9022\n\nCorrelation Structure: AR(1)\n Formula: ~1 \n Parameter estimate(s):\n      Phi \n0.9483234 \n\nCoefficients:\n               Value Std.Error   t-value p-value\n(Intercept) 4764.172 1600.4333  2.976802  0.0040\nYear          -2.387    0.8171 -2.921499  0.0047\n\n Correlation: \n     (Intr)\nYear -1    \n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.0593757 -0.5673406 -0.1571088  0.3360972  2.0991647 \n\nResidual standard error: 41.39414 \nDegrees of freedom: 70 total; 68 residual\n```\n:::\n:::\n\n\n\nIf you plot the residuals of this new model, and plot the ACF, you will see that the correlation coefficients *still have high vaules*.  However, in the new `gls()` fit, this correlation has now been taken into account in the standard errors (which are larger -- compare the coefficient tables to verify it), so it is OK now to trust the model parameter estimates and predictions.\n\n## Heteroscedasticity (Non-constant Error Variance)\n\nSometimes, model diagnostics for a linear regression indicate that variance of\nerrors is not constant over the full range of response values.  Often, it is the\ncase that the error variance grows larger as the predicted response value grows\nlarger, resulting in a \"trumpet-like\" shape in the plot of residuals versus\nfitted values.\n\nIf you spot this problem, first, make sure that the linearity assumption is not\nviolated. Next, if you have the option of including additional predictors in\nyour model, it may be helpful.  Next, transforming the response variable may\nhelp.  Specifically, a log or square-root transformation of the response\nvariable may be useful.  (See more details and examples later in this chapter,\nwhen transformations are discussed in detail.)\n\nFinally, if none of the previous options provide a solution, you can fit a model\nthat actually *expects* and accounts for non-constant error variance. We will\nnot cover this topic in any detail, but this brief example is included for your\nfuture reference (outside this class).  Example: if variance increases with\nfitted response values, you can fit an appropriate model with the `gls` function\nfrom the `nlme` package.  To do so, replace `lm(y ~ x)` with something like\n`nls(y ~ x, weights = varPower())`.\n\nHere is a brief example, using the `Ornstein` dataset from the `car` package.\nIt gives data on 248 Canadian companies, collected in the mid-1970s.  The\nvariable `assets` gives each company's assets in millions of dollars, and\n`interconnects` gives the number of director and executive positions that are\nshared with other firms.  A scatter plot shows that the richest companies have\nmany of these \"interlocks\", so we might model assets as a function of\ninterlocks...however, the residuals have non-constant variance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\ngf_point(assets ~ interlocks, data = Ornstein)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/Ornstein-1.png){width=432}\n:::\n\n```{.r .cell-code}\nom <- lm(assets ~ interlocks, data = Ornstein)\ngf_point(resid(om) ~ fitted(om))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/Ornstein-2.png){width=432}\n:::\n:::\n\n\n\nWe can try to correct for this problem by fitting a model that \"expects\" this non-constant variance, by using the function `gls` with the input `weights = varPower()`.  (There are many other ways to model non-constant error variance; this small example gives you just a taste, and for this course, you would not be expected to deal with any cases other than ones like this, where error variance increases with fitted values.)\n\n::: {.cell}\n\n```{.r .cell-code}\nom2 <- gls(assets ~ interlocks, data = Ornstein, weights = varPower())\n```\n:::\n\n \n\nAs with the non-independence case, if you plot the residuals for this model, you will see that they DO still have non-constant variance...but again, now it is OK because our model has taken it into account, and computed parameter estimates and standard errors appropriately.\n\n## Non-linear Relationships\n\nThe rest of this chapter will provide detailed information on how to deal with some non-linear relationships in regression.\n\nLinear regression assumes a linear relationship between predictor and response variables, but not all relationships between pairs of quantitative variables are linear.  There are two common ways to deal with\nnonlinear relationships:\n::: {.enumerate}\n\n#. Transform the data before beginning linear regression analysis, so that there *is* a linear relationship between the (transformed) variables.\n#. Fit a model that explicitly expects, and accounts for, the nonlinear relationship between the two variables. \n::: \n<!-- end enumerate -->\n\n\n## Transformations in Linear Regression\n\nThe applicability of linear models can be extended through the use of \nvarious transformations of the data.  \nThere are several reasons why one might consider a transformation of\nthe predictor or response (or both).\n\n::: {.itemize}\n\n#. To correspond to a theoretical model.\n\n\tSometimes we have *a priori* information that tells us what \n\tkind of non-linear relationship we should anticipate.  \n\tFor example, an experiment to estimate Planck's constant ($\\hbar$) using \n\tLED lights and a voltage meter is based on the relationship\n\t\n$$\n\tV_a = \\frac{\\hbar c }{e \\lambda} + k\n$$\nwhere $V_a$ is the activation voltage (the voltage at which the LED just begins to emit\nlight), $c$ is the speed of light, $e$ is the energy of an electron, $\\lambda$ is the \nfrequency of the light emitted, and $k$ is a constant that relates to the energy \nlosses inside the semiconductor’s p-n junction.\nIf we take $c$ and $e$ as known for now (in a fancier version we would work their\nuncertainties into this, too), we can design an experiment that measures $V_a$\nand $\\lambda$ for a number of different colors.\n\t\nA little algebra gives us\n<!--  This might lead us to fit a model with no intercept  -->\n<!--  (e.g., in R, lm(y ~ 0 + x)) after applying an inverse transformation -->\n<!--  to the predictor $m$: -->\n\n$$\n\tV_a = \\frac{\\hbar c }{e} \\cdot \\frac{1}{\\lambda} + k\n$$\nSo if we fit a model with $V_a$ as the response and $\\frac{1}{\\lambda}$ as the predictor,\nthen the slope and intercept of the resulting least squares regression line will \ngive us and estimate for $\\frac{\\hbar c }{e}$, from which we can solve for $\\hbar$.\n(Note: if we know uncertainties for $c$, for $e$, and for the slope, we can compute\nan estimated uncertainty for $\\hbar$ using our propagation of uncertainty methods.)\n\nTheory says that a scatter plot of $V_a$ and $1/\\lambda$ should form a straight line,\nso the the model we would fit would look something like\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(voltage ~ I(1/wavelength), data = mydata)\n```\n:::\n\n\n\nWe need to wrap `1/lambda` in `I()` because the arithmetic symbols \n(`+`, `-`, `*`, `/`, and  `^`)\nhave special meanings inside the formula for a model. \n`I()` stands for *inhibit* special interpretation.\n\nNotice that the intercpet is not directly involved in estimating $\\hbar$, but that we can't fit\nthe line and obtain our slope without it.\n\t\n\tMany non-linear relationships can be transformed to linearity.  \n\t@exr-transformations presents several more examples\n\tand asks you to determine a suitable transformation.\n\n#.  To obtain a better fit.\n\n\tIf a scatterplot or residual plot shows a clearly non-linear pattern \n\tto the data, then it would be inappropriate to fit a linear regression (and conclusions drawn from that model would be incorrect and misleading).  \n\tIn the absence of theoretical reasons to expect a particular mathematical relationship between the variables being studied, we may select \n\ttransformations based on the shape of the relationship as \n\trevealed in a scatterplot.  @sec-ladder-of-re-expression\n\tprovides some guidance for selecting transformations in\n\tthis situation.\n\n#.  To obtain better residual behavior.\n\n\tSometimes transformations are used to improve the agreement between the\n\tdata and the assumptions about the error terms in the model.  For example,\n\tif the variance in the response appears to increases as the predictor \n\tincreases, a logarithmic or square root transformation of the response may \n\tdecrease the disparity in variance.  \n\tSome transformations are used to improve the agreement between the\n\tdata and the assumptions about the error terms in the model. \n\tFor example, if data are heteroscedastic -- for example, \n\tif the variance in the response appears to increase as the predictor increases -- \n\ta logarithmic or square root transformation of the response may help.  \n::: \n<!-- end itemize -->\n\n\nIn practice, all three of these issues are intertwined.  A transformation that\nimproves the fit, for example, may or may not have a good theoretical \ninterpretation.  \nSimilarly, a transformation performed to achieve **homoskedasticity** \n\\myindex{heteroskedasticity}<!--  -->\n\\myindex{homoskedasticity}<!--  -->\n(equal variance; the opposite is called **heteroskedasticity**) \nmay result in a fit that does not match the overall shape of the data very well.\nDespite these potential problems, there are many situations where a relatively\nsimple transformation is all that is needed to greatly improve the model.  Here, when we say \"improve\" the model, we mean that the assumptions of the model are satisfied, and the model fits the data acceptably well.\n\n### Three Important Laws\nIn the sciences, relationships between variables based on some scientific theory\nare often referred to as \"laws\".  Many of these fall into one of three categories that\nare easily handled by transforming the data and fitting a linear regression model.\n\n#### Linear Laws\nWe've already talked about linear relationships, but it is worth mentioning them again because there\nare so many situations in which a linear relationship arises.\n\n#### Power Laws\n\nRelationships of the form \n$$ \ny = A x^p\n$$\nare often called power laws.  The two parameters are the exponent $p$ and a constant of proportionality $A$.\nPower laws can be linearized by taking logarithms:\n$$ \n\\log(y) = \\log(A x^p) = \\log(A) + p \\log(x)\n$$\nSo if we fit a model of the form\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(log(y) ~ log(x))\n```\n:::\n\n\nThen $\\beta_0 = \\log(A)$ and $\\beta_1 = p$.  \nIf a power law is a good fit for the data then\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point( log(y) ~ log(x) )\n```\n:::\n\n\nwill produce a roughly linear plot.\n\nFitting a power law results in estimates for the parameters $\\beta_0 = \\log(A)$ and $\\beta_1 = p$.\nNote that we can use logarithms with any base for this transformation.  Typically natural logarithms are used\n(that's what `log()` does in R).  \nIn some specific applications we might use base 10 logarithms (`log10()` in R) \nor base 2 logarithms (`log2()` in R); \nthis yields the commonly used scale for \n$\\beta_0 = \\log(A)$, the constant of proportionality.\n\nSome common situations that are modeled with power laws include drag force vs speed, \nvelocity vs. force, and frequency vs. force.\n\n#### Exponential Laws\n\nRelationships of the form \n$$ \ny = A B^x = A e^{Cx}\n$$\nare often called exponential laws.  \nThe two parameters are the base $B = e^C$ and a constant of proportionality $A$.\nExponential laws can also be linearized by taking logarithms:\n$$ \n\\log(y) = \\log(A B^x) = \\log(A) + x \\log(B)\n$$\nSo if we fit a model of the form\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(log(y) ~ x)\n```\n:::\n\n\nThen $\\beta_0 = \\log(A)$ and $\\beta_1 = \\log(B) = C$.  \nIf an exponential law is a good fit for the data then\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point( log(y) ~ x )\n```\n:::\n\n\nwill produce a roughly linear plot.\n\nFitting an exponential law results in estimates for the parameters $\\beta_0 = \\log(A)$ and $\\beta_1 = \\log(B) = C$.\nAgain, we will generally use natural logarithms. In this course, if you see a `log()` without an indication of the base of the logartihm, you can assume it is base \"e\", a natural logarithm.  Similarly, remember that for R, the function `log()` takes the natural logarithm.\n\nSome common situations that are modeled with exponential laws include\npopulation growth and radioactive decay.  Note that exponential growth models\nare typically only good approximations over a limited range since exponential\nfunctions eventually grow quickly, and often some external constraints will\nlimit this growth.  For example, a culture of bacteria may grow roughly\nexponentially for a while, but eventually, limits on space and nourishment will\nmake it impossible for exponential growth to continue.\n\n#### Log-log and semi-log plots\n\nGraphs of $\\log(y)$ vs. $\\log(x)$ (log-log) or $\\log(y)$ vs $x$ (semi-log)\ncan be used to assess whether the power law or exponential law appears to apply\nin a given situation.  If the law were a perfect description of the situation,\nall the points on the log-log or semi-log plot would fall along a straight line.\nIn practice, the fit won't be perfect, but the plot is a useful diagnostic. \nFor example, you can compare a plot of $y$ as a function of $x$ with a log-log or semi-log plot, \nand see which one shows the most linear relationship between the two variables.\n\nIn the old days, before computers could readily transform the data, special\ngraph paper was produced with semi-log or log-log scales to facilitate this\nsort of plot.  \n<!--  R can also create plots with transformed scales.   -->\n<!-- % Here are some -->\n<!--  examples using artifical data satisfying a power law (with exponent 1.5). -->\nR can easily create plots with transformed scales.  \nUse `gf_refine()` with input `scales_*_log10()` to your call \nto `gf_point()`, as detailed in the example below:\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:10\ny <- 3 * x^1.5\ngf_point(log(y) ~ log(x))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-13-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(log(y) ~ x)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-13-2.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(y ~ x) |>\n  gf_refine(scale_x_log10(), scale_y_log10())\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-13-3.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(y ~ x) |>\n  gf_refine(scale_y_log10())\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-13-4.png){width=432}\n:::\n:::\n\n\n\nAs expected, the log-log transformation makes things linear.  Of course, with real data, the \nfit won't be perfect like this.\n\n### Other Models That Can Be Transformed to Linear\n\nThe three laws above are not the only kinds of relationships that\ncan be transformed to linear.\n\n::: {.example}\n\nA chemical engineering text book suggest a law of the form\n$$\n\\log( - \\frac{dC}{dt} ) =  \\log(k) + \\alpha \\log(C)\n$$\nwhere $C$ is concentration and $t$ is time.\n\nThis is equivalent to \n$$\n\\begin{aligned}\n - \\frac{dC}{dt}  &=  k \\cdot C^\\alpha\n\\\\\n - \\int C^{-\\alpha} \\; dC  &=  \\int k \\;dt \n \\\\\n - \\frac{1}{1-\\alpha} C^{1-\\alpha} &=  k t + d \n \\\\\n \\frac{1}{\\beta} C^{-\\beta} & = k t + d\n \\\\\nC^{-\\beta} & = \\beta k t + \\beta d\n\\end{aligned}\n$$\n\nIf we know $\\beta = \\alpha - 1$ (i.e., if we know $\\alpha$), \nthen we can fit a linear model using\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(C^(-1/beta) ~ t)\n```\n:::\n\n\nThe intercept of such a model will be $\\beta d$ and the slope will be $\\beta k$,\nfrom which we can easily recover $d$ and $k$.\n\nAlternatively, if we know $d = 0$ (i.e., if we know that $C = 0$ when $t = 0$), \nthen we can use\n$$\n\\begin{aligned}\n\t\\log( C^{-\\beta} )  = -\\beta \\log(C) &= \\log(\\beta k t ) = \\log(\\beta k) + \\log t\n \\\\\n \\log(C) &= - \\frac{\\log(\\beta k)}{\\beta} - \\frac{1}{\\beta} \\log t\n\\end{aligned}\n$$\n\nNow if we fit a model of the form \n\n::: {.cell}\n\n```{.r .cell-code}\nlm(C ~ log(t))\n```\n:::\n\n\nthe intercept will be $\\frac{-\\log(\\beta k)}{\\beta}$ and the slope will be \n$\\frac{-1}{\\beta}$.  From this we can solve for $k$ and $\\beta$.\n::: \n<!-- end example -->\n\n\n::: {.example}\n\nContinuing the previous example, we will fit the following data \n\n::: {.cell}\n\n```{.r .cell-code}\nConcentration <- data.frame(\n  time = c(0, 50, 100, 150, 200, 250, 300),               # minutes\n  concentration = c(50, 38, 30.6, 25.6, 22.2, 19.5, 17.4) # mol/dm^3 * 10^3\n)\ngf_point(concentration ~ time, data = Concentration)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-16-1.png){width=432}\n:::\n:::\n\n\nunder the assumption that \n$\\alpha = 2$, so $\\beta = 1$.  In this case, our relationship becomes\n$$\n\\frac{1}{C}  = - k t - d \\;.\n$$\nWe can now fit a model and see how well it does.\n\n::: {.cell}\n\n```{.r .cell-code}\nconc.model <- lm(1/concentration ~ time, data = Concentration)\nsummary(conc.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = 1/concentration ~ time, data = Concentration)\n\nResiduals:\n         1          2          3          4          5          6          7 \n-1.175e-04 -4.144e-05  8.281e-05  2.259e-04 -3.128e-05 -3.398e-05 -8.447e-05 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.012e-02  8.762e-05   229.6 2.97e-11 ***\ntime        1.248e-04  4.860e-07   256.8 1.70e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0001286 on 5 degrees of freedom\nMultiple R-squared:  0.9999,\tAdjusted R-squared:  0.9999 \nF-statistic: 6.593e+04 on 1 and 5 DF,  p-value: 1.7e-11\n```\n:::\n\n```{.r .cell-code}\nconfint(conc.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   2.5 %       97.5 %\n(Intercept) 0.0198922974 0.0203427516\ntime        0.0001235447 0.0001260434\n```\n:::\n:::\n\n\nThis provides estimates for \nthe intercept $- \\beta d$ \nand the slope $- \\beta k$ \nof our model.\nWe can divide by $-\\beta$ to obtain estimates for $d$ and $k$.\n\nOf course, we should always look to see whether the fit is a good fit.\n<!--  mplot(conc.model, w = 1:2)  # residual plot and qq-plot of residuals -->\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(resid(conc.model) ~ fitted(conc.model))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-18-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ resid(conc.model)) |> gf_qqline()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The following aesthetics were dropped during statistical transformation: sample\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-18-2.png){width=432}\n:::\n:::\n\n\nNotice that these residuals are very small relative to the values for\nconcentration.  (We can see this from the vertical scale of the plot and \nalso from the small value for residual standard error in the summary output.)\nThe shape of the residual plot would be more disturbing if the\nmagnitudes were larger and if there were more data.  \nAs is, even if there is some systematic problem, it is roughly five orders of \nmagnitude smaller than our concentration measurements, which likely can't be \nmeasured to that degree of accuracy.\n\nIf we want to show the fit on top of the original data, we must remember to\nuntransform the response, since the model we fitted is a model for $1/C$, but we want to show a model for $C$:\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point( concentration ~ time, data = Concentration ) |>\ngf_line( 1/fitted(conc.model) ~ time, data = Concentration)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-19-1.png){width=432}\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n\n\n### The Ladder of Re-expression {#sec-ladder-of-re-expression}\n\nSometimes we have data for which there is no theory (yet) to suggest the form\nof a model.  In such a case, we may let the data help suggest a model.\nIf we find a model that fits well, we can return to the question of whether \nthere is an explanation for that type of model.\n\nIn the 1970s, Mosteller and Tukey introduced what they called\nthe **ladder of re-expression** and **bulge rules** \n@Tukey:1977:EDA,Mosteller:1977:DataAnalysis that can be used to \nsuggest an appropriate transformation to improve the fit when the \nrelationship between two variables ($x$ and $y$ in our examples) \nis monotonic and has a single bend.  \nTheir idea was to apply a power transformation to \n$x$ or $y$ or both -- that is, to work with $x^a$ and $y^b$ for\nan appropriate choice of $a$ and $b$.  Tukey called \nthis ordered list of transformations the *ladder of re-expression*.\nThe identity transformation has power~$1$.\nThe logarithmic transformation is a special case and is included in the \nlist associated with a power of $0$.  \nThe direction of the required transformation can be obtained\nfrom @fig-tukey-bulge and @tbl-ladder, which shows four bulge types,\nrepresented by the curves in each of the four quadrants.  \nA bulge can potentially be straightened by \napplying a transformation to one or both variables, moving up \nor down the ladder as indicated by the arrows.  More severe bulges\nrequire moving farther up or down the ladder.  \n<!-- \\authNote{ed: There is a standard ordering of quadrants in mathematics, so labeling -->\n<!-- isn't needed.  We could use directions (upper right), but I don't want to add  -->\n<!-- quadrant numbers to the figure.  Also in next example.  --- 2010-11-8}<!--  --> -->\nA curve bulging in the \nsame direction as the one in the first quadrant of @fig-tukey-bulge,\nfor example, might be straightened by moving up the ladder of transformations\nfor $x$ or $y$ (or both), while a curve like the one in the second quadrant, \nmight be straightened by moving up the ladder for $y$ or down the ladder for $x$.\n\n:::{.columns}\n\n::::{.column width=\"45%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![Tukey and Mosteller's buldging rule](08-nonlinear_files/figure-html/fig-tukey-bulge-1.png){#fig-tukey-bulge width=432}\n:::\n:::\n\n\n::::\n\n::::{.column width=\"45%\"}\n\n| power     | transformation                 |\n|-----------|--------------------------------|\n| $\\vdots$  | $\\vdots$                       |\n| $3$       | $x \\mapsto x^3$                |\n| $2$       | $x \\mapsto x^2$                |\n| $1$       | $x \\mapsto x$                  |\n| $\\frac12$ | $x \\mapsto \\sqrt{x}$           |\n| $0$       | $x \\mapsto \\log(x)$            |\n| $-1$      | $x \\mapsto \\frac1x$            |\n| $-2$      | $x \\mapsto \\frac{1}{x^2}$      |\n| $\\vdots$  | $\\vdots$                       |\n\n: Ladder of re-expression {#tbl-ladder}\n\n::::\n\nThis method focuses primarily on transformations designed to improve\nthe overall fit.  The resulting models may or may not have \na natural, or obvious, interpretation.  These transformations also affect the \nshape of the distributions of the explanatory and response variables\nand, more importantly, of the residuals from the linear model  \n(see @exr-tukey-bulge-skew).\nWhen several different transformations lead to reasonable linear fits, these other\nfactors may lead us to prefer one over another.\n\n::: {.example #exm-tukey-bulge}\n\n**Q.** \nThe scatterplot in @fig-tukey-bulge-example\nshows a curved relationship between\n$x$ and $y$.\nWhat transformations of $x$ and $y$ improve the linear fit?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A scatterplot illustrating a non-linear relationship between $x$ and $y$.](08-nonlinear_files/figure-html/fig-tukey-bulge-example-1.png){#fig-tukey-bulge-example width=432}\n:::\n:::\n\n\n\n**A.** \nThis type of bulge appears in quadrant IV of @fig-tukey-bulge,\nso we can hope to improve the fit by moving \nup the ladder for $x$ or down the ladder for $y$.\nAs we see in @fig-tukey-bulge-many, \nthe fit generally improves as we move down and to the right -- but not too\nfar, lest we over-correct.\nA $\\log$-transformation of the response ($a = 1$, $b = 0$) seems \nto be especially good in this case. Not only is the resulting relationship\nquite linear, but the residuals appear to have a better distribution as well.\n::: \n<!-- end example -->\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Using the ladder of re-expression to find a better fit.](08-nonlinear_files/figure-html/fig-tukey-bulge-many-1.png){#fig-tukey-bulge-many width=576}\n:::\n:::\n\n\n\n::: {.example #exm-balldrop}\n\nSome physics students conducted an experiment in which they dropped steel \nballs from various heights and recorded the time until the ball hit the \nfloor.   We begin by fitting a linear model to this data.\n\n::: {.cell}\n\n```{.r .cell-code}\nball.model <- lm(time ~ height, data = BallDrop)\nsummary(ball.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = time ~ height, data = BallDrop)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0200108 -0.0089383  0.0001623  0.0082016  0.0186519 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.190243   0.004303   44.21   <2e-16 ***\nheight      0.251841   0.005516   45.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01009 on 28 degrees of freedom\nMultiple R-squared:  0.9867,\tAdjusted R-squared:  0.9863 \nF-statistic:  2085 on 1 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\ngf_point(time ~ height, data = BallDrop) |> gf_lm()\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/balldrop-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(resid(ball.model) ~ fitted(ball.model))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/balldrop-2.png){width=432}\n:::\n:::\n\n\n\nAt first glance, the large value of $r^2$ and the reasonably good fit \nin the scatterplot might leave us satisfied that we have found a good model.\nBut a look at the residual plot \nreveals a clear curvilinear pattern in this data.  \nA knowledgeable physics student knows that\n(ignoring air resistance) the time should be proportional to the \n*square root* of the height.  \nThis transformation agrees with Tukey's ladder of re-expression, which suggests\nmoving down the ladder for `height` or up the ladder for `time`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nball.modelT <- lm(time ~ sqrt(height), data = BallDrop)\nsummary(ball.modelT)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = time ~ sqrt(height), data = BallDrop)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0087773 -0.0038851  0.0000571  0.0030558  0.0125552 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.016078   0.004084   3.937 0.000498 ***\nsqrt(height) 0.430803   0.004863  88.580  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.005225 on 28 degrees of freedom\nMultiple R-squared:  0.9964,\tAdjusted R-squared:  0.9963 \nF-statistic:  7846 on 1 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\ngf_point(time ~ sqrt(height), data = BallDrop) |>\n  gf_lm(time ~ sqrt(height), data = BallDrop) \n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/balldrop-transT-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(resid(ball.modelT) ~ fitted(ball.modelT))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/balldrop-transT-2.png){width=432}\n:::\n:::\n\n\nThis model does indeed fit better, but the residual plot indicates that\nthere may be some inaccuracy in the measurement of the height.  \nIn this experiment, the apparatus was set up once for each height and then several observations were made.  So any error in this set-up affected all time measurements for\nthat height in the same way.  This could explain why the residuals for \neach height are clustered the way they are since it violates the assumption\nthat the errors are *independent*.  (See @exm-balldrop-rm\nfor a simple attempt to deal with this problem.)\n::: \n<!-- end example -->\n\n\n<!-- It is also important to note that although $r^2$ is very large in this model, -->\n<!-- %it no longer has the usual interpretation because the model does not -->\n<!-- have an intercept term.  Exercise~@prob:r2NoIntercept explores  -->\n<!-- %this further. -->\n\n\n::: {.example #exm-balldrop-rm}\n\nOne simple way to deal with the lack of independence in the previous example\nis to average all the readings made at each height.  \n(This works reasonably well in our example because we have nearly\nequal numbers of observations at each height.)\nWe pay for this \ndata reduction in a loss of degrees of freedom, but it may be easier to \njustify that the errors in average times at each height are independent \n(if we believe that the errors in the height set-up are independent and \nnot systematic).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBallDropAvg <- BallDrop |>\n  group_by(height) |>\n  dplyr::summarize(time = mean(time))\n\nball.modelA <- lm(time ~ sqrt(height), data = BallDropAvg)\nsummary(ball.modelA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = time ~ sqrt(height), data = BallDropAvg)\n\nResiduals:\n         1          2          3          4          5          6 \n 0.0039552 -0.0040318  0.0028227 -0.0051717  0.0001571  0.0022686 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.016078   0.007404   2.172   0.0956 .  \nsqrt(height) 0.430803   0.008816  48.863 1.05e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.004236 on 4 degrees of freedom\nMultiple R-squared:  0.9983,\tAdjusted R-squared:  0.9979 \nF-statistic:  2388 on 1 and 4 DF,  p-value: 1.05e-06\n```\n:::\n\n```{.r .cell-code}\ngf_point(time~ height, data = BallDropAvg) |> gf_lm()\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/balldrop-avg-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(resid(ball.modelA) ~ fitted(ball.modelA))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/balldrop-avg-2.png){width=432}\n:::\n:::\n\n\nUsing a square root transformation on averaged `height`\nmeasurements in the `BallDrop` data gives a similar fit but\na very different residual plot.  The interpretation of this model\nis also different.\n\nNotice that the parameter estimates are essentially the same as in \nthe preceding example.  The estimate for $\\sigma$ has decreased some.\nThis makes sense since we are now estimating the variability in \n*averaged* measurements rather than in individual measurements.\n\nOf course, we've lost a lot of degrees of freedom, and as a result,\nthe standard error for our parameter estimate is about twice as \nlarge as before.  This might have been different; had the mean values\nfit especially well, our standard error might have been smaller despite\nthe reduced degrees of freedom.\n\nOne disadvantage of the data reduction is that it is hard to interpret\nthe residuals (because there are fewer of them).  \nAt first glance there appears to be a downward trend\nin the residuals, but this is largely driven by the fact that the largest\nresidual happened to be for the smallest fit.  \n::: \n<!-- end example -->\n\n\n\n::: {.example #exm-soap}\n\n**Q.** \nRex Boggs of Glenmore State High School in Rockhampton, Queensland, had an\ninteresting hypothesis about the rate at which bar soap is used in the shower.\nHe writes:\n\n>    I had a hypothesis that the daily weight of my bar of soap [in grams] \n\tin my shower wasn't a linear function, the reason being that the tiny \n\tlittle bar of soap at the end of its life seemed to hang around for just \n\tabout ever. I wanted to throw it out, but I felt I shouldn't do so until \n\tit became unusable. And that seemed to take weeks.\n\n> Also I had recently bought some digital kitchen scales and felt I needed\n    to use them to justify the cost. I hypothesized that the daily weight of\n    a bar of soap might be dependent upon surface area, and hence would be a\n    quadratic function \\dots .\n\n> The data ends at day 22. On day 23 the soap broke into two pieces and one\npiece went down the plughole. \n\nThe data indicate that although Rex showered daily, he failed to record the \nweight for some of the days.\n\nWhat do the data say in regard to Rex's hypothesis?\n\n**A.** \nRex's assumption that weight should be a (quadratic) function of time \ndoes not actually fit his intuition.  His intuition corresponds roughly to the \ndifferential equation\n$$\n\\Partial{t}{W} = k W^{2/3}\\,,\n$$\nfor some negative constant $k$ since the rate of change should be \nproportional to the surface area remaining.  \n(We are assuming that the bar shrinks in such a way \nthat its shape remains proportionally unaltered.)\nSolving this equation (by separation of variables) gives\n<!-- $$  -->\n<!-- \\log(W) = k t + C \\;. -->\n<!-- $$  -->\n$$\nW^{1/3} = k t + C\n\\;.\n$$\nWe can fit untransformed and transformed models \n(`weight^(1/3) ~ day`) to this data and compare.\n\n::: {.cell}\n\n```{.r .cell-code}\nsoap.model1 <- lm(weight ~ day, data = Soap)\nsummary(soap.model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = weight ~ day, data = Soap)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2436 -1.2950  0.3078  1.3942  5.5040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 123.1408     1.3822   89.09   <2e-16 ***\nday          -5.5748     0.1068  -52.19   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.949 on 13 degrees of freedom\nMultiple R-squared:  0.9953,\tAdjusted R-squared:  0.9949 \nF-statistic:  2724 on 1 and 13 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe scatterplot in @fig-soap \n(darker line) indicate that the untransformed model is already a good fit.^[For now, it suffices\nto know that larger values of $r^2$ generally indicate a better fit. We will discuss $r^2$ and what it measures \n@sec-r-squared.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsoap.model2 <- lm(I(weight^(1/3)) ~ day, data = Soap)\nsummary(soap.model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = I(weight^(1/3)) ~ day, data = Soap)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31107 -0.13666  0.01605  0.15044  0.20095 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.297706   0.083813   63.21  < 2e-16 ***\nday         -0.146980   0.006477  -22.69 7.67e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1788 on 13 degrees of freedom\nMultiple R-squared:  0.9754,\tAdjusted R-squared:  0.9735 \nF-statistic:   515 on 1 and 13 DF,  p-value: 7.669e-12\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparing untransformed (darker) and transformed  (lighter) fits to soap use data.](08-nonlinear_files/figure-html/fig-soap-1.png){#fig-soap width=432}\n:::\n:::\n\n\n\n<!--  -->\nThe transformed model in this case actually fits worse.\nThe higher value of $r^2$ for the untransformed model is an indication \nthat the untransformed model explains a larger proportion of the variance in soap weights.  It is left as an exercise for you to examine diagnostic plots of the model residuals in both cases; you should see that neither one looks markedly better than the other. (There is perhaps an issue with a small amount of non-independence, or correlation over time, of the residuals; we might expect that with data collected over time.  However, the dataset is so small that it is hard to tell for sure if the problem is real and worth worrying about.)  \n@fig-soap shows a scatterplot\nwith both fits.  \nThe data do not support Rex's assumption that a transformation\nis necessary.  \n<!--  We can also fit a quadratic model of the form \\verb!Weight ~ I(Day^2)!, -->\n<!-- % but this model is worse still.  Fitting a full quadratic model requires  -->\n<!--  two predictors (\\verb!Day! and \\verb!Day^2!) and so will have to wait -->\n<!-- % until our discussion of multiple linear regression.   -->\nThe scatterplot and especially the residual plots both show that the \nresiduals are mostly positive near the ends of the data and negative\nnear the center.  Part of this is driven by a flattening of the pattern\nof data points near the end of the measurement period.  Perhaps as the soap\nbecame very small, Rex used slightly less soap than when the soap was\nlarger. @exr-soap asks you to remove the last few observations\nand see how that affects the models.\n\nFinally, since a linear model appears to fit at least reasonably well\n(but see @exr-soap), \nwe can give a confidence interval for $\\beta_1$, \nthe mean amount of soap Rex uses each shower.\n<!-- \\authNoted{check regression diagnostics for soap models -- some cause for worry}<!--  --> -->\n<!-- \\authNote{in text reference for quote from Rex? --2010-11-27}<!--  --> -->\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(soap.model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %     97.5 %\n(Intercept) 120.154672 126.126895\nday          -5.805514  -5.344014\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n\n\n## Nonlinear Least Squares\n\nAnother approach to non-linear relationships is called **nonlinear least squares**\nor **nonlinear regression**.  In this approach, instead of attempting to transform\nthe relationship until it becomes linear, we fit a nonlinear function by minimizing the \nthe sum of the squared residuals relative to that (paramterized) nonlinear\nfunction (form).  That is, our model now becomes\n$$\ny = f(x) + \\varepsilon\n$$\nwhere $f$ may be any parameterized function.\n\nThe R function for fitting these\nmodels is `nls()`.  This function works much like `lm()`,\nbut there are some important differences:\n:::: {.enumerate}\n\na.  Because the model does not have to be linear, we have\n\t\t\tto use a more verbose description of the model.\n#.  Numerical optimization is used to fit the model, and the algorithm\n\t\t\tused needs to be given a reasonable starting point for its search.\n\t\t\tSpecifying this starting point simultaneously lets R know what the \n\t\t\tparameters of the model are.  (Each quantity with a starting value\n\t\t\tis considered a parameter, and the algorithm will adjust all the parameters\n\t\t\tlooking for the best fit -- i.e., the smallest MSE (and hence also\n\t\t\tthe smallest SSE and RMSE).\n:::: \n<!-- end enumerate -->\n\n\nLet's illustrate with an example.\n\n::: {.example #exm-balldrop}\n\nReturning to the ball dropping experiment, let's fit \n$$\n\\begin{aligned}\n\\texttt{time} &= \\alpha_0 + \\alpha_1 \\sqrt{\\texttt{height}}\n\\end{aligned}\n$$ {#eq-balldrop}\n\nusing nonlinear least squares.  \n\n::: {.cell}\n\n```{.r .cell-code}\nnls.model <- nls(time ~ alpha0 + alpha1 * sqrt(height), \n                  data = BallDrop, \n                  start = list(alpha0 = 0, alpha1 = 1))\n```\n:::\n\n\n\tNotice how the model formula compares with the formula in (@eq:balldrop).\n\tThe starting point for the algorithm is specified with \n\t`start = list(alpha0 = 0, alpha1 = 1)`, which also declares \n\tthe parameters to be fit.\n\n\tWe can obtain the coefficients of the fitted model with\n\n::: {.cell}\n\n```{.r .cell-code}\nnls.model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNonlinear regression model\n  model: time ~ alpha0 + alpha1 * sqrt(height)\n   data: BallDrop\n alpha0  alpha1 \n0.01608 0.43080 \n residual sum-of-squares: 0.0007645\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 2.112e-07\n```\n:::\n:::\n\n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(nls.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    alpha0     alpha1 \n0.01607833 0.43080348 \n```\n:::\n:::\n\n\nA more complete summary can be obtained by\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(nls.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFormula: time ~ alpha0 + alpha1 * sqrt(height)\n\nParameters:\n       Estimate Std. Error t value Pr(>|t|)    \nalpha0 0.016078   0.004084   3.937 0.000498 ***\nalpha1 0.430803   0.004863  88.580  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.005225 on 28 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 2.112e-07\n```\n:::\n:::\n\n\nWe can restrict our attention to the coefficients table with\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(summary(nls.model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Estimate  Std. Error   t value     Pr(>|t|)\nalpha0 0.01607833 0.004084015  3.936894 4.975519e-04\nalpha1 0.43080348 0.004863416 88.580433 7.732182e-36\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun(nls.model)\ngf_point(time ~ height, data = BallDrop) |>\n  gf_fun(f(height) ~ height, color = 'gray40')\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-29-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(resid(nls.model) ~ fitted(nls.model))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-29-2.png){width=432}\n:::\n:::\n\n\n\nWe can compare this to the ordinary least squares model by plotting both together on the same plot.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.model <- lm(time ~ sqrt(height), data = BallDrop)\ng <- makeFun(lm.model)\ngf_point(time ~ height, data = BallDrop) |>\n  gf_fun(f(height) ~ height, color = 'gray80', size = 1.5) |>\n  gf_fun(g(height) ~ height, color = 'red', size = 0.5, linetype = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-30-1.png){width=432}\n:::\n:::\n\n\nIn this particular case, there is very little difference between the two models, but this is not always the case.\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(nls.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    alpha0     alpha1 \n0.01607833 0.43080348 \n```\n:::\n\n```{.r .cell-code}\ncoef(lm.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept) sqrt(height) \n  0.01607833   0.43080348 \n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n::: {.example #exm-balldrop-again}\n\nHere is example where we fit a different model to the `BallDrop` data, namely\n$$\n\t`time` = \\alpha * `height`^p\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\npower.model <- nls(time ~ alpha * height^power, data = BallDrop, \n                   start = c(alpha = 1, power = .5))\ncoef(summary(power.model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Estimate  Std. Error   t value     Pr(>|t|)\nalpha 0.4472102 0.001342627 333.08590 6.333427e-52\npower 0.4796679 0.005805313  82.62567 5.387914e-35\n```\n:::\n:::\n\n\n\nA power law can also be fit using `lm()` by using a log-log transformation.\n\n::: {.cell}\n\n```{.r .cell-code}\npower.model2 <- lm(log(time) ~ log(height), data = BallDrop)\ncoef(summary(power.model2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) -0.807610 0.004330482 -186.49426 7.101233e-45\nlog(height)  0.471911 0.006424548   73.45435 1.431476e-33\n```\n:::\n:::\n\n\nAgain, the parameter estimates (and uncertainties) are very similar.  Recall that to \ncompare our intercept in the second model to the $\\alpha$ value in the first model,\nwe must untransform:\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coef(power.model2)[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n  0.4459225 \n```\n:::\n:::\n\n\nWe can use the delta method to estimate the uncertainty.  Since \n$\\frac{d}{dx} e^x = e^x$ the uncertainty is approximately\n$$ \n0.4459225 \\cdot 0.0043305\n=\n0.0019311\n$$\n\n::: \n<!-- end example -->\n\n::: {.example #exm-looking-at-residuals}\n\nIn addition to comparing estimated parameters and their uncertainties, we should\nalways look at the residuals of our model.  For both the linear regression and \nthe nonlinear least squares models, the assumption is that the error terms are\nindependent, normally distributed, and have a common standard deviation.\nFrom the plots below we see\n\t\n:::: {.enumerate}\n\n#.  The nonlinear least squares model is a better match for these assumptions\n\t\t\tthan the linear regression model.\n#.  Both models reveal a lack of independence -- at a given height, the \n\t\t\tresiduals move up or down as a cluster as was discussed in the \n\t\t\tprevious section.  Neither model is designed to handle \n\t\t\tthis flaw in the design of the experiment.\n:::: \n<!-- end enumerate -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_qq( ~ resid(power.model), main = \"model 1\")\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-35-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ resid(power.model2))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-35-2.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(resid(power.model) ~ fitted(power.model))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-35-3.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(resid(power.model2) ~ fitted(power.model2))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-35-4.png){width=432}\n:::\n:::\n\n\n::: \n<!-- end example -->\n\nNow let's take a look at an example where we need the extra flexibilty\nof the nonlinear least squares approach.\n\n::: {.example #exm-hot-water}\n\nA professor at Macalester College put hot water in a mug and recorded the temperature as it cooled. \nLet's see if we can fit a reasonable model to this data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(temp ~ time, data = CoolingWater, ylab = \"temp (C)\", xlab = \"time (sec)\")\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-36-1.png){width=432}\n:::\n:::\n\n\n\nOur first guess might be some sort of exponential decay\n\n::: {.cell}\n\n```{.r .cell-code}\ncooling.model1 <- \n  nls(temp ~ A * exp( -k * time), data = CoolingWater, \n      start = list(A = 100, k = 0.1))\nf1 <- makeFun(cooling.model1)\ngf_point(temp ~ time, data = CoolingWater, xlim = c(-50, 300), ylim = c(0, 110), \n        ylab = \"temp (C)\", xlab = \"time (sec)\") |>\ngf_fun(f1(time) ~ time)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-37-1.png){width=432}\n:::\n:::\n\n\n\nThat doesn't fit very well, and there is a good reason.  The model says that eventually the water will freeze because\n$$\n\\lim_{t \\to \\infty} A e^{-k t} = 0\n$$\nwhen $k >0$.  But clearly our water isn't going to freeze sitting on a lab table.  We can fix this by \nadding in an offset to account for the ambient temperature:\n\n::: {.cell}\n\n```{.r .cell-code}\ncooling.model2 <- nls(temp ~ ambient + A * exp(k * (1+time)), data = CoolingWater,\n                      start = list(ambient = 20, A = 80, k = -.1) )\nf2 <- makeFun(cooling.model2)\ngf_point(temp ~ time, data = CoolingWater, xlim = c(-50, 300), ylim = c(0, 110),\n        ylab = \"temp (C)\", xlab = \"time (sec)\") |>\ngf_fun(f1(time) ~ time, linetype = 2, color = \"gray80\") |>\ngf_fun(f2(time) ~ time, color = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-38-1.png){width=432}\n:::\n:::\n\n\nThis fits much better.  Furthermore, this model can be derived from a differential equation\n$$\n\\frac{dT}{dt} = -k (T_0 - T_{\\mathrm{ambient}})\n\\;,\n$$\nknown as Newton's Law of Cooling.\n\nLet's take a look at the residual plot\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(resid(cooling.model2) ~ time, data = CoolingWater) \n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-39-1.png){width=432}\n:::\n\n```{.r .cell-code}\nplot(cooling.model2, which = 1)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-39-2.png){width=432}\n:::\n:::\n\n\nHmm.  These plots show a clear pattern and very little noise.\nThe fit doesn't look as good when viewed this way.  \nIt suggests that Newton's Law of Cooling does not take into account all that is going on here.\nIn particular, there is a considerable amount of evaporation (at least at the beginning when the \nwater is warmer).  More complicated models that take this into account can fit even better.\nFor a discussion of a model that includes evaporation, \nsee <http://stanwagon.com/public/EvaporationPortmannWagonMiER.pdf>.^[\nThe model with evaporation adds another complication in that the resulting\ndifferential equation cannot be solved algebraically, so there is no algebraic\nformula to fit with `nls()`.  \nBut the method of least squares can still be used by creating a parameterized\nnumerical function that computes the sum of squares and using a numerical\nminimizer to find the optimal parameter values.  Since the use of numerical\ndifferential equation solvers is a bit beyond the scope of this course, we'll\nleave that discussion for another day.]\n::: \n<!-- end example -->\n\n\n### Choosing Between Linear and Non-linear Models\n\nSo how do we choose between linear and non-linear models?  Let's enumerate some\nof the differences between them:\n\n::: {.enumerate}\n\n1. Some models cannot be expressed as linear models, even after transformations.\n\n    In this case we only have one option, the non-linear model.\n\n2. Linear models can be fit quickly and accurately without numerical\n\t\toptimization algorithms because they satisfy nice linear algebra\n\t\tproperties.\n\n    The use of numerical optimizers in non-linear least squares models\n\t\tmakes them subject to potential problems with the optimizers.  They may\n\t\tnot converge, may converge to the wrong thing, or convergence may depend\n\t\ton choosing an appropriate starting point for the search.\n\n3. The two types of models make different assumptions about the error terms.\n\n    In particular, when we apply transformations to achieve a linear model,\n\t\tthose transformations often affect the distribution of the error terms as \n\t\twell.  For example, if we apply a log-log transformation to fit a power law,\n\t\tthen the model is\n\t\t\n    $$\n    \\begin{aligned}\n\t\t\\log( y ) &= \\beta_0 + \\beta_1 \\log(x) + \\varepsilon\n\t\t\\\\\n\t\ty &= e^{\\beta_0}  x^{\\beta_1} e^\\varepsilon\n\t\t\\\\\n\t\ty &= \\alpha  x^{\\beta_1} e^\\varepsilon\n    \\end{aligned}\n    $$\n\n    So the errors are multiplicative rather than additive and\n\tthey have a normal distribution *after* applying the logarithmic\n\ttransformation.  This implies that the relative errors should be about\n\tthe same magnitude rather than the absolute errors.\n\t\n    This is potentially very different from the nonlinear\n\tmodel where the errors are additive:\n\t\n    $$\n\t    y = \\alpha x^\\beta + \\varepsilon\n    $$\n\n    Plots of residuals vs. fits and qq-plots for residuals can help us diagnose\n\t\twhether the assumptions of a model are reasonable for a particular \n\t\tdata set.\n\n4. Linear models provide an easy way to produce confidence intervals for \n\t\ta mean response or an individual response.\n\n    The models fit using `nls()` do not have this capability.\n::: \n<!-- end enumerate -->\n\n\n## Exercises\n\n::: {.problem}\n**Ball drop, revisited**\n\nIn @exm-balldrop, we applied a square root transformation\nto the height.  Is there another transformation that yields an even \nbetter fit?\n::: \n<!-- end problem -->\n\n::: {.problem #exr-soap}\n**Soap, revisited**\n\nRemove the last few days from the `Soap` data set and \nrefit the models in @exm-soap.\nHow much do things change?  Do the residuals look better, or \nis there still some cause for concern?\n::: \n<!-- end problem -->\n\n\n\n::: {.problem #exr-transformations}\n**Transformations**\n\nFor each of the following relationships between a response $y$ and an\nexplanatory variable $x$, \nif possible find a pair of transformations $f$ and $g$ so that\n$g(y)$ is a linear function of $f(x)$:\n$$\n  g(y) = \\beta_0 + \\beta_1 f(x) \\;.\n$$\nFor example, if \n\t  $y = a e^{bx}$, \n\t  then $\\log(y) = \\log(a) + bx$, so \n\t  $g(y) = \\log(y)$,\n\t  $f(x) = x$, \n\t  $\\beta_0= \\log(a)$, and \n\t  $\\beta_1 = b$.\n\n::::{.columns}\n:::::{.column width=\"50%\"}\na. $y = a b^x$.\na. $y = a x^b$.\na. $y = \\frac{1}{a + bx}$.\na. $y = \\frac{x}{a + bx}$.\n:::::\n\n:::::{.column width=\"50%\"}\ne. $y = a x^2 + b x + c$.\na. $\\displaystyle y = \\frac{1}{1+e^{a+bx}}$.\na. $\\displaystyle y = \\frac{100}{1+e^{a+bx}}$.\n:::::\n::::\n:::\n\n::: {.solution}\n\n:::: {.enumerate}\n\na. $\\log(y) = \\log(a) + x \\log(b)$, \n\tso $g(y) = \\log(y)$,\n\t$f(x) = x$, \n\t$\\beta_0 = \\log(a)$,\n\tand $\\beta_1 = \\log(b)$.\na. $\\log(y) = \\log(a) + b \\log(x)$, \n\tso $g(y) = \\log(y)$,\n\t$f(x) = \\log(x)$, \n\t$\\beta_0 = \\log(a)$,\n\tand $\\beta_1 = b$.\na. $\\frac{1}{y} = a + b x$, \n\tso $g(y) = \\frac{1}{y}$,\n\t$f(x) = x$, \n\t$\\beta_0 = a$,\n\tand $\\beta_1 = b$.\na. $\\frac{1}{y} = \\frac{a}{x} + b$, \n\tso $g(y) = \\frac{1}{y}$,\n\t$f(x) = \\frac{1}{x}$, \n\t$\\beta_0 = b$,\n\tand $\\beta_1 = a$.\na. not possible\na. $\\frac{1}{y} = 1 + e^{a + bx}$, so\n\t$\\log(\\frac{1}{y} - 1) = \\log( \\frac{1-y}{y} ) = {a + bx}$, \n\tso $g(y) = \\log(\\frac{1-y}{y})$,\n\t$f(x) = {x}$, \n\t$\\beta_0 = a$,\n\tand $\\beta_1 = b$.\na. $\\frac{100}{y} = 1 + e^{a + bx}$, so\n\t$\\log(\\frac{100}{y} - 1) = \\log( \\frac{100-y}{y} ) = {a + bx}$, \n\tso $g(y) = \\log(\\frac{100-y}{y})$,\n\t$f(x) = {x}$, \n\t$\\beta_0 = a$,\n\tand $\\beta_1 = b$.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n::: {.problem}\n**Errors and transformations**\n<!-- \\probNote{Would make a good partial solution for students.}% -->\n  What happens to the role of the error terms ($\\varepsilon$) when we transform \n  the data?  For each transformation from @exr-transformations,\n  start with the form\n$$\n  g(y) = \\beta_0 + \\beta_1 f(x) + \\varepsilon\n$$\n  and transform back into a form involving the untransformed $y$ and $x$ to\n  see how the error terms are involved in these transformed linear regression\n  models.\n\n  It is important to remember that when we fit a linear model to transformed\n  data, the usual assumptions of the model are that the errors in the (transformed) \n  linear form are additive and normally distributed.  The errors may appear\n  differently in the untransformed relationship.\n::: \n<!-- end problem -->\n\n\n::: {.problem #exr-tukey-bulge-skew}\n**Tukey bulge and skew**\n\nThe transformations in the ladder of re-expression also affects the shape\nof a distribution.  \n\n:::: {.enumerate}\n\na. If a distribution is symmetric, how does the shape change as we \nmove up the ladder?\na. If a distribution is symmetric, how does the shape change as we \nmove down the ladder?\na. If a distribution is left skewed, in what direction should we move to \nmake the distribution more symmetric?\na. If a distribution is right skewed, in what direction should we move to \nmake the distribution more symmetric?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nMoving up the ladder will spread the larger values more than the \nsmaller values, resulting in a distribution that is right skewed.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-pendulum}\n**Pendulum**\n\nBy attaching a heavy object to the end of a string, \nit is easy to construct pendulums of different lengths.  Some physics students\ndid this to see how the period (time in seconds until a pendulum\nreturns to the same location) depends on the length (in meters) \nof the pendulum.  \nThe students constructed pendulums of lengths varying from\n$10$ cm to $16$ m and recorded the period length (averaged over several\nswings of the pendulum).  The resulting data are in\nthe `Pendulum` data set in the **`fastR2`** package.\n\n:::: {.enumerate}\n\na. \n\t\tFit a power law to this data using a transformation and\n\t\ta linear model.  \n\t\tHow well does the power law fit?  \n\t\tWhat is the estimated power in the power law based on this model?\n#. \nFit a power law to this data using a nonlinear model.\nHow well does the power law fit?  \nWhat is the estimated power in the power law based on this model?\n\n#. \n\tCompare residual plots and normal-quantile plots for the residuals\n\tfor the two models.  How do the models compare in this regard?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\tAt first glance, the two models might appear equally good.  In each case the \n\tpower is a bit below 2 and the fits look good on top of the raw data.  \n\t<!--  (Note: the  -->\n\t<!--  function produced by `makeFun()` does not know how to invert the log -->\n\t<!--  transformation on the response variable, so we have to do that ourselves.) -->\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(log(period) ~ log(length), data = Pendulum)\nmodel2 <- nls(period ~ A * length^power, data = Pendulum, start = list(A = 1, power = 2))\nf <- makeFun(model)\ng <- makeFun(model2)\ngf_point(period ~ length, data = Pendulum) |>\n  gf_fun(f(x) ~ x, col = 'gray50')\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-40-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(period ~ length, data = Pendulum) |>\n  gf_fun(g(x) ~ x, col = 'red', lty = 2)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-40-2.png){width=432}\n:::\n\n```{.r .cell-code}\ncoef(summary(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Estimate  Std. Error  t value     Pr(>|t|)\n(Intercept) 0.7207055 0.006360981 113.3010 2.027047e-35\nlog(length) 0.4784757 0.003937676 121.5122 3.536212e-36\n```\n:::\n\n```{.r .cell-code}\ncoef(summary(model2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Estimate Std. Error  t value     Pr(>|t|)\nA     2.0469760 0.02202717 92.92961 2.843896e-33\npower 0.4827226 0.00482899 99.96348 4.610858e-34\n```\n:::\n:::\n\n\n\tBut if we look at the residuals, we see that the linear model is clearly \n\tbetter in this case.  The non-linear model suffers from heteroskedasticity.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(resid(model) ~ fitted(model)) |> gf_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess'\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-41-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(resid(model2) ~ fitted(model2)) |> gf_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess'\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-41-2.png){width=432}\n:::\n:::\n\n\n\tBoth residual distributions are reasonably close to normal, but not perfect.\n\tIn the ordinary least squares model, the largets few residuals are not as large\n\tas we would expect.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_qq( ~ resid(model))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-42-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ resid(model2))\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-42-2.png){width=432}\n:::\n:::\n\n\n\t\n\tThe residuals in the non-linear model show a clear change \n\tin variance as the fitted value increases.  This is counteracted by the logarithmic\n\ttransformation of the explanatory variable.  (In other cases, the non-linaer model\n\tmight have the preferred residual distribution.)\n\t\nThe estimated power based on the linear model is $0.478 \\pm 0.004$.\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-pressure}\n**Vapor pressure**\n\nThe `pressure` data set contains \ndata on the relation between temperature in degrees Celsius and \nvapor pressure in millimeters (of mercury).\nWith temperature as the predictor and pressure as the response,\nuse transformations or nonlinear models as needed to obtain a good fit.\nMake a list of all the models you considered and explain\nhow you chose your best model.\nWhat does your model say about the relationship between \npressure and temperature?\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nUsing Tukey's buldge rules it is pretty easy to land at something like one of the following\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(log(pressure) ~ temperature, data = pressure)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-43-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(log(pressure) ~ log(temperature), data = pressure)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-43-2.png){width=432}\n:::\n:::\n\n\n\nNeither of these is pefect (although both are much more linear than the original untransformed relationship).\nBut if you think in degrees Kelvin instead, you might find a much better transformation.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(log(pressure) ~ (273.15 + temperature), data = pressure)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-44-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(log(pressure) ~ log(273.15 + temperature), data = pressure)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-44-2.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(log(pressure) ~ 1/(273.15 + temperature), data = pressure)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-44-3.png){width=432}\n:::\n:::\n\n\n\nAh, that last one looks quite good.  Let's try that one.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(log(pressure) ~ I(1/(273.15 + temperature)), data = pressure)\nmplot(model, 1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-45-1.png){width=432}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[2]]\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-45-2.png){width=432}\n:::\n:::\n\n\nThings still aren't perfect. There's a bit of a bow in the residual plot, but the size of the residuals is\nquite small relative to the scale of the log-of-pressure values:\n\n::: {.cell}\n\n```{.r .cell-code}\nrange( ~ resid(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.0745625  0.1517124\n```\n:::\n\n```{.r .cell-code}\nrange( ~ log(pressure), data = pressure)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -8.517193  6.692084\n```\n:::\n:::\n\n\nAlso the residual for the fourth observation is quite a bit larger than the rest.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-cornit}\n**Fertilizing corn**\n\nThe `cornnit` data set in the package **`faraway`** \ncontains data from a study investigating the relationship between \ncorn yield (bushels per acre) and nitrogen (pounds per acre) fertilizer \napplication in Wisconsin.\nUsing nitrogen as the predictor and corn yield as the response,\nuse transformations (if necessary) to obtain a good fit.\nMake a list of all the models you considered and explain\nhow you chose your best model.\n::: \n<!-- end problem -->\n\n\n::: {.problem #exr-act-gpa}\n**ACT and GPA**\n\nThe data set `ACTgpa` (in the **`fastR2`** package)\n<!--   <http://www.calvin.edu/~stob/courses/m344/S07/data/actgpanona.csv> -->\ncontains the ACT composite scores and GPAs of some randomly selected seniors \nat a Midwest liberal arts college.\n\n:::: {.enumerate}\n\na.  Give a 95% confidence interval for the mean ACT score \n\t  of seniors at this school.\nb.  Give a 95% confidence interval for the mean GPA \n\t  of seniors at this school.\nc.  Use the data to estimate with 95% confidence \n\t  the average GPA for all students who score 25 on the ACT.\nd.  Suppose you know a high school student who scored 30 on the ACT.\n\t  Estimate with 95% confidence his GPA as a senior in college.\ne.  Are there any reasons to be concerned about the analyses you\n\t  have just done?  Explain.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n:::: {.enumerate}\n\na.  We can build a confidence interval for the mean by fitting \n\t\t\ta model with only an intercept term.\n\n::: {.cell}\n\n```{.r .cell-code}\ngrades <- ACTgpa\nconfint(lm(ACT ~ 1, data = grades))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               2.5 %   97.5 %\n(Intercept) 24.24932 27.90453\n```\n:::\n:::\n\n\nBut this isn't the only way to do it.  Here are some other ways.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# here's another way to do it; but you don't need to know about it\nt.test(grades$ACT)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  grades$ACT\nt = 29.386, df = 25, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 24.24932 27.90453\nsample estimates:\nmean of x \n 26.07692 \n```\n:::\n\n```{.r .cell-code}\n# or you can do it by hand\nx.bar <- mean( ~ ACT, data = grades); x.bar\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 26.07692\n```\n:::\n\n```{.r .cell-code}\nn <- nrow(grades); n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 26\n```\n:::\n\n```{.r .cell-code}\nt.star <- qt(.975, df = n-1); t.star\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.059539\n```\n:::\n\n```{.r .cell-code}\nSE <- sd( ~ ACT, data = grades) / sqrt(n); SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.887387\n```\n:::\n\n```{.r .cell-code}\nME <- t.star * SE; ME\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.827608\n```\n:::\n:::\n\n\n\t\t\tSo the CI is $26.0769231 \\pm 1.8276077$.  Of course, that is too many digits, we \n\t\t\tshould do some rounding to\n\t\t\t$26.1 \\pm 1.8$.  \n\t\t\t\nb. \n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(lm(GPA ~ 1, data = grades))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               2.5 %   97.5 %\n(Intercept) 3.185408 3.578515\n```\n:::\n\n```{.r .cell-code}\n# this could also be done the other ways shown above.\n```\n:::\n\n\nc. \n\n::: {.cell}\n\n```{.r .cell-code}\ngrades.model <- lm(GPA ~ ACT, data = grades)\nf <- makeFun(grades.model)\nf(ACT = 25, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 3.288243 3.166694 3.409792\n```\n:::\n:::\n\n\n\t\t\nd. \n\n::: {.cell}\n\n```{.r .cell-code}\nf(ACT = 30, interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 3.723365 3.100775 4.345955\n```\n:::\n:::\n\n\ne. \n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(GPA ~ ACT, data = grades) |>\n  gf_lm(interval = \"confidence\") |>\n  gf_lm(interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-51-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(resid(grades.model) ~ fitted(grades.model)) |> gf_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess'\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-51-2.png){width=432}\n:::\n:::\n\n\n\t\t\tThere are no major concerns with the regression model. The\n\t\t\tresiduals look pretty good.  (There is perhaps a bit more variability\n\t\t\tin GPA for the lower ACT scores and if you said you were worried about\n\t\t\tthat, I would not argue.)  \n\n\t\t\tThe prediction intervals are very wide and hardly useful, however.\n\t\t\tIt's pretty hard to give a precise estimate for an individual\n\t\t\tperson -- there's just too much variability from preson to person, \n\t\t\teven among people with the same ACT score. \n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-drag1}\n**Drag force**\n\nIn the absence of air resistance, a dropped object will continue to accelerate\nas it falls.  But if there is air resistance, the situation is different.\nThe drag force due to air resistance depends on the velocity of an object\nand operates in the opposite direction of motion.  Thus as the object's velocity\nincreases, so does the drag force until it eventually equals the force\ndue to gravity.  At this point the net force is $0$ and the object \nceases to accelerate, remaining at a constant velocity called the \nterminal velocity.\n\nNow consider the following experiment to determine how terminal velocity\ndepends on the mass (and therefore on the downward force of gravity) of \nthe falling object.  A helium balloon is rigged with a small basket and \njust the right ballast to make it neutrally buoyant.  Mass is then added\nand the terminal velocity is calculated by measuring the time it takes to\nfall between two sensors once terminal velocity has been reached.\n\nThe `Drag` data set (in the **`fastR2`** package) contains the results \nof such an experiment\nconducted by some undergraduate physics students.  Mass is measured \nin grams and velocity in meters per second.  \n(The distance between the two sensors used for determining\nterminal velocity is given in the `height` variable.)\n\nBy fitting models to this data, determine which of the following \"drag laws\" matches the data best:\n\n:::: {.itemize}\n\na. Drag is proportional to velocity.\n#. Drag is proportional to the square of velocity.\n#. Drag is proportional to the square root of velocity.\n#. Drag is proportional to the logarithm of velocity.\n:::: \n<!-- end itemize -->\n\n::: \n<!-- end problem -->\n\n::: {.solution}\n\nThe best of these four models is a model that says drag is proportional\nto the square of velocity.\nGiven the design of the experiment, it makes the most sense to fit\nvelocity as a function of drag force.  Here are several ways we could \ndo the fit:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- lm(velocity^2 ~ force.drag, data = Drag)\nmodel2 <- lm(velocity ~ sqrt(force.drag), data = Drag)\nmodel3 <- lm(log(velocity) ~ log(force.drag), data = Drag)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(summary(model1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) -0.06227051 0.221930683 -0.2805854 7.804746e-01\nforce.drag   0.08399767 0.002320596 36.1965958 3.448978e-32\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(summary(model2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept)      -0.03585585 0.054832417 -0.653917 5.169073e-01\nsqrt(force.drag)  0.29097916 0.006806886 42.747762 5.239126e-35\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(summary(model3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)     -1.1622713 0.04539588 -25.60301 2.020737e-26\nlog(force.drag)  0.4744661 0.01241003  38.23248 4.103776e-33\n```\n:::\n:::\n\n\nNote that `model1`, `model2`, and `model3` are not \nequivalent, but they all tell roughly the same story.\n\n::: {.cell}\n\n```{.r .cell-code}\nf1 <- makeFun(model1)\nf2 <- makeFun(model2)\nf3 <- makeFun(model3)\ngf_point(velocity ~ force.drag, data = Drag) |>\n  gf_fun(sqrt(f1(x)) ~ x, alpha = .4) |>\n  gf_fun(f2(x) ~ x, alpha = .4, color = 'red') |>\n  gf_fun(f3(x) ~ x, alpha = .4, color = 'brown')\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/unnamed-chunk-55-1.png){width=432}\n:::\n:::\n\n\n\nThe fit for these models reveals some \npotential errors in the design of this experiment.  Separating out the data\nby the height used to determine velocity suggests that perhaps some of the \nvelocity measurements are not yet at terminal velocity.\nIn both groups, the velocities for the greatest drag forces are \nnot as fast as the pattern of the remaining data would lead us to expect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(velocity^2 ~ force.drag, data = Drag, color = ~ height)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/drag-plots-1.png){width=432}\n:::\n\n```{.r .cell-code}\nmplot(model1, w = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/drag-plots-2.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(log(velocity) ~ log(force.drag), data = Drag, color = ~ height)\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/drag-plots-3.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_point(velocity ~ force.drag, data = Drag, color = ~ height) |>\n  gf_refine(scale_x_log10(), scale_y_log10())\n```\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/drag-plots-4.png){width=432}\n:::\n\n```{.r .cell-code}\nmplot(model3, w = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](08-nonlinear_files/figure-html/drag-plots-5.png){width=432}\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-drag-problems}\n**Drag force, revisited**\n\nConstruct a plot that reveals a likely systematic problem with the \n`Drag` (see @exr-drag1) data set.\nSpeculate about a potential cause for this.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nSee previous problem.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-drag-subset}\n**Drag force, subsetting the data**\n\n@exr-drag-problems suggests that some\nof the data should be removed before analyzing the `Drag` data set.\nRedo @exr-drag1 after removing this data.\n::: \n<!-- end problem -->\n\n\n::: {.problem #exr-spheres-1}\n**Spheres**\n\nThe `Spheres` data set (in the **`fastR2`** package)\ncontains measurements of the diameter (in meters)\nand mass (in kilograms) of a set of steel ball bearings.  We would expect\nthe mass to be proportional to the cube of the diameter.  Fit a model \nand see if the data reflect this.\n::: \n<!-- end problem -->\n\n\n\n::: {.problem #exr-spheres-2}\n**More spheres**\n\nThe `Spheres` data set (in the **`fastR2`** package)\ncontains measurements of the diameter (in meters)\nand mass (in kilograms) of a set of steel ball bearings.  We would expect\nthe mass to be proportional to the cube of the diameter.  \nUsing appropriate transformations fit two models: one that predicts \nmass from diameter and one that predicts diameter from mass.  \nHow do the two models compare?\n::: \n<!-- end problem -->\n\n::: {.problem #exr-utilities-therms-by-temp}\n**Utilities**\n\nThe `Utilities` data set has information from utilities\nbills at a Minnesota residence.\nFit a linear model that predicts `thermsPerDay` from `temp`.\n\n:::: {.enumerate}\na. \nWhat observations should you remove from the data before doing the \nanalysis? Why?  \n#. \nAre any transformations needed?\n#. \nHow happy are you with the fit of your model?  Are there any reasons\nfor concern?\n#. \nInterpret your final model (even if it is with some reservations listed in\npart c)).  \nWhat does it say about the relationship between average monthly temperature \nand the amount of gas used at this residence?  What do the parameters represent?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n",
    "supporting": [
      "08-nonlinear_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}