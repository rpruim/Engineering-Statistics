{
  "hash": "f900853d591138e400fb2e2c8b506fd0",
  "result": {
    "markdown": "# Hypothesis Testing {#sec-hypothesis-testing}\n\n\n\n\n\n\n\nThis chapter is concerned with statistical hypothesis testing, and how we can use it to make inferences and draw conclusions from data about questions of scientific interest.\n\n## Experimental Design in Statistics\nBefore we begin to talk about hypothesis testing, let's review the general process of designing and carrying out a statistical experiment.\n\n::: {.enumerate}\n\n#. Determine the question of interest.\n\n    Just what is it we want to know?  It may take some effort to make a vague idea\nprecise.  The precise questions may not exactly correspond to our vague\nquestions, and the very exercise of stating the question precisely may modify\nour question.  Sometimes we cannot come up with any way to answer the question\nwe really want to answer, so we have to live with some other question that is\nnot exactly what we wanted but is something we can study and will (we hope) give\nus some information about our original question.\n\n\n#. Determine the **population**. \n\n    Just who or what do we want to know about?  For example, are we only interested\nin one specific person, or women in general, or all women, or all people?  Or,\nare we interested in the energy efficiency of one particular device, or all the\nmachines in a certain factory, or all machines of a certain type, or all\nmachines of a certain class, or all factories in a certain industry?\n\n#. Select **measurements**.\n\n    We are going to need some data. We get our data by making some measurements.\nThese might be physical measurements with some device (like a ruler or a scale).\nBut there are other sorts of measurements too, like the answer to a question on\na form. Sometimes it is tricky to figure out just what to measure. (How do we\nmeasure happiness or intelligence, for example?) Just how we do our measuring\nwill have important consequences for the subsequent statistical analysis. The\nrecorded values of these measurements are called\n**variables** (because the values vary from one individual to another).\n\n#. Determine the **sample**.\n\n    Usually we cannot measure every individual in our population; we have to select\nsome to measure. But how many and which ones? These are important questions that\nmust be answered. Generally speaking, bigger is better, but it is also more\nexpensive. Moreover, no size is large enough if the sample is selected\ninappropriately.\n\n    For example, if we wanted to draw conclusions about energy use across a whole\nindustry, we would have to be careful not to sample from just a single factory,\nor a single type of manufacturing device.  If we wanted to draw conclusions\nabout all people, we would have to be careful not to study only male college\nstudents.  The sample should be a random selection from the whole population (or\nas close as we can get to that standard).\n\n\n#. Make and record the measurements.\n\n    Once we have the design figured out, we have to do the legwork of data\ncollection.  This can be a time-consuming and tedious process. A study of public\nopinion may require many thousands of phone calls or personal interviews. In a\nlaboratory setting, each measurement might be the result of a carefully\nperformed laboratory experiment.\n\n#. Organize the data.\n\n    Once the data have been collected, it is often necessary or useful\n\tto organize them.  Data are typically stored in spreadsheets or \n\tin other formats that are convenient for processing with \n\tstatistical packages.  Very large data sets are often stored in \n\tdatabases.  \n\t\n\t  Part of the organization of the data may involve producing graphical and\n\tnumerical summaries of the data.  These summaries may give us initial\n\tinsights into our questions or help us detect errors that may have occurred\n\tto this point.\n\n#. Draw conclusions from data.\n\n    Once the data have been collected, organized, and analyzed, we need to reach a\nconclusion. What is the answer to our scientific question? Is our idea or\nhypothesis about the way things work incorrect, or do the data support it? How\nsure are we about these conclusions?\n\n#. Produce a report.\n\n    Typically the results of a statistical study are reported in \n\t\tsome manner.  This may be as a refereed article in an academic \n\t\tjournal, as an internal report to a company, or as a solution\n\t\tto a problem on a homework assignment.  These reports may themselves\n\t\tbe further distilled into press releases, newspaper articles,\n\t\tadvertisements, and the like.  The mark of a good report\n\t\tis that it provides the essential information about each \n\t\tof the steps of the study.\n\n::: \n<!-- end enumerate -->\n\n\nAt this point, you may be wondering who the innovative scientist was and \nwhat the results of the experiment were.\nThe scientist was R. A. Fisher, who first described this situation\nas a pedagogical example in his 1925 book on \nstatistical methodology @Fisher:1925:Methods.\nFisher developed statistical methods that are among the most\nimportant and widely used methods to this day, and most of his \napplications were biological.\n\\nocite{Fisher:1970:Methods}\n\n\n## Coins and Cups\n\nYou might also be curious about how the experiment came out.\nHow many cups of tea were prepared?  How many did the woman \ncorrectly identify?  What was the conclusion?\n\nFisher never says.  In his book he is interested in the method, not the \nparticular results.  But let's suppose we decide to test the lady with\nten cups of tea.  \nWe'll flip a coin to decide which way to prepare the cups.  \nIf we flip a head, we will pour the milk in first; if tails, we \nput the tea in first.\nThen we present the ten cups to the lady and have her state which ones she\nthinks were prepared each way.  \n\nIt is easy to give her a score (9 out of 10, or 7 out of 10, or whatever\nit happens to be).  It is trickier to figure out what to do with her score.\nEven if she is just guessing and has no idea, she could get lucky and \nget quite a few correct -- maybe even all 10.  But how likely is that?\n\nHere's one way we could find out.  Suppose I flip a coin ten times and record\nthe pattern of heads and tails.\nYour job is to guess the sequence of heads and tails. To make it more interesting,\nwe'll get a lot of other people to guess too and see how everyone does.\n\n$\\vdots$\n\nIf we compare all of the guessers, we will undoubtedly see that some \ndid better and others worse.\n\nNow let's suppose the lady gets 9 out of 10 correct.  That's not perfect,\nbut it is better than we would expect for someone who was just guessing.\nOn the other hand, it is not impossible to get 9 out of 10 just by guessing.\nSo here is Fisher's great idea:  Let's figure out how hard it is to get\n9 out of 10 by guessing.  If it's not so hard to do, then perhaps that's \njust what happened, so we won't be too impressed with the lady's tea tasting\nability.  On the other hand, if it is really unusual to get 9 out of 10 \ncorrect by guessing, then we will have some evidence that she must \nbe able to tell something.\n\nBut how do we figure out how unusual it is to get 9 out of 10 just by \nguessing?  We'll learn another method later, but for now, let's just \nflip a bunch of coins and keep track.  If the lady is just guessing, she \nmight as well be flipping a coin.\n\nSo here's the plan.  We'll flip 10 coins.  We'll call the heads correct \nguesses and the tails incorrect guesses.  Then we'll flip 10 more coins,\nand 10 more, and 10 more, and \\dots.  That would get pretty tedious.\nFortunately, computers are good at tedious things, so we'll let the computer \ndo the flipping for us using a tool in the **`mosaic`** package.\n\nThe `rflip()` function can flip one coin\n\n\n::: {.cell hash='Stat241-HypothesisTesting_cache/pdf/flip1coin_5e7f4be3633b388c86a9f967cc4a206d'}\n\n```{.r .cell-code}\nlibrary(mosaic)\nrflip()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFlipping 1 coin [ Prob(Heads) = 0.5 ] ...\n\nT\n\nNumber of Heads: 0 [Proportion Heads: 0]\n```\n:::\n:::\n\n\nor a number of coins\n\n\n::: {.cell hash='Stat241-HypothesisTesting_cache/pdf/flip10coins_41b8cec2354b44ff3e5c428b77fffb8f'}\n\n```{.r .cell-code}\nrflip(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFlipping 10 coins [ Prob(Heads) = 0.5 ] ...\n\nT H T H H T H T T T\n\nNumber of Heads: 4 [Proportion Heads: 0.4]\n```\n:::\n:::\n\n\nand show us the results.\n\nTyping `rflip(10)` a bunch of times is almost as tedious as \nflipping all those coins.   But it is not too hard to tell R to `do()` this a bunch of times.\n\n\n::: {.cell hash='Stat241-HypothesisTesting_cache/pdf/flip2_d74e4a63096f8c532f5ee72aa70ad044'}\n\n```{.r .cell-code}\ndo(2) * rflip(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   n heads tails prop\n1 10     3     7  0.3\n2 10     6     4  0.6\n```\n:::\n:::\n\n\nLet's get R to `do()` it for us 10,000 times and make a table and a histogram of the results.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell hash='Stat241-HypothesisTesting_cache/pdf/flip4_eb564e873f63bab95cdebd897e76840a'}\n\n```{.r .cell-code}\nRandomLadies <- do(10000) * rflip(10)\ngf_histogram( ~ heads, data = RandomLadies, binwidth = 1)  \n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/flip4-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntally( ~ heads, data = RandomLadies)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nheads\n   0    1    2    3    4    5    6    7    8    9   10 \n   5  102  467 1203 2048 2470 2035 1140  415  108    7 \n```\n:::\n\n```{.r .cell-code}\ntally( ~ heads, data = RandomLadies, format = 'percent')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nheads\n    0     1     2     3     4     5     6     7     8     9    10 \n 0.05  1.02  4.67 12.03 20.48 24.70 20.35 11.40  4.15  1.08  0.07 \n```\n:::\n\n```{.r .cell-code}\ntally( ~ heads, data = RandomLadies, format = 'proportion')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nheads\n     0      1      2      3      4      5      6      7      8      9     10 \n0.0005 0.0102 0.0467 0.1203 0.2048 0.2470 0.2035 0.1140 0.0415 0.0108 0.0007 \n```\n:::\n:::\n\n\n\nYou might be surprised to see that the number of correct guesses\nis exactly 5 (half of the 10 tries) only \n25%\nof the time.  But most of the results are quite close to 5 correct.\n67% of the results are \n4, 5, or 6, for example.\nAnd 90% of the results \nare  between 3 and 7 (inclusive).\nBut getting 8 correct is a bit unusual, and getting 9 or 10 correct is even \nmore unusual.  \n\nSo what do we conclude?  It is possible that the lady could get 9 or 10 correct\njust by guessing, but it is not very likely (it only happened in about\n1.2% of our simulations). \nSo *one of two things must be true*:\n\n::: {.itemize}\n\n#.  The lady got unusually \"lucky\", or \n#.  The lady is not just guessing.\n::: \n<!-- end itemize -->\n\n\nAlthough Fisher did not say how the experiment came out, others have reported\nthat the lady correctly identified all 10 cups!\n@salsburg\n\nThis same reasoning can be applied to answer a wide range of questions that\nhave a similar form.  For example, the question of whether dogs can smell\ncancer could be answered essentially the same way (although it would be a bit\nmore involved than preparing tea and presenting cups to the Lady).\n\n\n## A General Framework\n\nIn statistical hypothesis testing, we can follow the following general\nprocedure.  We usually begin with some idea about how the process we are\nstudying should work.  For example, we might have a hunch that highway bridges\nwith higher traffic flows are in poorer condition, and therefore merit more\nfrequent repairs.  For statistical hypothesis testing, we must translate that\n\"hunch\" into a *testable* **null hypothesis**, often called $H_0$:\none that can be demonstrated to be very unlikely in light of our data.  In the\ncase of the bridges, a testable null hypothesis might be that bridge condition\n*does not* depend on traffic flow.  Then, if our data shows a strong\napparent relationship between condition and traffic, we have evidence to\n*reject* the null hypothesis, and the data support the idea that there is\nsome relationship between condition and traffic.\n\nNull hypothesis are usually \"boring,\" no-result hypotheses:  there is no pattern; there is no relationship between the variables of interest; there is no difference between the two samples of interest.  If we can reject the null hypothesis in a certain case, we have some evidence -- but *NOT* proof -- that there *is* an interesting pattern in our data, and thus in the population we are trying to draw conclusions about. \n\nThe alternative to the null hypothesis is called the alternative hypothesis, often called $H_1$.  The alternative hypothesis, stated most generally, is usually some form of \"the null hypothesis is not true\" -- so there IS a pattern in the data, or a difference between the samples, etc.\n\n::: {.description}\n\n* [hypothesis] A statement that can be true or false.\n* [statistical hypothesis] A hypothesis about a parameter or parameters.\n::: \n<!-- end description -->\n\n\nIn our bridge example, a statistical null hypothesis $H_0$ might be: the true slope of the regression of condition as a function of traffic is 0.\n\n::: {.example #exm-null-hypotheses}\n\nThe following are examples of null hypotheses.\n\n:::: {.enumerate}\n\na.  $H_0: \\mu = 0$.  (The population mean is 0.)\n#.  $H_0: \\beta_1=0$.  (The \"true\" slope is 0 -- assuming a model like $\\E(Y) = \\beta_0 + \\beta_1 x$.)  \n#.  $H_0: \\beta_1 = \\beta_2$ (Two parameters in the model are equal.)\n#.  $H_0: \\beta_2 = \\beta_3 = 0$ (Two parameters in the model are both equal to 0.)\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end examples -->\n\n\n### The Four Step Process\n\nHypothesis testing generally follows a four-step process.\n\n\n#### 1. State the null ($H_0$) and alternative ($H_1$) hypotheses. {-}\n\nThe null hypothesis is on trial and innocent until proven guilty.  We will\n\t\trender one of two verdicts:  Reject the null hypothesis (guilty) or do not\n\t\treject the null hypothesis (not guilty).  *Important!  We can never **accept**\n\t\tor **prove** either hypothesis -- only reject the null (because it doesn't \n\t\tseem compatible with the data), or fail to\n\t\treject the null (since it appears plausibly compatible with teh data).\n\n#### 2. Compute a test statistic. {-}\n\nFor a statistical hypothesis test, all the evidence against the null\n\t\thypothesis must be summarized in a single number called the test statistic.\n\t\tIt is a statistic because it is a number computed from the data.  It is called\n\t\ta test statistic because we are using it to do hypothesis testing.\n\n#### 3. Determine the p-value. {-}\n\nThe p-value is a probability:  *Assuming* the null hypothesis is true, how\n\t\tlikely are we to get at least as much evidence against it as we have in our\n\t\tdata (i.e., *a test statistic at least as unusual as the one observed*) just\n\t\tby random chance?\n\n### 4. Interpret the results. {-}\n\nIf the p-value is small, then one of two things is true:\n\t\t\na. The null hypothesis is true and something very unlikely occurred\n\t\t\t in our sample, or \nb. The null hypothesis is false, so it is unsurprising that our observed\n    data yield statistics that seem unlikely based on that (incorrect, untrue)\n    hypothesis.\n\nFor this reason we consider small p-values to provide evidence against the null\nhypothesis.\n\n\n### The Lady Tasting Tea, Revisted\n\n::: {.example #exm-lady-binom}\n\nFor the lady tasting tea, this process looks like\n\n##### 1. State Hypotheses\n\n* $H_0$: The probability of being correct is $0.5$ (she's just guessing).\n\n* $H_a$ The probability of being correct is larger than $0.5$ (she can do better\n\t\tthan someone who just guesses).\n\t\t\n##### 2. Compute the test statistic\n\n* $x = 9$ (of $n = 10$) were correct.\n\n##### 3. Determine the p-value\n\nBased on our empirical method, we estimate a p-value of\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop( ~ (heads >= 9), data = RandomLadies)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nprop_TRUE \n   0.0115 \n```\n:::\n:::\n\n\nThe probabilities for the number of correct guesses \n\t\tcan be worked out theoretically as well.  The resulting distribution\n\t\tis called a **binomial** distribution.  As with other distributions we \n\t\thave seen, R includes the functions \n\t\t`dbinom()`, `pbinom()`, `qbinom()`, and `rbinom()`. \n\t\t\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dist(\"binom\", size = 10, prob = 0.5) # size = # of flips; prob = probability of heads\n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ndbinom(9, 10, .5) + dbinom(10, 10, .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01074219\n```\n:::\n\n```{.r .cell-code}\n1 - pbinom(8, 10, .5)                   # Note: P(X >= 9) = 1 - P(X <= 8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01074219\n```\n:::\n:::\n\n\n##### 4. Interpret the p-value.\n\nHow small is \"small\", and how small does a p-value have to be before we\nreject the null hypothesis?  Often, a \\textbf{significance level} (usually\ncalled $\\alpha$) of 0.05 is used.  Sometimes $\\alpha = 0.01$ is used\ninstead.  Basically, this corresponds to a 5\\% chance of seeing results as\nextreme as those found in our data, were the null hypothesis really true.\nIf we want to be more conservative about our judgement (not rejecting the\nnull hypothesis unless the evidence in the data is stronger against it), we\ncould use a smaller $\\alpha$ value.\n\nIt is also common for researchers to simply report the p-values they obtain\nfrom their analysis, allowing readers to draw conclusions on their own.\n    \n::: \n<!-- end example -->\n\n\n<!-- % If talking about the results of a statistical hypothesis test in a report, the following language may be useful.  It relates statements about the strength of the evidence found in the data with p-values (approximately -- there are no universal standards and rules here, just guidelines). -->\n\n::: {.example #exm-binom-test}\n\nThis situation is so common that there is a function to do the calculations for us.\nWe just need to provide the values of $x$ and $n$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinom.test(x = 9, n = 10)           # default test is 2-sided\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n\ndata:  9 out of 10\nnumber of successes = 9, number of trials = 10, p-value = 0.02148\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5549839 0.9974714\nsample estimates:\nprobability of success \n                   0.9 \n```\n:::\n:::\n\n\nThe output above doesn't match what we obtained in @exm-lady-binom\nbecause the alternative hypothesis is different.  It accepts both low number of correct\nidentifications (0 or 1) and high numbers (9 or 10) as evidence against the null\nhypothesis.  If we want a one-sided p-value, we just need to ask:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinom.test(x = 9, n = 10, alternative = \"greater\")        \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n\ndata:  9 out of 10\nnumber of successes = 9, number of trials = 10, p-value = 0.01074\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.6058367 1.0000000\nsample estimates:\nprobability of success \n                   0.9 \n```\n:::\n:::\n\n\n\n\n| Approximate p-value | Translation                                        |\n|---------------------|----------------------------------------------------|\n| $> 0.10$            | No convincing evidence against the null hypothesis |\n| $0.05-0.10$         | Weak evidence against the null hypothesis          |\n| $0.01-0.05$         | Some evidence against the null hypothesis          |\n| $<0.01$             | Strong evidence against the null hypothesis        |\n| $<0.001$            | Very strong evidence against the null hypothesis   |\n\n::: \n<!-- end example -->\n\n\n## Statistical Significance\n\nThe word \"significant\" has a special meaning in statistics.  If we say that a\ndifference or relationship between variables is significant, that means that we\nhave applied a hypothesis test, and have *failed* to reject the null\nhypothesis -- in other words, we have data that provides some evidence against\nthe null hypothesis, and supporting the alternative.\n\nSo, a pattern strong enough cause us to reject a null hypothesis of \"no\ndifference\" or \"no pattern\" or \"no relationship\" is called a\n**statistically significant** difference, pattern, or relationship.\nDifferences or relationships may fail to be statistically significant if they\nare small or weak, if they are masked by underlying variability, or if there is\ntoo little data.  Good studies will collect enough data and work to reduce\nvariability (if that is possible) in order to have a reasonable expectation of\ndetecting differences if they are large enough to be scientifically interesting.\n\n## T-tests\n\n<!-- \\authNote{Check the examples here.  Seems to be some mixing of two examples through each other.} -->\n\nMany hypothesis tests are conducted based on a $t$-distribution, and so they are\ncalled \"t-tests\". This is because, according to the Central Limit Theorem, the\nsampling distributions of most of our statistics (parameter estimates calculated\nfrom data) follow Normal distributions.  But just as we did when computing\nconfidence intervals, we'll always have to use the t-distribution rather than\nthe normal distribution, since we don't know $\\sigma$ (the true population\nstandard deviation), and since our sample size is finite.   These t-tests all\nuse a similar sort of test statistic:\n\n$$\nt = \\frac{\\mbox{estimate} - \\mbox{hypothesized value}}{\\mbox{standard error}}\n$$\n\nThe numerator tells us that the more the estimate and the hypothesized value\ndiffer, the stronger the evidence.  The denominator tells us that differences\nmean more when the standard deviation is small than when the standard deviation\nis large.\n\nThe test statistic is converted to a p-value by comparing it to the\nt-distribution with appropriate degrees of freedom.  For linear models, this is\nthe degrees of freedom associated with the residual standard error.  If\nconsidering some statistic (say, the mean value) for a single variable, the\ndegrees of freedom will be n-1, where n is the sample size.\n\n### The 1-sample t-test\n\nThe 1-sample t-test tests the null hypothesis\n\n::: {.itemize}\n\n*  $H_0: \\mu = \\mu_0$, vs.\n*  $H_a: \\mu \\neq 0$.\n\n::: \n<!-- end itemize -->\n\nThat is, it tests whether there is evidence that the mean of some population ($\\mu$) is different\nfrom some hypothesized value ($\\mu_0$) -- often $\\mu_0 = 0$.\n\n::: {.example #exm-weight-loss}\n\nLet's look at some data on weight loss programs.  In this data set, there were two groups.  One group\nreceived a monetary incentive if they lost weight while following a weight loss program.  The controls\ndid not receive a monetary incentive, but followed the same program otherwise.  Our null hypothesis is that the control participants would not lose weight -- that the true weight loss without incentives would average 0 pounds. Let's see whether on average the controls lost weight:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Stat2Data)\ndata(WeightLossIncentive)\nControls <- WeightLossIncentive |> filter(Group == \"Control\")\ndf_stats( ~ WeightLoss, data = Controls )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    response min Q1 median    Q3 max     mean       sd  n missing\n1 WeightLoss -17 -2      3 11.25  20 3.921053 9.107785 19       0\n```\n:::\n\n```{.r .cell-code}\ngf_boxplot( ~ WeightLoss, data = Controls)\n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThe standard error when doing inference for a mean is \n$$\nSE = \\frac{s}{\\sqrt{n}} = \n\\frac{ \n\t9.1077854\n}{\n\t\\sqrt{ 19 } \n}\n=\n2.0894693 \n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nSE <- 9.108 / sqrt(19); SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.089519\n```\n:::\n:::\n\n\nIf we want to test our null hypothesis, then we compute a t-statistic:\n\n::: {.cell}\n\n```{.r .cell-code}\nt <- (3.92 - 0) / SE; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.87603\n```\n:::\n:::\n\n\nand from this a p-value, which is the tails probability for a t-distribution with 18 degrees of freedom.  In other words, the p-value gives the probability of getting a test statistic at least as big as the one we really got, assuming that the test statistic follows a t-distribution with $n-1$ degrees of freedom.\n\nFirst, we find the probability of observing a test statistic of $t= 1.8760303$ or larger.  Then, we have to double this value -- it would be at least as unlikely to see a test statistic of $-1.8760303$ or smaller.  Our p-value should give the area under the t-distribution curve for x-values smaller than $-1.8760303$ *and* larger than $1.8760303$; it's the shaded area in the figure below:\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pt( t, df = 19-1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03848197\n```\n:::\n\n```{.r .cell-code}\n2 * ( 1 - pt( t, df = 19-1 ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.07696394\n```\n:::\n:::\n\n\nOur p-value is $0.077$ (1 or 2 significant digits are sufficient for reporting \np-values).  This is not compelling evidence that the weight loss program (without incentives) actually leads to a change in weight.  A change this large could occur just by chance in nearly 8% of samples.\n::: \n<!-- end example -->\n\n\n::: {.example #exm-t-test}\n\nIn R, there is also a function to automate this t-test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test( ~ WeightLoss, data = Controls)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  WeightLoss\nt = 1.8766, df = 18, p-value = 0.07688\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.4687594  8.3108647\nsample estimates:\nmean of x \n 3.921053 \n```\n:::\n:::\n\n\n\nIf we don't want so much out put, we can ask R to report only the p-value:\n\n::: {.cell}\n\n```{.r .cell-code}\npval(t.test( ~ WeightLoss, data = Controls))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   p.value \n0.07688493 \n```\n:::\n:::\n\n\nBy default, `t.test()` uses a significance level $\\alpha = 0.05$. If we want to specify a different $\\alpha$, we can use the input `conf.level` as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test( ~ WeightLoss, data = Controls, conf.level = 0.01)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  WeightLoss\nt = 1.8766, df = 18, p-value = 0.07688\nalternative hypothesis: true mean is not equal to 0\n1 percent confidence interval:\n 3.894498 3.947607\nsample estimates:\nmean of x \n 3.921053 \n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n### The paired 2-sample t-test\n\nSometimes, a dataset contains *paired* observations.  These might be, for\nexample, two measurements on the same experimental subject before and after some\nexperimental treatment; measurements on the same subject at two times, or in two\ndifferent situations; (or others).  In this case, we can not consider the\nmeasurements within a pair to be independent of each other.  But what we are\nreally interested in is the magnitude of the difference between the 2\nobservations in each pair.  So this \"paired t-test\" problem reduces to a\none-sample t-test, where the test statistic is constructed using the\n*differences* $D$ between the 2 measurements in each pair:\n\n$$\nt = \\frac{\\mbox{observed average difference} - \\mbox{hypothesized average difference}}{\\mbox{standard error}}\n$$\n\nLet's consider an example.\n\n::: {.example #exm-corneal-thickness}\n\nThe following table provides the corneal thickness in microns of\nboth eyes of patients who have glaucoma in one eye:\n\nHealthy    | 484 | 478 | 492 | 444 | 436 | 398 | 464 | 476 |\n-----------|-----|-----|-----|-----|-----|-----|-----|-----|\nGlaucoma   | 488 | 478 | 480 | 426 | 440 | 410 | 458 | 460 |\nDifference | 4   | 0   | -12 | -18 | 4   | 12  |  -6 |  -1 |\n\n\n\n\n\n\n\nThe corneal thickness is likely to be similar in the two eyes of any\nsingle patient, so that the two observations on the same patient\ncannot be assumed to be independent. But maybe (after accounting for differences between people), there is some difference in corneal thickness that has to do with the presence or absence of glaucoma.  First, we can have a look at the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGlaucoma <- tibble(\n  subject = 1:8,\n  glaucoma = c(488, 478, 480, 426, 440, 410, 458, 460),\n  healthy = c(484, 478, 492, 444, 436, 398, 464, 476),\n  diff = glaucoma - healthy\n)\n\ngf_point(glaucoma ~ subject, data = Glaucoma, \n         color = ~\"glaucoma\", shape = ~\"glaucoma\", size = 3, alpha = 0.9) |> \ngf_point(healthy ~ subject, data = Glaucoma, \n         color = ~\"healthy\", shape = ~\"healthy\", size = 3, alpha = 0.6) |>\n  gf_refine(\n    scale_color_discrete(name = \"status\"),  \n    scale_shape_discrete(name = \"status\")\n  )\n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\nIt looks like healthy corneas are often thicker.  To try to quantify this, we consider the difference\nbetween each pair of observations, denoted by $d_i$. We wish to\ntest,\n$$\nH_0: \\mu = 0 \\qquad \\mbox{ vs } \\qquad H_1: \\mu \\ne 0.\n$$\nUnder $H_0$,\n$$\nT = \\frac{\\bar{D}}{S/\\sqrt{n}} \\sim t_{n-1}.\n$$\n\nIn R, we can compute $D$ (the average of $d_i$) and its standard error to obtain the test statistic $t$ and the corresponding p-value:\n\n<!-- \\authNote{Check on this -- base plot? lm or t test?} -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- nrow(Glaucoma); n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8\n```\n:::\n\n```{.r .cell-code}\ndf_stats(~ diff, data = Glaucoma, mean, sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  response mean       sd\n1     diff   -4 10.74377\n```\n:::\n\n```{.r .cell-code}\nmeanD <- mean( ~ diff, data = Glaucoma); meanD\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -4\n```\n:::\n\n```{.r .cell-code}\nSE <- sd( ~ diff, data = Glaucoma) / sqrt(n); SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.798496\n```\n:::\n\n```{.r .cell-code}\nt <- (meanD - 0) / SE; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -1.053048\n```\n:::\n\n```{.r .cell-code}\npval <- 2 * pt(t, df=n-1); pval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3273053\n```\n:::\n:::\n\n\n\n\nNote that we used 2 * pt(...) because our test statistic $t$ was negative.  If it had been positive, we would have used 2 * (1-pt(...)) instead.  The illustration below may help make this clearer -- the p-value we are computing corresponds to the shaded areas in the plot. \n\n::: {.cell}\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-18-1.pdf)\n:::\n:::\n\n\n\nWe can also let R make all the computations for us. Notice that we use the *differences* between pairs as the input data, rather than the raw data itself.\n\n<!-- \\authNote{lm or t test?} -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_stats(WeightLoss ~ Group, data = WeightLossIncentive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    response     Group   min   Q1 median    Q3 max      mean       sd  n\n1 WeightLoss   Control -17.0 -2.0      3 11.25  20  3.921053 9.107785 19\n2 WeightLoss Incentive  -0.5  7.5     18 24.00  30 15.676471 9.413988 17\n  missing\n1       0\n2       2\n```\n:::\n\n```{.r .cell-code}\nmodel2 <- lm(WeightLoss ~ Group, data = WeightLossIncentive)\ncoef(summary(model2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)     3.921053   2.122817 1.847099 0.0734505837\nGroupIncentive 11.755418   3.089152 3.805387 0.0005635376\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n### The (unpaired) 2-sample t-test\n\nIn some cases, our research sample may contain data from 2 different categories\nof observational units.  For example, in the weight loss data, there were the\ncontrol participants we considered above; there were also a number of\nincentivized participants, who received money if they lost weight.  We might be\ninterested in considering whether the incentive made a difference.  Before we\nbegin, we can plot the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_boxplot(WeightLoss ~ Group, data = WeightLossIncentive)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nIt looks like the incentive group lost more weight.  But how can we judge whether the difference between groups is really an effect of the incentive, and not just random variation?\n\nIn this case, the null hypothesis $H_0$ would be that the average weight loss by control participants ($\\mu_c$) was the same as the average weight loss by incentivized participants ($\\mu_i$) -- $H_0: \\mu_c = \\mu_i$.  The alternative hypothesis would be $H_1: \\mu_c \\neq \\mu_i$ -- there was a difference between the two groups.\n\nBut in this case -- with data from 2 different categories -- how can we compute the appropriate standard error and test statistic for a t-test?  We have to consider the fact that there may be different numbers of data points in the 2 categories.  Let $n$ be the sample size within the first category $X$ (control participants in the example), and $m$ be the sample size in the other category $Y$ (incentive participants in the example). How can we compute a standard error for a difference in means between the two groups?\n\nIn this case, if we assume that the sample variaces are equal between the two categories, then we can define the pooled sample variance $s_p$:\n\n$$\ns^2_p = \\frac{(n-1)s_X^2 + (m-1)s_Y^2}{(m+n-2)}\n$$\n\nHere, $s_Y$ and $s_X$ are the sample standard deviations within each category.\nWe will not provide proof here, but it is known that in this case, the test\nstatistic is:\n\n$$\nT = \\frac{\\bar{X}-\\bar{Y}}{S_p\\sqrt{\\frac{1}{m}+\\frac{1}{n}}}\n\\sim t_{m+n-2}.\n$$\n\nAs written in the equation above, this test statistic follows a t-distribution with $m+n-2$ degrees of freedom.  We could carry out a two-sample t-test by hand using the test statistic defined above, or we can use the function `t.test()` and R will do the computations for us. For the weight loss example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(WeightLossIncentive, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  WeightLoss   Group Month7Loss\n1       12.5 Control       -2.0\n2       12.0 Control        7.0\n3        1.0 Control       19.5\n4       -5.0 Control       -0.5\n```\n:::\n\n```{.r .cell-code}\nt.test( ~ WeightLoss, groups = Group, data = WeightLossIncentive,\n       var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tTwo Sample t-test\n\ndata:  WeightLoss by Group\nt = -3.8054, df = 34, p-value = 0.0005635\nalternative hypothesis: true difference in means between group Control and group Incentive is not equal to 0\n95 percent confidence interval:\n -18.033330  -5.477506\nsample estimates:\n  mean in group Control mean in group Incentive \n               3.921053               15.676471 \n```\n:::\n:::\n\n\n\nNote the \"var.equal=TRUE\" input argument.  With this input, R will carry out the t-test assuming equal variance between the two categories.  If we want to avoid making this assumption, there is a modified version of the two-sample t-test called the Welch two-sample t-test, which does not assume equal variances.  If we omit the \"var.equal\" input, or set it to \"var.equal=FALSE\", then R will do a Welch two-sample t-test for us. (We will not cover in this course how to do a Welch test by hand).\n\nWe might be able to judge informally whether the equal-variance assumption is valid by looking at the distributions of our variable of interest grouped by category, and computing the sample standard deviations by groups:\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dens(~ WeightLoss, color = ~Group, data = WeightLossIncentive)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing non-finite values (`stat_density()`).\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-22-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nsd( ~ WeightLoss | Group, data = WeightLossIncentive, na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Control Incentive \n 9.107785  9.413988 \n```\n:::\n:::\n\n\n\nWe don't have an obvious indication of unequal variances. (This assumption can also be tested statistically, although we won't learn how in this class, and it's generally not recommended to do such a test prior to running a t-test).  If we wanted to do the t-test without the equal variance assumption, in R, we would use:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test( ~ WeightLoss, groups = Group, data = WeightLossIncentive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  WeightLoss by Group\nt = -3.7982, df = 33.276, p-value = 0.0005889\nalternative hypothesis: true difference in means between group Control and group Incentive is not equal to 0\n95 percent confidence interval:\n -18.05026  -5.46058\nsample estimates:\n  mean in group Control mean in group Incentive \n               3.921053               15.676471 \n```\n:::\n:::\n\n\n\n*In practice, there is no real disadvantage to simply using Welch's test all the time.*  So in general, if you are doing a 2-sample unpaired test, you should always use `var.equal=FALSE` (or omit the input `var.equal`, and it will default to FALSE).\n\n\n### Testing Model Coefficients\n\nWe can also use t-tests to test the null hypothesis that there is no\nrelationship between the predictor and explanatory variables in a regression\nmodel.  If we can't reject that hypothesis, then we have evidence that there\n*is* really some relationship, and that the response can be predicted based upon\nthe explanatory variable.\n\n\n\n::: {.example #exm-drag-force}\n\nSuppose you suspect that drag force should be proportional to the square of velocity.  Let's see\nif that is consistent with the data collected by some physics students.  \nIn this experiment, the students rigged up neutrally buoyant balloon and then loaded it \nwith different amounts of weight and dropped it until and recorded its terminal velocity.\nAt that point the force due to gravity (determined by the mass loaded to the balloon) \nis equal to the drag force (because there is no acceleration).\n\nWe'll fit a power law model\n$$\n\\texttt{force.drag} = A \\cdot \\texttt{velocity}^a\n$$\nand test the hypothesis that $a = 2$.  We can fit this model using a log-log transformation:\n$$\n\\log(\\texttt{force.drag}) = \\log(A)  +  a \\log( \\texttt{velocity})\n$$\nSo $a = \\beta_1$ in our usual linear model notation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastR2)\ndrag.model <- lm(log(force.drag) ~ log(velocity), data = Drag)\ngf_point(log(force.drag) ~ log(velocity), data = Drag)\n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/drag-again-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe fit is not perfect, and in fact suggests a systematic problem with the way\nthese data were collected.^[There is some evidence in these data that some of the \nobservations did not reach critical velocity.  \nIt would be good to refit this data with those observations removed from the data.\nSee @exr-drag-refit.] This is even clearer if we look at the residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(resid(drag.model) ~ fitted(drag.model))\n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-24-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ resid(drag.model))\n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-24-2.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(drag.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(force.drag) ~ log(velocity), data = Drag)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.34162 -0.14967 -0.04673  0.06663  0.66155 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    2.47366    0.04425   55.91   <2e-16 ***\nlog(velocity)  2.05149    0.05366   38.23   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2451 on 40 degrees of freedom\nMultiple R-squared:  0.9734,\tAdjusted R-squared:  0.9727 \nF-statistic:  1462 on 1 and 40 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nNone of the p-values produced in this output is what we want.  \nThey are testing the hypotheses that $\\beta_0 =$ and that $\\beta_1 = 0$.\n$\\beta_1 = 0$.  \nBut we can easily calculate the p-value we want since we have the standard error\nand degrees of freedom.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta1.hat <- 2.051\nSE <- 0.05366\nt <-  ( beta1.hat - 2 ) / SE; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9504286\n```\n:::\n\n```{.r .cell-code}\n2 * pt( - abs(t), df= 40 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3476022\n```\n:::\n:::\n\n\nWith this large a p-value, we cannot reject the null hypothesis that $p = 2$.\n**A large p-value does not prove that $p = 2$, \nbut it does say that our data are consistent with that value.**  \nOf course, our data may be consistent with many other values of $p$ as well.\n::: \n<!-- end example -->\n\n\n::: {.example #exm-drag-force-2}\n\nWe could also do the previous example using a nonlinear model\n\n::: {.cell}\n\n```{.r .cell-code}\ndrag.model2 <- nls(force.drag ~ A * velocity^p, data = Drag, start = list(A = 1,p = 2))\nsummary(drag.model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFormula: force.drag ~ A * velocity^p\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \nA 12.90528    1.61361   7.998 7.96e-10 ***\np  1.92989    0.09442  20.440  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.48 on 40 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 3.737e-06\n```\n:::\n:::\n\n\nAgain, the p-values listed are not of interest (they are testing the hypotheses that \neach coefficient is 0).  But we can compute the p-value of interest as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nt <- (2 - 1.92989) / 0.0944; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7426907\n```\n:::\n\n```{.r .cell-code}\n2 * pt( - abs(t), df = 40) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4620085\n```\n:::\n:::\n\n\nAlthough the two p-values are different, the conclusion is the same using either model:  \nOur data are consistent with the hypothesis that $p = 2$.\n::: \n<!-- end example -->\n\n\n::: {.example #exm-porche}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(PorschePrice, package = \"Stat2Data\")\nhead(PorschePrice)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Price Age Mileage\n1  69.4   3    21.5\n2  56.9   3    43.0\n3  49.9   2    19.9\n4  47.4   4    36.0\n5  42.9   4    44.0\n6  36.9   6    49.8\n```\n:::\n\n```{.r .cell-code}\nporsche.model <- lm(Price ~ Mileage, data = PorschePrice)\nsummary(porsche.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Price ~ Mileage, data = PorschePrice)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.3077  -4.0470  -0.3945   3.8374  12.6758 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 71.09045    2.36986    30.0  < 2e-16 ***\nMileage     -0.58940    0.05665   -10.4 3.98e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.17 on 28 degrees of freedom\nMultiple R-squared:  0.7945,\tAdjusted R-squared:  0.7872 \nF-statistic: 108.3 on 1 and 28 DF,  p-value: 3.982e-11\n```\n:::\n\n```{.r .cell-code}\ngf_point( resid(porsche.model) ~ fitted(porsche.model)) |> gf_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess'\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-28-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ngf_qq( ~ resid(porsche.model) )\n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-28-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe model looks reasonable.  What are the two hypotheses being tested?\n\n:::: {.enumerate}\n\n1. $H_0: \\beta_0 = 0$.\n\n    Often this is not an interesting test because often we are not so interested in the intercept\n\t\t$\\beta_0$, and especially not in whether it is 0.  In this case, the intercept might\n\t\tbe interesting because it tells us the price of a Porsche with no miles.  On the other hand,\n\t\twe might not expect a used car, even one with very few miles to fit the same pattern as \n\t\ta new car.  There is probably a loss in value that occurs as soon as a car is purchased.\n\t\t\n    In any case, it is clear that the intercept will not be 0; we don't need a hypothesis test\n\t\tto tell us that.  Indeed, the evidence is incredibly strong.\n\n    A confidence interval for the intercept is more interesting since it gives a sort \n\t\tof \"starting price\" for used Porches.\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(porsche.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %     97.5 %\n(Intercept) 66.2360186 75.9448869\nMileage     -0.7054401 -0.4733618\n```\n:::\n:::\n\n\n\n\n2. $H_0: \\beta_1 = 0$.\n\n    There is strong evidence against this hypothesis as well.  This is also not surprising.\n\t\tIf $\\beta_1 = 0$, that would mean that the price of the cars does not depend on the mileage.\n\n    A test of $\\beta_1 = 0$ in a simple linear model is often called the **model utility test**\n\t\tbecause it is testing whether the predictor (without any others) is of any use to us or not.\n\t\t\n<!-- \\authNote{ -->\n<!-- \t\tA test of $\\beta_1=0$ in a regression model is often used for *model selection* -- if we can not reject the null hypothesis of no relationship between the predictor and the response, then the predictor is not really a useful predictor at all. We might want to use (or *select*) a model without that predictor, instead.} -->\n\n3. $H_0: \\beta_1 = \\beta_{10}$.\n\n    Although the output above doesn't do all of the work for us, we can test other\n\t\thypotheses as well.  (The notation above is a bit tricky, $\\beta_{10}$ should be read\n\t\t``$\\beta_1$ null\" -- it is a hypothesized value for $\\beta_1$.)\n\t\t\n    For example, let's test $\\beta_1 = -1$.  That the hypothesis that the value \n\t\tdrops one dollar per mile driven.\n    \n    While this example is interesting as an exercise, it is quite rare to have a sensible hypothesized value for a regression slope parameter that we want to test.  It is much more common to ask, as we did above, \"is there a pattern here indicating that the predictor is a useful predictor of the response\"?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt <- (-0.5894 - (-1) ) / 0.0566; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.254417\n```\n:::\n\n```{.r .cell-code}\n2 * pt( - abs(t), df = 28 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.748867e-08\n```\n:::\n:::\n\n\nThis p-value is small enough to cause us to reject this value for $\\beta_1$.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end example -->\n\n\n## Connection to Confidence Intervals {#sec-ci-ht-duality}\n\nThere is a natural duality between t-based hypothesis tests and confidence intervals.\nSince the p-value is computed using tail probabilities of the t-distribution and confidence\nlevel describes the central probability, the p-value will be below 0.05 exactly when the \nhypothesized value is not contained in the 95% confidence interval.  (Similar statements can\nbe made for other confidence levels.)\n<!-- So a confidence interval generally conveys more information than a p-value in these situations. -->\n\n::: {.example #exm-ht-ci-duality}\n\nIn the preceding example we rejected the null hypothesis that $\\beta_1 = -1$.\nIn fact, we will reject (at the $\\alpha = 0.05$ level) any hypothesized value not contained \nin the 95% confidence interval.\n\n::: {.cell}\n\n```{.r .cell-code}\nt <- (-0.5894 - (- .71)) / 0.0566; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.130742\n```\n:::\n\n```{.r .cell-code}\n2 * pt( - abs(t), df = 28)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04203043\n```\n:::\n:::\n\n\nBut we won't reject values inside the confidence interval.\n\n::: {.cell}\n\n```{.r .cell-code}\nt <- (-0.5894 - (- .70)) / 0.0566; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.954064\n```\n:::\n\n```{.r .cell-code}\n2 * pt( - abs(t), df = 28 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.06074829\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n\n::: {.example #exm-ht-ci-duality-2}\n\nThe output below illustrates this duality.\n\n::: {.cell}\n\n```{.r .cell-code}\ndrag.model <- lm(log(force.drag) ~ log(velocity), data = Drag)\ncoef(summary(drag.model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept)   2.473663 0.04424622 55.90677 1.358299e-39\nlog(velocity) 2.051493 0.05365839 38.23248 4.103776e-33\n```\n:::\n\n```{.r .cell-code}\nbeta1.hat <- 2.051\nSE <- 0.05366\nt <-  ( beta1.hat - 2 ) / SE; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9504286\n```\n:::\n\n```{.r .cell-code}\n2 * pt( - abs(t), df= 40 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3476022\n```\n:::\n\n```{.r .cell-code}\nconfint(drag.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %   97.5 %\n(Intercept)   2.384238 2.563088\nlog(velocity) 1.943045 2.159941\n```\n:::\n:::\n\n\nSince the confidence interval for $p$ (i.e., for $\\beta_1$) includes 2, \n2 is a plausible value for the power (i.e., consistent with our data).\nA 2-sided p-value larger than 0.05 says the same thing at the same level of confidence.  \n::: \n<!-- end example -->\n\n## Exercises\n\n::: {.problem #exr-geiger}\n**Geiger counter**\n\nAn experiment was conducted to see if the number of clicks \non a Geiger counter in a 7.5 minute interval is related to\nthe distance (in m) between a radioactive source and the detection \ndevice according to an inverse square law:\n\n$$\n\t\\mbox{clicks} =\tA + \\frac{k}{\\mbox{distance}^{2}}\n$$\n\nAnswer the questions below using the following output\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm( clicks ~ I(1/(distance^2)), data = Geiger)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = clicks ~ I(1/(distance^2)), data = Geiger)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.234 -15.817   4.027  10.899  34.091 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        114.281     10.787   10.59 5.51e-06 ***\nI(1/(distance^2))   31.477      1.001   31.46 1.13e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.51 on 8 degrees of freedom\nMultiple R-squared:  0.992,\tAdjusted R-squared:  0.991 \nF-statistic: 989.7 on 1 and 8 DF,  p-value: 1.134e-09\n```\n:::\n\n```{.r .cell-code}\nplot(model, w = 1:2)\n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-35-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-35-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::: {.enumerate}\n\n#. Are there are any reasons (from what you can tell in the output\n\t\tabove) to be concerned about using this model?\n#. What does $A$ tell us about this situation?  \n\t\tWhat would it mean if $A$ were 0?  What if $A \\neq 0$?\n#. What is the estimate for $A$?  Express this as an estimate $\\pm$ uncertainty\n\tusing our rules for numbers of digits.\n#. What is the p-value for the test of the null hypothesis that $A = 0$?\n\tWhat conclusion do we draw from this?\n#. What does $k$ tell us about this situation?  In what situations would $k$\n\tbe larger or smaller?  What would it mean if $k$ were 0?\n#. Express the estimate for $k$ as an estimate $\\pm$ uncertainty.\n#. What is the p-value for the test of the null hypothesis that $k = 0$?\n\tWhat conclusion do we draw from this?\n#. A standard radioactive substance has a value of $k = 29.812$.  Might that be the\nsubstance we are using here?  Conduct an appropriate hypothesis test to answer\nthis question.  Carefully show all four steps.\n::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n:::: {.enumerate}\n\na.  The normal-quantile plot looks pretty good for a sample of this size.\n\t\t\t\tThe residual plot is perhaps not quite as \"noisy\" as we would like \n\t\t\t\t(the first few residuals cluster above zero, then next few below), but it \n\t\t\t\tis not terrible either. Ideally we would like to have a larger \n\t\t\t\tdata set to see whether this pattern persists or whether things look more\n\t\t\t\tnoisy as we \"fill-in\" with more data.\n#.  $A$ is a measure of background radiation levels, it is the amount of clicks \n\t\t\t\twe would get if our test substance were \"at infinity\", i.e., so far away (or \n\t\t\t\tbeyond some shielding) that it does not affect the Geiger counter.\n#.  $114 \\pm 11$\n#.  \\ensuremath{5.5\\times 10^{-6}}.  This gives strong evidence that there is some background radiation\n\t\t\t\tbeing measured by the Geiger counter.\n#.  $k$ measures the rate at which our test substance is emitting radioactive \n\t\t\t\tparticles.  If $k$ is 0, then our substance is not radioactive (or at least not\n\t\t\t\tbeing detected by the Geiger counter).\n#.  $31.5  \\pm 1.0$\n#.  \\ensuremath{1.1\\times 10^{-9}}.  We have strong evidence that our substance is decaying and \n\t\t\t\tcontributing to the Geiger counter clicks.\n#.  $H_0: k = 29.812$;  $H_a: k \\neq 29.812$\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    t <- (31.5 - 29.812) / 1.0; t     # test statistic\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [1] 1.688\n    ```\n    :::\n    \n    ```{.r .cell-code}\n    2 * (1 - pt(t, df = 8))           # p -value\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [1] 0.1298892\n    ```\n    :::\n    :::\n\n\nConclusion.  With a p-value this large, we cannot reject the null hypothesis.\nOur data are consistent with the hypothesis that our test substance is the same\nas the standard substance.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-gentleman-tasting-wine}\n**Gentleman tasting wine**\n\nA gentleman claims he can distinguish between four vintages of \n  a particular wine.  His friends, assuming he has probably just \n  had too much of each, decide to test him.  They prepare one glass\n  of each vintage and present the gentleman with four unlabeled glasses\n  of wine.  What is the probability that the gentleman correctly \n  identifies all four simply by guessing?\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nWe can use the product of conditional probabilities:\n  \n$$\n\\frac14 \\cdot \\frac13 \\cdot \\frac12 \\cdot \\frac11 = \\frac1{24}\n$$\nSince the probability of guessing the first glass correctly is $\\frac 14$, the\nprobability of guess the second correctly -- assuming the first was correctly\nguessed -- is $\\frac13$; the probability of guessing the third correctly --\nassuming the first two were correctly guessed -- is $\\frac 12$; and if the\nfirst three are guessed correctly, the last is guaranteed to be correct.\n\nAltnernative solution: The wines can be presented in \n$4 \\cdot 3 \\cdot 2 \\cdot 1 = 24$ different orders, \nso the probability of guessing correctly is $1/24$.\n::: \n<!-- end solution -->\n\n::: {.problem #exr-drag-refit}\n**Drag refit**\n\nRedo the drag force analysis after removing observations that appear not to have reached\nterminal velocity.\n\nIf you can describe the rows you want to remove logically, the `filter()` command\nworks well for this.  You can also remove rows by row number.  For example, the following\nremoves rows 1, 3, 5 and 7:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDrag[ - c(1, 3, 5, 7), ] \n```\n:::\n\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nLet's remove the fastest values at each height setting.  Although they are the fastest, it appears \nthat terminal velocity has not yet been reached.  At least, these points would fit the \noverall pattern better if the velocity were larger.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point( force.drag ~ velocity, data = Drag,\n          show.legend = FALSE,\n          color = ~ (velocity < 3.9) & !(velocity > 1 & velocity < 1.5)) |>\n  gf_theme(legend.position = \"top\") \n```\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-38-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nDrag2 <- Drag |> filter((velocity < 3.9) & !(velocity > 1 & velocity < 1.5))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndrag2.model <- lm(log(force.drag) ~ log(velocity), data = Drag2)\nsummary(drag2.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(force.drag) ~ log(velocity), data = Drag2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31581 -0.10503  0.01792  0.08009  0.25424 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    2.36586    0.02722   86.93   <2e-16 ***\nlog(velocity)  2.11416    0.03679   57.46   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1367 on 28 degrees of freedom\nMultiple R-squared:  0.9916,\tAdjusted R-squared:  0.9913 \nF-statistic:  3302 on 1 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nmplot(drag2.model, w = 1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-39-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n[[2]]\n```\n:::\n\n::: {.cell-output-display}\n![](Stat241-HypothesisTesting_files/figure-pdf/unnamed-chunk-39-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe model is still not as good as we might like, and it seams like the fit is different for the \nheavier objects than for the lighter ones.  This could be due to some flaw in the design of the \nexperiment or because drag force actually behaves differently at low speeds vs. higher speeds.\nNotice the data suggest an exponent on velocity that is just a tiny bit larger than 2:\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(drag2.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %   97.5 %\n(Intercept)   2.310110 2.421611\nlog(velocity) 2.038795 2.189534\n```\n:::\n:::\n\n\nSo our data (for whichever reason, potentially still due to design issues) is not compatible \nwith the hypothesis that this exponent should be 2.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-veg-oil}\n**Vegetable oil**\n\nSixteen samples of a certain brand of hydrogenated vegetable oil were tested to determine \ntheir melting point.  The mean melting point for the 16 samples was 94.32 degrees \nand the standard deviation was 1.2 degrees.\n\n:::: {.enumerate}\n\na. Conduct a test of the hypothesis $\\mu = 95$.  Follow the four step procedure.\na. Will 95 be inside or outside of a 95% confidence interval for the melting point? \n    How do you know?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSE <- 1.2 / sqrt(16); SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3\n```\n:::\n\n```{.r .cell-code}\nt <- (94.32 - 95) / SE; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.266667\n```\n:::\n\n```{.r .cell-code}\np_val <- 2 * pt(t, df = 15); p_val    # p-value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03862989\n```\n:::\n:::\n\n\n95 will be  outside \nthe confidence interval because the p-value is  \nless than 0.05. \nAs a double check, here is the 95% confidence interval.\n\n::: {.cell}\n\n```{.r .cell-code}\nt_star <- qt(.975, df = 11); t_star\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.200985\n```\n:::\n\n```{.r .cell-code}\n94.32 + c(-1,1) * t_star * SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 93.6597 94.9803\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-v-notch} \n**Charpy V-notch impact test**\n\nThe [Charpy V-notch impact test](https://en.wikipedia.org/wiki/Charpy_impact_test)\nis a common way to test the toughness of a material.\nThis test was applied to 42 specimens of a particular alloy at 110 degrees F.\nThe mean amount of transverse lateral expansion was computed to be\n73.1 mils with a sample standard deviation of 5.9 mils.\n\nTo be suitable for a particular application, the true amount of expansion must be less than 75 mils.\nThe alloy will not be used unless their is strong evidence (a p-value below 0.01) that \nthis specification is met.\n\n:::: {.enumerate}\n\n#.  Use a p-value to decide whether this alloy may be used.\n#.  Use a confidence interval to decide whether this alloy may be used.\n#.  Are there advantages to one approach over the other?  If you had to present\n\t\t\tan argument to your boss, which approach would you use?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n:::: {.enumerate}\n\n#.  In this situation it makes sense to do a one-sided test since we will reject\n\t\t\tthe alloy only if the amount of expansion is to high, not if it is too low.\n\n::: {.cell}\n\n```{.r .cell-code}\nSE <- 5.9/sqrt(42); SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9103898\n```\n:::\n\n```{.r .cell-code}\nt <- (73.1 - 75) / SE; t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.087018\n```\n:::\n\n```{.r .cell-code}\npt(t, df = 41)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02157239\n```\n:::\n:::\n\n\n#.  Corresponding to a 1-sided test with a significance level of $\\alpha = 0.01$,\n\t\t\twe could make a 98% confidence interval (1% in each tail).  The upper end of this\n\t\t\twould be the same as for a 1-sided 99% confidence interval\n\n::: {.cell}\n\n```{.r .cell-code}\nt_star <- qt(0.99, df = 41)\n73.1 + c(-1, 1) * t_star * SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 70.89613 75.30387\n```\n:::\n:::\n\n\n#.  If all we are interested in is a decision, the two methods are equivalent.\n\t\t\tI confidence interval might be easier to explain to someone less familiar with\n\t\t\tstatistics.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n<!-- % \\begin{problem} -->\n<!-- % \tData from a 1993 study to see how well lichens serve as an indicator for air pollution are in the  -->\n<!-- % \t\\dataframe{ex12.20} data set in the \\pkg{Devore7} package.  -->\n<!-- % \tIn that paper, a simple linear model was fit to see how the wet deposition of $NO^{-}_3$  -->\n<!-- % \trelated to the percentage dry weight of lichen. -->\n<!-- % \tUsing this model,  -->\n<!-- % \t\\begin{enumerate} -->\n<!-- % \t\t\\item -->\n<!-- % \t\t\tTest the hypothesis that the slope of the linear relationship is 1. -->\n<!-- % \t\t\\item -->\n<!-- % \t\t\tCompute a 90\\% confidence interval for the slope of the linear relationship. -->\n<!-- % \t\\end{enumerate} -->\n<!-- % <<>>= -->\n<!-- % library(Devore7) -->\n<!-- % summary(lm( LichenN ~ deposition, data = ex12.20)) -->\n<!-- % @ -->\n<!-- % \\end{problem} -->\n<!-- % \\begin{solution} -->\n<!-- % <<tidy = FALSE>>= -->\n<!-- % t <- (0.967 - 1) / 0.183 -->\n<!-- % 2 * pt( -abs(t), df = 11 )           # p-value -->\n<!-- % @ -->\n<!-- % With such a large p-value, we do not reject the null hypothesis.  Our data are consistent  -->\n<!-- % with the hypothesis that the slope is 1. -->\n<!-- % <<>>= -->\n<!-- % t_star <- qt(.95, df = 11); t_star -->\n<!-- % .967 + c(-1,1) * t_star * 0.183    # 90% CI -->\n<!-- % @ -->\n<!-- % Notice that this interval includes 1.  We knew it would because 1 would not be rejected at the  -->\n<!-- % $\\alpha = 0.10$ level. -->\n<!-- % <<>>= -->\n<!-- % t_star <- qt(.975, df = 11) -->\n<!-- % 95 + c(-1,1) * t_star * SE -->\n<!-- % @ -->\n<!-- % \\end{solution} -->\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}