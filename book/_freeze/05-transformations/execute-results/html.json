{
  "hash": "987031783b8fe97843c686044c65554e",
  "result": {
    "markdown": "\n# Transformation and Combinations of Random Variables {#sec-transformations}\n\n\n\n\n\n\nWe will often be interested in random variables that are formed by transformations\nor combinations other random variables.\n\n::: {.itemize}\n\n* If we roll to dice and let $X$ and $Y$ be the results on each die,\n\t\tthen the sum is $X+Y$.\n* If $R$ is the radius of a circle, then $A = \\pi R^2$ is its area.\n* If $X$ and $Y$ are the length and width of a rectangle, then\n\t\tthe area is given by $A = X Y$.\n* If $F$ is a temperature measured in degrees Fahrenheit, then \n\t\t$C = \\frac{5}{9}(F-32) = \\frac59 F - \\frac{160}{9}$ is the \n\t\ttemperature in degrees Celsius. (Most other unit conversions\n\t\tare even simpler linear functions.)\n* If $W$ is weight in kg and $H$ is height in meters, then\n\t\t$B = W/H^2$ is bmi (body mass index)\n* If $X_1, X_2, \\dots, X_n$ is a random sample, then the mean of the sample is \n$$\n\t\t\\mean X = \\frac{X_1 + X_2 + \\cdots X_n}{n}\n\t\t=\n\t\t\\frac1n X_1 +\n\t\t\\frac1n X_2 + \\cdots +\n\t\t\\frac1n X_n \n$$\n::: \n<!-- end itemize -->\n\n\nMore generally, we are interested in determining the distribution of the random \nvariable $Y$ if \n\n$$\nY = f(X_1, X_2, \\dots, X_n)\n$$\n\nand we already know the distributions of $X_1, X_2, \\dots, X_n$.\nThere are methods for working out the distribution of $Y$ in many such situations, \nOften it is possible to figure out the distribution of variables formed this way,\nbut the methods require more techniques of probability than we will develop in this \ncourse.  We will generally be satisfied with one of the following approaches:\n\n::: {.enumerate}\n\n1. Use simulations to approximate the new distribution.\n2. Calculate (or estimate) the mean and variance of the new distribution,\n\t\twhich is often much easier than determining the pdf.\n3. Rely on theorems that tell us the new distribution in certain special\n\t\tcases.\n::: \n<!-- end enumerate -->\n\n\n## Simulations\n\nIn this section we will make use of the fact that each of our familiar distributions\nhas a function in R that lets us simulate randomly sampling data from that distribution.\n\n\n::: {#fig-runif .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nX <- runif(10000, 0,1)\nY <- runif(10000, 0,1)\ngf_dhistogram( ~ X , main = \"Sample from Unif(0,1)\", \n               binwidth = .05, center = 0.025)\ngf_dhistogram( ~ Y , main = \"Another sample from Unif(0,1)\", \n               binwidth = .05, center = 0.025)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/fig-runif-1.png){#fig-runif-1 width=432}\n:::\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/fig-runif-2.png){#fig-runif-2 width=432}\n:::\n\nTwo samples from a uniform distribution.\n:::\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nS <- X + Y\nP <- X * Y\ngf_dhistogram( ~ S , main = \"Sum of two iid Unif(0,1) rvs\", \n               binwidth = 0.1, center = 0.05)\ngf_dhistogram( ~ P , main = \"Product of two iid Unif(0,1) rvs\", \n               binwidth = 0.05, center = 0.025)\n```\n\n::: {.cell-output-display}\n![Sum of two uniform random variables.](05-transformations_files/figure-html/fig-sum-product-uniform-iid-1.png){#fig-sum-product-uniform-iid-1 width=432}\n:::\n\n::: {.cell-output-display}\n![Product of two uniform random variables.](05-transformations_files/figure-html/fig-sum-product-uniform-iid-2.png){#fig-sum-product-uniform-iid-2 width=432}\n:::\n:::\n\n\n#### Independence Matters {-}\nIt is important that we have created `x` and `y` independently.\nIndependent random variables that have the same distribution are called \n**independent identically distributed** (iid) random variables.  As an illustration\nof an extreme situation where the variables in our sum are not independent, let's \nuse the values of `x` in both roles:\n\n\n::: {#fig-sum-product-uniform-non-iid .cell}\n\n```{.r .cell-code}\nS2 <- X + X\nP2 <- X * X \ngf_dhistogram( ~ S2 , main = \"Sum of two non-iid Unif(0,1) rvs\", binwidth = 0.1, center = 0.05)\ngf_dhistogram( ~ P2 , main = \"Product of two non-iid Unif(0,1) rvs\", binwidth = 0.05, center = 0.025)\n```\n\n::: {.cell-output-display}\n![Sum](05-transformations_files/figure-html/fig-sum-product-uniform-non-iid-1.png){#fig-sum-product-uniform-non-iid-1 width=432}\n:::\n\n::: {.cell-output-display}\n![Product](05-transformations_files/figure-html/fig-sum-product-uniform-non-iid-2.png){#fig-sum-product-uniform-non-iid-2 width=432}\n:::\n\nAdding and Multiplying two uniform random variables that are not independent.\n:::\n\n\nNotice how different the resulting distributions are, especially for the sum.\n\nSimilar procedures can be used to give an approximate distribution for any combination\nof random variables that we can simulate.\n\n## Propagation of Mean and Variance\n\nSometimes it is not necessary to know everything about a distribution.  Sometimes knowing \nthe mean or variance suffices, and there are several common situations where the mean\nand variance are easy to calculate\n\n### Linear Transformations\n\nThe easiest of these is a linear transformation of a random variable.  \n\n::: {.boxedText}\n\nIf $X$ is a random variable with known mean and variance, then \n\n$$\n\\begin{aligned}\n\t\\E(a X + b) &= a \\E(X) + b \\; \\mbox{, and}\n\t\\\\\n\t\t\\Var(a X + b) &= a^2 \\Var (X) \\; .\n\\end{aligned}\n$$\n\n::: \n<!-- end boxedText -->\n\n\nThese are actually pretty easy to prove from the definitions of mean and variance.  (Just \nwrite down the integrals and do some algebra.)  But these results also match our \nintuition.\n\n:::: {.enumerate}\n\n1. If we add or subtract a constant $b$, that increases or decreases every\n\t\tvalue by the same amount.  This will increase the mean by that amount,\n\t\tbut does nothing to the variance (since everything is no more or less\n\t\tspread out than it was before).  This explains the $+b$ in the first\n\t\tequation and why $b$ does not appear at all in the formula for the\n\t\tvariance.\n\n$$\n\\begin{aligned}\n\t\t\\E(X + b) &= \\int_{-\\infty}^{\\infty} (x+b) f(x) \\; dx  \n\t\t      = \\int_{-\\infty}^{\\infty} x f(x) \\; dx  \n\t\t      \t+ \\int_{-\\infty}^{\\infty} b f(x) \\; dx  \n\t\t\t  = \\E(X) + b\n\t\t\t  \\\\\n\t\t\\Var(X+b) &= \\int_{-\\infty}^{\\infty} (x + b - (\\mu + b))^2 f(x) \\; dx  \n\t\t\t\t\t= \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\; dx  = \\Var(X) \n\\end{aligned}\n$$\n\n\n2. Now consider multiplying each value by the same amount $a$.  \n\t\tThis scales all the values by $a$, and hence scales the mean by $a$ as well.\n\t\tThis also makes the values more or less spread out (more when $|a| > 1$, less when\n\t\t$|a| < 1$).  \n\t\tBut it is the standard deviation -- not the \n\t\tvariance -- that increases or decreases by the factor $|a|$.\n\t\tThe variance scales with $a^2$.\n\n$$\n\\begin{aligned}\n\t\t\\E(aX) &= \\int_{-\\infty}^{\\infty} ax f(x) \\; dx  \n\t\t      = a \\int_{-\\infty}^{\\infty} x f(x) \\; dx = a \\E(X)\n\t\t\t  \\\\\n\t\t\\Var(aX) &= \\int_{-\\infty}^{\\infty} (ax - a\\mu)^2 f(x) \\; dx  \n\t\t      = a^2 \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x) \\; dx = a^2 \\Var(X)\n\\end{aligned}\n$$\n\n:::: \n<!-- end enumerate -->\n\n\n::: {.example #exm-mean-sd-of-linear-trans}\n\n**Q.** Suppose $X$ has a mean of 5 and a standard deviation of 2.  What are the mean\n\tand standard deviation of $3 X + 4$?\n\n**A.** $\\E(3 X + 4) = 3 \\E(X) + 4 = 3 \\cdot 5 + 4 = 19$\n\n$\\Var(3 X + 4) = 3^2 \\Var(X) = 3^2 \\cdot 2^2 = 36$.  So he standard deviation of \n$3X + 4$ is $\\sqrt{36} = 6$.  Notice that $6 = 2 \\cdot 3$.\n::: \n<!-- end example -->\n\n\n### Sums\n\nThe most important combination of two random variables is a sum.\n\n::: {.boxedText #thm-mean-var-of-sum}\n**Mean and variance of a sum of independent random variables**\n\nLet $X$ and $Y$ be two random variables with known means and variances, then\n\t\n$$\n\\begin{aligned}\n\t\\E(X + Y) &= a \\E(X) + \\E(Y) \\; \\mbox{, and}\n\t\\\\\n\t\\Var(X + Y) &= \\Var (X)  + \\Var(Y) \\; \\mbox{, provided $X$ and $Y$ are independent.}\n\\end{aligned}\n$$\n\nThat is,\n\n\n:::: {.itemize}\n\n* The expected value of a sum is the sum of the expected values. \n* The variance of a sum is the sum of the variances -- *provided the variables are independent.*\n:::: \n<!-- end itemize -->\n\n::: \n<!-- end boxedText -->\n\nThe independence condition for the variance rule is critical.<!--  -->\n^[\nThese results can be proved by setting up the appropriate integrals and \nrearranging them algebraically.  In this case, one needs to know a bit about \njoint, marginal, and conditional distributions, so for the sake of time we will \nomit the proofs.]\n\n::: {.example #exm-mean-sd-of-sum}\n\n**Q.** \nSuppose $X$ and $Y$ are independent random variables with means 3 and 4 and standard \ndeviations 1 and 2.  What are the mean and standard deviation of $X + Y$?\n\n**A.** \n$\\E(X+Y) = 3 + 4$.  $\\Var(X + Y) = 1^2 + 2^2 = 5$, so $\\SD(X + Y) = \\sqrt{5} \\approx\n2.236$.\n::: \n<!-- end example -->\n\n\n::: {.example #exm-mean-var-of-sum-of-unif}\n\n**Q.** \nLet $X \\sim \\Unif(0,1)$ and $Y \\sim \\Unif(0,1)$ be independent random variables\nand let $S = X+Y$.  What are the mean and variance of $S = X + Y$?\n\n**A.** \n$\\E(S) = \\E(X) + \\E(Y) = \\frac12 + \\frac12 = 1$.\n$\\Var(S) = \\Var(X) + \\Var(Y) = \\frac1{12} + \\frac1{12} = \\frac16$.\n\nNote that this matches the mean and variance of a $\\Tri(0,2,1)$-distribution, since \n$$ \n\\frac{ 0 + 2 + 1}{3} = 1 \\;, \n$$\n\tand \n$$\n\t\\frac{ 0^2 + 2^2 + 1^2 - 0 \\cdot 2 - 0\\cdot 1 - 1\\cdot 2}{18} \n\t= \\frac{3}{18} = \\frac16 \\;. \n$$  \nIn fact, it can be shown that $S \\sim \\Tri(0,2,1)$.\n::: \n<!-- end example -->\n\n\nIf we express the rule for variances in terms of standard deviations we get \n\n::: {.boxedText #thm-pythagorean-sd}\n\n**The Pythagorean Theorem for standard deviations.**\n\nIf $X$ and $Y$ are independent random variables, then\n$$\n\t\\SD(X+Y) = \\sqrt{ \\SD(X)^2 + \\SD(Y)^2 }\\; .\n$$\nThe independence condition plays the role of the right triangle condition\nin the usual Pythagorean Theorem.\n::: \n<!-- end boxedText -->\n\n\n### Linear Combinations\n\nThe results in the preceding sections can be combined and iterated to get results for\narbitrary linear combinations of random variables.\n\n::: {.boxedText #thm-linear-combinations}\n**Expected value and variance for linear combinations**\n\nLet $Y = a_1 X_1 + a_2 X_2 + \\cdots + a_k X_k$, then \n\n$$\n\\begin{aligned}\n\t\\E(Y) & = \n\ta_1 \\E(X_1) + \n\ta_2 \\E(X_1) + \n\t\\cdots\n\ta_k \\E(X_k)  \n\t\\\\[2mm]\n\t\\Var(Y) & = \n\ta_1^2 \\Var(X_1) + \n\ta_2^2 \\Var(X_1) + \n\t\\cdots\n\ta_k^2 \\Var(X_k) \\; ,\n\t\\\\\n\t& \\qquad \\mbox{provided $X_1, X_2, \\dots, X_k$ are independent.}  \n\\end{aligned}\n$$\n\n::: \n<!-- end boxedText -->\n\n\n::: {.example #exm-mean-sd-of-sum-of-3}\n\n**Q.** \nSuppose the means and standard deviations of three independent random variables\nare as in the table below.  \n\n|     | mean   | standard deviation |\n|-----|--------|--------------------|\n|\t$X$ | 100    | 15                 |\n|\t$Y$ | 120    | 20                 |\n|\t$Z$ | 110    | 25                 |\n\nDetermine the mean and standard deviation of $X + 2Y - 3Z$.\n\n\n**A.**   The mean is $100 + 2 (120) - 3(110) = 10$.\n\nThe variance is \n$1^2\\cdot 15^2 + 2^2 \\cdot 20^2 + (-3)^2 \\cdot 25^2 = \n7450  $,\nso the standard deviation is \n$\\sqrt{ 7450} =\n86.3$.\n::: \n<!-- end example -->\n\n\n\n## Normal distributions are special\n\nNormal distributions are special because \n\\myindex{Central Limit Theorem}\n\n::: {.boxedText}\n\n**Special Properties of Normal Distributions**\n\n:::: {.enumerate}\n\n#. Linear combinations of independent normal random variables are again normal.\n\n#. Sums of iid random variables from *any distribution* are approximately\nnormal provided the number of terms in the sum is large enough.\n\n    This result follows from what is known as the **Central Limit Theorem**.\n\t  The Central Limit Theorem explains why the normal distributions\n\t  are so important and why so many things have approximately normal distributions.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end boxedText -->\n\n\nThis means that just knowing the mean and standard deviation \ntells us everything we need to know about the distribution of the linear \ncombination of normal random variables.\n\n::: {.example #exm-mean-sd-of-sum-of-normals}\n\nLet $X \\sim \\Norm(10,2)$ and $Y\\sim \\Norm(12,4)$.  If $X$ and $Y$ are independent,\nthen \n\n$$\nX + Y \\sim \\Norm(10 + 12, \\sqrt{2^2 + 4^2}) \n\t= \\Norm(22, 4.47) \\;.\n$$\n\n::: \n<!-- end example -->\n\n\n::: {.example #exm-mean-sd-of-combo-of-normal}\n\nLet $X \\sim \\Norm(10,2)$ and $Y\\sim \\Norm(12,4)$.  If $X$ and $Y$ are independent, then\n\t\n$$\nX - Y \\sim \\Norm(10 - 12, \\sqrt{2^2 + 4^2}) \n\t= \\Norm(-2, 4.47)\\; .\n$$\n\n::: \n<!-- end example -->\n\n\n::: {.example #exm-confirm-with-sims}\n\n**Q.**  Use simulation to illustrate the previous two results.\n\n**A.** \n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nX <- rnorm(5000, 10, 2)\nY <- rnorm(5000, 12, 4)\nSum <- X + Y\nDiff <- X - Y\nfitdistr(Sum, \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mean           sd     \n  22.01845912    4.37393899 \n ( 0.06185684) ( 0.04373939)\n```\n:::\n\n```{.r .cell-code}\nfitdistr(Diff, \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mean           sd     \n  -1.94699670    4.47063317 \n ( 0.06322430) ( 0.04470633)\n```\n:::\n\n```{.r .cell-code}\ngf_dhistogram(~ Sum) |> gf_fitdistr(dist = \"norm\") \n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-5-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_dhistogram(~ Diff) |> gf_fitdistr(dist = \"norm\") \n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-5-2.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq(~ Sum) \n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-5-3.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq(~ Diff) \n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-5-4.png){width=432}\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n::: {.example #exm-sum-of-twelve-unifs}\n\nLet $X_i \\simiid \\Unif(0,1)$.  Consider $S = \\sum_{i = 1}^{12} X_i$.  \nSince $\\E(X_i) = \\frac12$ and $\\Var(X_i) = \\frac{1^2}{12} = \\frac1{12}$\n\n$$\n\\begin{aligned}\n\t\\E(S) &= \\frac12  + \\frac12  + \\cdots \\frac12  = 12 \\cdot \\frac12  = 6\n\t\\\\\n\t\\Var(S) & = \\frac1{12} + \\frac1{12} + \\cdots \\frac 1{12} = 12 \\cdot \\frac1{12} = 1\n\\end{aligned}\n$$\n\nFurthermore, the normal approximation for $S$ is quite good:\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nX1 <- runif(5000,0,1); X2 <- runif(5000,0,1); X3 <- runif(5000,0,1) \nX4 <- runif(5000,0,1); X5 <- runif(5000,0,1); X6 <- runif(5000,0,1) \nX7 <- runif(5000,0,1); X8 <- runif(5000,0,1); X9 <- runif(5000,0,1) \nX10 <- runif(5000,0,1); X11 <- runif(5000,0,1); X12 <- runif(5000,0,1) \nS <- X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12\nfitdistr(S, \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mean           sd     \n  6.016979438   0.997959137 \n (0.014113273) (0.009979591)\n```\n:::\n\n```{.r .cell-code}\ngf_dhistogram(~S)  |> gf_fitdistr(dist = \"norm\")\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-6-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_qq(~S)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-6-2.png){width=432}\n:::\n:::\n\n\nThis means that $S - 6 \\approx \\Norm(0,1)$.  This has been used in computer software\nas a relatively easy way to simulate normal data given a good psuedorandom number generator\nfor $\\Unif(0,1)$.\n::: \n<!-- end example -->\n\n\n## Estimating the Mean of a Population by Sampling\n\nAs important as data are in statistics, typically we are not interested in our \ndata set, but rather in what we can learn from our data about some larger situation.\nFor example,\n\n::: {.enumerate}\n\n#. Quality assurance engineers test a few parts to make a decision about \nwhether the production process is working correctly for all the parts.\n#. Automobile manufacturers crash a small number of vehicles to learn how\ntheir (other) cars might perform in an accident.\n#. Public opinion pollsters survey a (relatively) small number of people in order to learn\nabout the opinions of millions of people.\n#. In order to estimate the number of dimes in a large sack of times, you decide to\nweigh the sack of dimes and divide by the mean weight of a dime.  To do this you\nneed to know the mean weight of a dime.  You decide to carefully weigh 30 dimes\nand use those weights to estimate the mean weight of a dime.\n\n::: \n<!-- end enumerate -->\n\n\nWe have now developed enough background to begin learning how this process works. \nWe begin by introducing some key terms:\n\n::: {.description}\n\n* **population** The collection of individuals, objects, or processes we want to \n\t\tknow something about.\n\t\t\n* **parameter** A number that describes (a feature of) a population.\n\n    In typical applications, parameters are unknown and data are collected\n\t\tfor the purpose of estimating parameters.\n\t\t\n* **sample** The collection of individuals, objects, or processes we have data\n\t\tabout.  Ideally, the sample is a well chosen subset of the population.\n\t\t\n* **statistic** A number that describes (a feature of) a sample.\n\t\t\n* **sampling distribution** The distribution of a statistic under random sampling.\n\n    The process of random sampling leads to a random sample, from which a statistic\n\t\tcould be computed.  Since that number depends on a random process (sampling),\n\t\tit is a random variable.  The sampling distribution should not be confused\n\t\twith the distribution of an individual sample (nor with the distribution of \n\t\tthe population).\n\n::: \n<!-- end description -->\n\n\n::: {.example #exm-population-sample-param-stat}\n\n:::: {.enumerate}\n\na.  Quality assurance engineers test a few parts to make a decision about \n\t\twhether the production process is working correctly for all the parts.  \n\t\t\n::::: {.itemize}\n\n    * population: all parts produced at the plant\n    * sample: the parts tested in the quality control protocol\n    * parameter: mean strength of all parts produced at the plant\n    * statistic: mean strength of the tested parts\n::::: \n<!-- end itemize -->\n\nb. Automobile manufacturers crash a small number of vehicles to learn how\ntheir (other) cars might perform in an accident.  \n\n::::: {.itemize}\n\n    * population: all cars (of a certain model) produced\n    * sample: the small number of cars that were used in the crash test\n    * paramter: average amount of force applied to crash dummy's head in all such crashes (not\n      just the crashes done in the lab)\n    * statistic: average amount of force applied to crash dummy's head in the crashes\n      that were tested\n::::: \n<!-- end itemize -->\n\nc.  Public opinion pollsters survey a (relatively) small number of people in order to learn\nabout the opinions of millions of people.\n\n::::: {.itemize}\n\n    * population: all voters\n    * sample: people actually contacted\n    * parameter: proportion of all voters who will vote for candidate A\n    * statistic: proportion of sample who claim they will vote for candidate A\n::::: \n<!-- end itemize -->\n\nd. The mean weight of a dime can be estimated from the weights of 30 dimes.\n\n::::: {.itemize}\n\n    * population: all dimes in the sack\n    * sample: 30 dimes actually weighed\n    * parameter: the mean weight of all the dimes in the sack\n    * statistic: the mean weight of the 30 dimes actually weighed.\n::::: \n<!-- end itemize -->\n\n\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end example -->\n\n\n::: {.boxedText #thm-central-limit}\n\n**The Central Limit Theorem**\n\nIf $X_1, X_1, \\dots, X_n$ is an iid random sample (of some quantitative variable) \nfrom a population with mean $\\mu$ and standard deviation $\\sigma$,\nthen the sampling distribution of the sample mean or sample sum \nof a large enough random sample is approximately normally distributed.  In fact,\n\n:::: {.itemize}\n\n* $\\displaystyle \\mean X \\approx \\Norm(\\mu, \\frac{\\sigma}{\\sqrt{n}})$\n* $\\displaystyle \\sum_{i = 1}^{n} X_i \\approx \\Norm(\\mu, {\\sigma}{\\sqrt{n}})$\n:::: \n<!-- end itemize -->\n\n\n\nThe approximations are better \n\n:::: {.itemize}\n\n* when the population distribution is similar to a normal distribution (unimodal, nearly symmetric, etc.),\nand \n* when the sample size is larger,\n:::: \n<!-- end itemize -->\n\nand are exact when the population distribution is normal.\n\nThe Central Limit Theorem is illustrated nicely in an applet available \nfrom the Rice Virtual Laboratory in Statistics\n(<http://onlinestatbook.com/stat_sim/sampling_dist/index.html>).\n::: \n<!-- end boxedText -->\n\n\nImportant things to note about the Central Limit Theorem\n\n::: {.enumerate}\n\n#. There are three distributions involved: the population, individual sample(s),\n\t\tand the sampling distribution.\n#. Large enough is usually not all that large (30--40 is large enough for \n\t\tmost quantitative population distributions you are likely to encounter).\n#. We could already calculate the expected value and variance of means\n\t\tand sums, the new information in the Central Limit Theorem is about the\n\t\tshape of the resulting sampling distribution.\n#. The Central Limit Theorem requires a random sample.  In situations where\n\t\trandom sampling is not possible, the Central Limit Theorem may still be \n\t\tapproximately correct or other more complicated methods may be required.\n::: \n<!-- end enumerate -->\n\n\n### Estimands, estimates, estimators\n\nWhen the goal of sampling is to estimate a parameter, it is handy to have the \nfollowing terminology:\n\n::: {.description}\n \n* **estimand** A parameter we are trying to estimate.  (Sometimes also called the **measureand**.)\n* **estimate** A statistic calculated from a particular data set and used to \n\t\testimate the estimand.  (Sometimes called a **measurement**.)\n* **estimator** A random variable obtained by calculating an estimate \n\t\tfrom a random sample.\n* **unbiased estimator** An estimator for which the expected value is equal\n\t\tto the estimand.  So an unbiased estimator is \"correct on average\".\n::: \n<!-- end description -->\n\nIn this section, our estimand is the mean of the population ($mu$) and our estimator \nis $\\mean X$, the mean of a random sample.  We will use $\\mean x$ to denote \nthe estimate computed from a particular sample; this is an estimate.\n\n### If we knew $\\sigma$\n\nTypically we will not know $\\sigma$ or $\\mu$.  (To know them, one would typically\nneed to know the entire population, but then we would not need to use statistics\nto estimate $\\mu$ because we would know the exact answer.)  But for the moment,\nlet's pretend we live in a fantasy world where we know $\\sigma$.\n\n::: {.example #exm-se-dimes}\n\nSuppose the standard deviation of the weight of all dimes in our sack of dimes is $0.03$.\nIf we collect a random sample of 25 dimes, then \n\n$$\n\t\\mean X \\approx \\Norm(\\mu, \\frac{\\sigma}{\\sqrt{n}}) = \\Norm(\\mu, 0.006)\n$$\n\nso \n\t\n$$\n\t\\mean X - \\mu  \\approx \\Norm(0, \\frac{\\sigma}{\\sqrt{n}}) = \\Norm(0, 0.006) \\;.\n$$\n\nThis means that\n\n$$\n\\begin{aligned}\n\t\\Prob(| \\mean X - \\mu | \\le 0.006) &\\approx 0.68\n\t\\\\[4mm]\n\t\\Prob(| \\mean X - \\mu | \\le 0.012) &\\approx 0.95\n\\end{aligned}\n$$\n\nSo we can be quite confident that our sample mean will be within 0.012 g of the $\\mu$,\nmean weight of all dimes in the sack.\n\nExpressed in words, the claim is that 95% of random samples lead to a sample mean\nthat is within 0.012 g of the true mean.  Of course, that means 5% of samples lead\nto a sample mean that is farther away than that.  For any given sample, there is \nno way to know if it is one of the 95% or one of the 5%.\n::: \n<!-- end example -->\n\n### Confidence Intervals ($\\sigma$ known)\n\nThe typical way of expressing this is with a confidence interval.  The key idea \nis this:\n\n::: {.center}\n\nIf $\\mean X$ is close to $\\mu$, then $\\mu$ is close to $\\mean X$.\n::: \n\n<!-- end center -->\n\n\nSo an approximate 95% confidence interval is \n$$\n\\mean x \\pm 2 SE \n= \\mean x \\pm 2 \\frac{\\sigma}{\\sqrt{n}}\n$$\nor more precisely\n$$\n\\mean x \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}\n$$\nbecause\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.975)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.959964\n```\n:::\n:::\n\n\nNotice the switch from $\\mean X$ to $\\mean x$.  We used $\\mean X$ when we were\nconsidering the random variable formed by taking a random sample and computing\nthe sample mean.  $\\mean X$ is a random variable with a distribution.\nWhen we are considering a specific data set, we write $\\mean x$ instead.\n\nThere is a subtly here that often gets people confused about the interpretation\nof a confidence interval.  Although 95% of samples result in 95% confidence\nintervals that contain the true mean, it is not correct to say that a particular \nconfidence interval has a 95% chance of containing the true mean.  Neither the \nparticular confidence interval nor the true mean are random, so no reasonable\nprobability statement can be made *about a particular confidence interval\ncomputed from a particular data set*.\n\n### Confidence Intervals ($\\sigma$ unknown)\n\nThe more typical situation is that $\\sigma$ is not known and needs to be estimated \nfrom the data.  It has been known for a long time that when randomly sampling \nfrom a normal population,\n\n$$\n\\E\\left( \\sum_{i = 1}^n (X_i - \\mean X)^2 \\right) = (n-1) \\sigma^2\n$$\n\nThis means that \n\n$$\nS^2 = \\frac{ \\sum_{i = 1}^n (X_i - \\mean X)^2 }{n-1}\n$$\nis an unbiased estimator of $\\sigma^2$.^[In fact more is known.  The\ndistribution of $S^2$ is a member of the Gamma family of distributions.] This\nexplains the reason for the $n-1$ in the denominator of the sample\nvariance.^[Side note: the sample standard deviation is a *biased*\nestimator for the population standard deviation.  (On average it will be too\nsmall.)]\n\nAn obvious, but not quite correct solution to our unknown $\\sigma$ dilemma is to use\n$s$ in pace of $\\sigma$.  In fact this was routinely done until 1908 @Student1908, \nwhen William Gosset, publishing under the pseudonym Student, pointed out that \nwhen sampling from a $\\Norm(\\mu, \\sigma)$ popoulation,\n\n$$\n\\frac{ \\mean X - \\mu}{\\sigma/\\sqrt{n}} \\sim \\Norm(0,1)\n\\mbox{\\;\\ but\\ } \n\\frac{ \\mean X - \\mu}{s/\\sqrt{n}} \\sim \\Tdist(n-1)\n$$\nThe new family of distributions (called Student's $t$-distributions) are very \nsimilar to the normal distributions -- but \"shorter and fatter\".  This means that\none must go farther into the tails of a $t$-distribution to capture the central 95%.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ngf_dist(\"norm\", main = \"Normal and T-distributions\") |>\n  gf_fun(dt(x, df = 2) ~ x, col = 'gray80') |>\n  gf_fun(dt(x, df = 4) ~ x, col = 'gray60') |>\n  gf_fun(dt(x, df = 8) ~ x, col = 'gray40') |>\n  gf_fun(dt(x, df = 16) ~ x, col = 'gray20')\n```\n\n::: {.cell-output-display}\n![Some t-distributions](05-transformations_files/figure-html/fig-t-dists-1.png){#fig-t-dists width=768}\n:::\n:::\n\n\n\nThe resulting confidence interval has the form\n$$\n\\mean x \\pm t_* \\frac{s}{\\sqrt{n}}\n$$\n\n\n<!-- <http://webspace.ship.edu/pgmarr/Geo441/Readings/Student%201908%20-%20The%20Probable%20error%20of%20a%20Mean.pdf> -->\n\n### Standard Error\n\nThe Central Limit Theorem tells us that (under certain condidtions), the standard \ndeviation of the sampling distribution for the sample mean is $\\frac{\\sigma}{\\sqrt{n}}$.\nTypically we don't know $\\sigma$ so we estimate this quantity with \n$\\frac{s}{\\sqrt{n}}$.\nTo avoid having to say \"the estimated standard deviation of the sampling distribution\", \nwe introduce a new term\n\n::: {.description}\n\n* **standard error** the estimated standard deviation of a sampling distribution\n::: \n<!-- end description -->\n\n\nWe will typically abbreviate standard error as SE. (Some authors use se.)\nStatistical software often includes standard errors in output.\n\nConfidence intervals for the mean can now be expressed as \n$$\n\\mean x \\pm t_* SE\n$$\nWe will see other intervals that make use of the $t$-distributions.  All of them\nshare a common structure:\n\n::: {.boxedText}\n\n$$\n\t\t\\mbox{estimate} \\pm t_* SE\n$$\n::: \n<!-- end boxedText -->\n\n\n\nThe value of $t_*$ needed for a 95% confidence interval is calculated\nsimilar to the way we calculated $z_*$, but we need to know the degrees \nof freedom parameter for the $t$-distribution ($n-1$ for this situation).\n\n::: {.example #exm-ci-dimes}\n\nSuppose a sample of 30 dimes has a mean weight of 2.258 g and a standard \ndeviation of 0.022 g.  We can calculate a 95% confidence interval as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_bar <- 2.258\nt_star <- qt(0.975, df = 29); t_star\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.04523\n```\n:::\n\n```{.r .cell-code}\nSE <- 0.022/ sqrt(30); SE     # standard error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.004016632\n```\n:::\n\n```{.r .cell-code}\nME <- t_star * SE; ME         # margin of error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.008214935\n```\n:::\n\n```{.r .cell-code}\nx_bar + c(-1,1) * ME\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.249785 2.266215\n```\n:::\n:::\n\n\n\nIf you have the data (and not just the \nthe summary statistics $\\mean x$ and $s$), R\\ can automate this entire computation\nfor us with the `t.test()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test( ~ mass, data = Dimes)    \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  mass\nt = 560.44, df = 29, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 2.249992 2.266474\nsample estimates:\nmean of x \n 2.258233 \n```\n:::\n\n```{.r .cell-code}\nconfint(t.test( ~ mass, data = Dimes))  # just the CI without the other stuff\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mean of x\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lower\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upper\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"level\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2.258233\",\"2\":\"2.249992\",\"3\":\"2.266474\",\"4\":\"0.95\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n### Interpreting Confidence Intervals\n\nHere is an illustration of 100 confidence intervals computed by sampling from a \nnormal population with mean $\\mu = 100$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/CIsim01-1.png){width=768}\n:::\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/CIsim01-2.png){width=768}\n:::\n:::\n\n\n\nNotice that some of the samples have larger means (the dots) and some smaller means.\nAlso some have wider intervals and some narrower (because $s$ varies from sample to\nsample).  But most of the intervals contain the estimand (100).  A few do not.\n\nIn the long-run, 95% of the intervals should \"cover\" the estimand and 5% should \nfail to cover.  95% is referred to as the **confidence level** or **coverage rate**.\n\nAs we can see, the estimand is not always contained in the confidence interval.\nBut, in a way that we will be able to make more formal later, a confidence interval \nis a range of plausible values for the estimand -- values that are consistent with \nthe data in a probabilistic sense.  The level of confidence is related to \nhow strong the evidence must be for us to declare that a value is not consistent\nwith the data.\n\n\n### Other confidence levels\n\nWe can use other **confidence levels** by using a different  **critical value** $t_*$.  \nSo the general form for our confidence interval is\n\n$$\n\\mean x \\pm t_* SE\n$$\n\n::: {.example #exm-critical-value-98}\n\nA 98% confidence interval, for example, requires a larger value of $t_*$.\nIf the sample size is $n = 30$, then we use\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqt(0.99, df = 29)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.462021\n```\n:::\n:::\n\n\nNotice the use of 0.99 in this command.  We want to find the limits of the \ncentral 98% of the standard normal distribution.  If the central portion contains\n98% of the distribution, then each tail contains 1%.  \n\nWe could also have used the following to calculate $t_*$.\n\n::: {.cell}\n\n```{.r .cell-code}\nqt(0.01, df = 29)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.462021\n```\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n::: {.example #exm-ci-dimes-98}\n\n**Q.**  Compute a 98% confidence interval for the mean weight of a dime\nbased on our `dimes` data set.\n\n**A.** \nWe can do this by hand:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_bar <- 2.258\nt_star <- qt(0.99, df = 29); t_star\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.462021\n```\n:::\n\n```{.r .cell-code}\nSE <- 0.022/ sqrt(30); SE     # standard error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.004016632\n```\n:::\n\n```{.r .cell-code}\nME <- t_star * SE; ME         # margin of error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.009889034\n```\n:::\n\n```{.r .cell-code}\nx_bar + c(-1,1) * ME\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.248111 2.267889\n```\n:::\n:::\n\n\n\nor let R\\ do the work for us:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test( ~ mass, data = Dimes, conf.level = 0.98)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  mass\nt = 560.44, df = 29, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n98 percent confidence interval:\n 2.248313 2.268154\nsample estimates:\nmean of x \n 2.258233 \n```\n:::\n\n```{.r .cell-code}\nconfint(t.test( ~ mass, data = Dimes, conf.level = 0.98))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mean of x\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lower\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upper\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"level\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2.258233\",\"2\":\"2.248313\",\"3\":\"2.268154\",\"4\":\"0.98\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n::: \n<!-- end example -->\n\n\n### Robustness\n\nThe confidence intervals based on the $t$-distributions assume that the population\nis normal.  The degree to which a statistical procedure works even when some or all\nof the assumptions used to derive its mathematical properties are not satisfied\nis referred to as the **robustness** of the procedure.  The $t$-based confidence\nintervals are quite robust.\n\nQuantifying robustness precisely is difficult because how well a procedure works\nmay depend on many factors. The general principles are\n\n::: {.itemize}\n\n* The bigger the better. \n\n\t\tThe larger the sample size, the less it mattes what the population\n\t\tdistribution is.\n* The more normal the better.  \n\n\t\tThe closer the population is to a normal distribution,\n\t\tthe smaller the sample sizes may be.\n::: \n<!-- end itemize -->\n\n\nFor assistance in particular applications, we offer the following rules of thumb.\n\n::: {.enumerate}\n\n#. If the population is normal, the confidence intervals achieve\n\t\tthe stated coverage rate for all sample sizes.\n\n\t\tBut since small data sets provide very little indication\n\t\tof the shape of the population distribution, the normality assumption\n\t\tmust be justified by something other than the data.\n\t\t(Perhaps other larger data sets collected in a similar fashion \n\t\thave shown that normality is a good assumption or perhaps there\n\t\tis some theoretical reason to accept the normality assumption.)\n#. For modestly sized samples ($15 \\le n \\le 40$), the $t$-based \n\t\tconfidence is acceptable as long as the distribution appears \n\t\tto be unimodal and is not strongly skewed.\n#. For large sample size ($n \\ge 40$), the $t$-procedure will work acceptably\n\t\twell for most unimodal distributions.\n\n\t\tBut keep in mind, if the distribution is strongly skewed, the \n\t\tmean might not be the best parameter to estimate.\n#. Because both the sample mean and the sample variance are sensitive\n\t\tto outliers, one should proceed with caution when outliers \n\t\tare present.\n\n\t\tOutliers that are due to mistakes and can be corrected, should be.  Outliers that can be verified\n\t\tto be incorrect but cannot be corrected should be removed.  It is not\n\t\tacceptable to remove an outlier just because you don't want it in your data.\n\t\tBut sometimes statisticians do \"leave one out analysis\" where they run the \n\t\tanalysis with and without the outlier.  If the conclusions are the same, then\n\t\tthe conclusions can be safely drawn.  But if the conclusions are different,\n\t\tlikely additional data will be needed to resolve the differences.\n\n\t\tDon't forget: sometimes the outliers are the interesting part of the story.\n\t\tDetermining what makes them different from the rest of the data may be \n\t\tthe most important thing.\n\n::: \n<!-- end enumerate -->\n\n## Exercises\n\n::: {.problem #exr-gamma-sims-and-qq}\n**Gamma distributions**\n\nLet $X$ and $Y$ be independent $\\Gamm(\\texttt{shape} = 2, \\texttt{scale} = 3)$ random \nvariables.  \nLet $S = X+Y$ and let $D = X - Y$.\nUse simulations (with 5000 replications) and quantile-quantile pltos \nto answer the following:\n\n:::: {.enumerate}\n\na. Fit a normal distribution to $S$ using `fitdistr()`.  \n\t\t\tIs the normal distribution a good fit?\nb. Fit a Gamma distribution to $S$ using `fitdistr()`.  \n\t\t\tIs the Gamma distribution a good fit?\nc. Fit a normal distribution to $D$ using `fitdistr()`.  \n\t\t\tIs the normal distribution a good fit?\nd. Why is it not a good idea to fit a Gamma distribution to $D$?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 5000\nX <- rgamma(n,  shape = 2, scale = 3)\nY <- rgamma(n,  shape = 2, scale = 3)\nS <- X + Y\nD <- X - Y\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistr(S, \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mean           sd     \n  12.08151124    5.91283684 \n ( 0.08362014) ( 0.05912837)\n```\n:::\n\n```{.r .cell-code}\ngf_qq( ~ S)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-16-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_density( ~ S)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-16-2.png){width=432}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistr(S, \"gamma\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      shape         rate    \n  4.116230437   0.340704877 \n (0.079212672) (0.006973328)\n```\n:::\n\n```{.r .cell-code}\ngf_qq( ~ S, distribution = qgamma, dparams = list(shape = 4.04, rate = 0.333))\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-17-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_density( ~ S)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-17-2.png){width=432}\n:::\n:::\n\n\nThe gamma distribution fits $S$ much better than the normal does.  But notice that the \nshape parameter is larger than the shape parameters for $X$ and $Y$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistr(D, \"normal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mean           sd     \n  -0.04940173    5.95646041 \n ( 0.08423707) ( 0.05956460)\n```\n:::\n\n```{.r .cell-code}\ngf_qq( ~ D)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-18-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_density( ~ D)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-18-2.png){width=432}\n:::\n:::\n\n\n\nThis time the distribution is symmetric, so the problems are not as obvious as for $S$, but the \nnormal quantile plots shows some clear curve (there is lots of data, so very little noise in\nthis plot).  The problem is that the shape is not like a normal distribution.  \nThe density plot perhaps makes this clearer if you are not familiar with qq-plots.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-normal-probabilities}\n**Normal probabilities**\n\nIf $X \\sim \\Norm(110, 15)$ and $Y \\sim \\Norm(100, 20)$ are independent\nrandom varaibles:\n\t\n:::: {.enumerate}\n\na. What is $\\Prob(X \\ge 140)$?\nb. What is $\\Prob(Y \\ge 140)$?\nc. What is $\\Prob(X \\ge 150)$?\nd. What is $\\Prob(Y \\ge 150)$?\ne. What is $\\Prob(X + Y \\ge 250)$?\nf. What is $\\Prob(X \\ge Y)$? (Hint: $X \\ge Y \\Leftrightarrow X - Y \\ge 0$.)\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(140, mean = 110, sd = 15) #  P(X > 140)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02275013\n```\n:::\n\n```{.r .cell-code}\n1 - pnorm(140, mean = 100, sd = 20) #  P(Y > 140)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02275013\n```\n:::\n\n```{.r .cell-code}\n1 - pnorm(150, mean = 110, sd = 15) #  P(X > 150)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.003830381\n```\n:::\n\n```{.r .cell-code}\n1 - pnorm(150, mean = 100, sd = 20) #  P(Y > 150)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.006209665\n```\n:::\n:::\n\n\n$X + Y \\sim \\Norm(210, 25 )$ because $110 + 100 = 210$ and $15^2 + 20^2 = 25^2$.  \nSo $\\Prob(X + Y \\ge 250)$ is\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(250, 210, 25) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.05479929\n```\n:::\n:::\n\n\n$X - Y \\sim \\Norm(10, 25 )$ because $110 - 100 = 210$ and $15^2 + 20^2 = 25^2$.  \nSo $\\Prob(X - Y \\ge 0)$ is\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(0, 10, 25) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6554217\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-trans-and-combos-1}\n**Linear Combinations**\n\nSuppose $X$ and $Y$ are independent random variables with means\nand standard deviations as listed below.\n\n\n|     | mean   | standard deviation |\n|-----|--------|--------------------|\n|\t$X$ | 54     | 12                 |\n|\t$Y$ | 48     | 9                  |\n\n\nWhat are the mean and standard deviation of each of the following:\n\n:::: {.enumerate}\n\na. $X + Y$\nb. $2X$\nc. $2X + 3Y$\nd. $2X - 3Y$\n::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# part a\nc(mean = 54 + 48, sd= sqrt(12^2 + 9^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmean   sd \n 102   15 \n```\n:::\n\n```{.r .cell-code}\n# part b\nc(mean = 2 * 54, sd= 2 * 12)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmean   sd \n 108   24 \n```\n:::\n\n```{.r .cell-code}\n# part c\nc(mean = 2 * 54 + 3 * 48, sd= sqrt(2^2 * 12^2 + 3^2 * 9^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     mean        sd \n252.00000  36.12478 \n```\n:::\n\n```{.r .cell-code}\n# part d\nc(mean = 2 * 54 - 3 * 48, sd= sqrt(2^2 * 12^2 + (-3)^2 * 9^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     mean        sd \n-36.00000  36.12478 \n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-Calvin-heights}\n**Calvin heights**\n\nYou are interested to know the mean height of male Calvin students.\nAssuming the standard deviation is similar to that of the population\nat large, we will assume $\\sigma = 2.8$ inches.\n\n:::: {.enumerate}\n\na. What is the distribution of $\\mean X - \\mu$?\n\t\t\t(Hint: start by determining the distribution of $\\mean X$.)\nb. If you measure the heights of a sample of 20 students,\n\t\t\twhat is the probability that your mean will be within \n\t\t\t1 inch of the actual mean?\nc. How large would your sample need to be to make this probability\n\t\t\tbe 95%?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n::: {.solution}\n\n:::: {.enumerate}\n\na. $\\Norm(0, \\frac{\\sigma}{\\sqrt{n}}) = \n\t\t\t \\Norm(0, \\frac{2.8}{\\sqrt{n}})$\nb.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(1, mean = 0, sd = 2.8/sqrt(20)) - pnorm(-1, mean = 0, sd = 2.8/sqrt(20))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.889777\n```\n:::\n:::\n\n\nc. We would need 1 to be equal to $2 \\frac{\\sigma}{\\sqrt n}$. So\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- (2 * (2.8))^2; n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31.36\n```\n:::\n\n```{.r .cell-code}\n# double check:\nn <- round(n); n  # should have an integer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31\n```\n:::\n\n```{.r .cell-code}\npnorm(1, mean = 0, sd = 2.8/sqrt(n)) - pnorm(-1, mean = 0, sd = 2.8/sqrt(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9532422\n```\n:::\n:::\n\n\n    This can also be done by guessing and checking for the value of $n$ (sort of like the \n\t\tguessing game where someone tells you \"higher\" or \"lower\" after each guess until \n\t\tyou converge on the number they have selected.\n\t\t\n\n::: {.cell}\n\n```{.r .cell-code}\nprob <- makeFun(pnorm(1, mean = 0, sd = 2.8/sqrt(n)) - pnorm(-1, mean = 0, sd = 2.8/sqrt(n)) ~ n)\nprob(20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.889777\n```\n:::\n\n```{.r .cell-code}\nprob(40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9761023\n```\n:::\n\n```{.r .cell-code}\nprob(30)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9495527\n```\n:::\n\n```{.r .cell-code}\nprob(32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9566482\n```\n:::\n\n```{.r .cell-code}\nprob(31)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9532422\n```\n:::\n:::\n\n\nR can even automate this for us:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuniroot(makeFun(prob(n) - 0.95 ~ n), c(20, 40))  # look for a solution between 20 and 40\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$root\n[1] 30.11704\n\n$f.root\n[1] 4.489753e-10\n\n$iter\n[1] 7\n\n$init.it\n[1] NA\n\n$estim.prec\n[1] 6.103516e-05\n```\n:::\n:::\n\n\nWe should round the answer, of course.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-ci-for-mean}\n**Confidence interval for a mean**\n\nGive an approximate 95% confidence interval for a population mean $\\mu$ \nif the sample of size $n = 25$ has mean $\\mean x = 8.5$ and the population standard deviation is \n$\\sigma = 1.7$.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSE <- 1.7 / sqrt(25) ; SE # standard error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.34\n```\n:::\n\n```{.r .cell-code}\nME <- 2 * SE; ME          # margin of error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.68\n```\n:::\n\n```{.r .cell-code}\n8.5 + c(-1,1) * ME        # approx 95<!--  CI -->\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.82 9.18\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-critical-values}\n**Critical values**\n\nDetermine the critical value $t_*$ for each of the following confidence levels and \nsample sizes.\n\n:::: {.enumerate}\n\na. 95% confidence level; $n = 4$\nb. 95% confidence level; $n = 24$\nc. 98% confidence level; $n = 15$\nd. 90% confidence level; $n = 20$\ne. 99% confidence level; $n = 12$\nf. 95% confidence level; $n = 123$\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nHere's a fancy version that gets all the answers at once.  (You didn't need to know\nyou could do it this way, but R\\ is pretty handy this way.)\n\n::: {.cell}\n\n```{.r .cell-code}\nanswers <- qt(c(.975, .975, .99, .95, .995, .975), df = c(3,23,14,19,11,122))\nnames(answers) <- letters[1:length(answers)]\nanswers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       a        b        c        d        e        f \n3.182446 2.068658 2.624494 1.729133 3.105807 1.979600 \n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-stride-rates}\n**Stride rate**\n\nBelow is a normal-quantile plot and some summary information from a \nsample of the stride rates (strides per second) \nof healthy men.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-29-1.png){width=432}\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"response\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"min\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q1\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"median\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"max\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sd\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[9],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"missing\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"strideRate\",\"2\":\"0.78\",\"3\":\"0.8575\",\"4\":\"0.93\",\"5\":\"0.96\",\"6\":\"1.06\",\"7\":\"0.9255\",\"8\":\"0.0809467\",\"9\":\"20\",\"10\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n:::: {.enumerate}\n\na. What is the standard error of the mean for this sample?\nb. Construct a 98% confidence interval \n\t\t\tfor the mean stride rate of healthy men.\nc. Does the normal-quantile plot suggest any reasons to worry about the \n\t\t\tnormality assumption?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test( ~ strideRate, data = ex07.37, conf.level = 0.98)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  strideRate\nt = 51.132, df = 19, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n98 percent confidence interval:\n 0.8795348 0.9714652\nsample estimates:\nmean of x \n   0.9255 \n```\n:::\n:::\n\n\nThe normal quantile plot is OK but not great.  There are some flat spots in\nthe plot indicating ties.  This is probably because the number of steps was\ncounted over a short length of time, so the underlying data is probably\ndiscrete -- leading to the ties.  But the distribution is unimodal and \nnot heavily skewed.  With a sample size of 20 we can probably proceed \ncautiously.  (In practice, we might want to confirm the results with a method\nthat doesn't require such a strong normality assumption, but we don't know\nany of those methods yet.)\n::: \n<!-- end solution -->\n\n\n<!-- ::: {.problem} -->\n\n<!-- Lake Mary problem. -->\n<!-- :::  -->\n<!-- end problem -->\n\n\n<!-- % \\begin{problem} -->\n<!-- % \tThe \\dataframe{lakemary} data set in the \\pkg{Stob} package describes the  -->\n<!-- % \tlengths and ages of a sample of fish.  Since size is likely associated with -->\n<!-- % \tage, let's create a subset of this sample that includes only the fish  -->\n<!-- % \tthat are 4 years old: -->\n<!-- % <<>>= -->\n<!-- % tally(~Age, lakemary) -->\n<!-- % fish4 <- subset(lakemary, Age==4)  # note the double == here -->\n<!-- % tally(~Age, fish4) -->\n<!-- % @ -->\n<!-- % \t\\begin{enumerate} -->\n<!-- %\\item -->\n<!-- % \t\t\tCompute a 95\\% confidence interval for the mean length  -->\n<!-- % \t\t\tof a 4-year-old fish in Lake Mary. -->\n<!-- %\\item -->\n<!-- % \t\t\tWhat assumptions must we be willing to make if we are  -->\n<!-- % \t\t\tgoing to interpret this confidence interval in the usual way? -->\n<!-- %\\item -->\n<!-- % \t\t\tWhat fraction of the fish in this sample have lengths within -->\n<!-- % \t\t\tthe confidence interval you just constructed?  Does this give  -->\n<!-- % \t\t\tany cause for worry? -->\n<!-- %\\item -->\n<!-- % \t\t\tRepeat this for 3-year-old fish.  Does this suggest that  -->\n<!-- % \t\t\t4-year-old fish are indeed larger than 3-year-old fish?   -->\n<!-- % \t\t\t -->\n<!-- % \t\t\t(Note: This is not really the right way to make this sort of comparison. -->\n<!-- % \t\t\tThere is a much better way. Stay tuned.) -->\n<!-- % \t\\end{enumerate} -->\n<!-- % \\end{problem} -->\n<!-- %  -->\n<!-- % \\begin{solution} -->\n<!-- % \t\\begin{enumerate} -->\n<!-- %\\item 4-year-old fish -->\n<!-- % Here's the 95\\% confidence interval. -->\n<!-- % <<>>= -->\n<!-- % confint(t.test( ~ Length, data = fish4)) -->\n<!-- % @ -->\n<!-- % \\noindent -->\n<!-- % We must be willing to assume that the sampling process produces a reasonable approximation -->\n<!-- % to a random (iid) sample.  (We might need to know something about how the fish were  -->\n<!-- % captured to decide if this is reasonable.  A method of catching fish that works better for  -->\n<!-- % larger fish than for smaller fish, or vice versa, would be problematic.)   -->\n<!-- % A normal-quantile plot doesn't suggest any reason to be overly worried about -->\n<!-- % the normality assumption. -->\n<!-- % <<>>= -->\n<!-- % qqmath(~Length, data = fish4) -->\n<!-- % @ -->\n<!-- % <<tidy = FALSE>>= -->\n<!-- % int <- confint(t.test( ~Length, data = fish4)) -->\n<!-- % int -->\n<!-- % lo <- int[2] -->\n<!-- % hi <- int[3] -->\n<!-- % tally ( ~ (lo < Length & Length < hi), data = fish4) -->\n<!-- % @ -->\n<!-- % None of the fish have a length inside the confidence interval -- this is far below 95\\%.   -->\n<!-- % \\textbf{\\emph{This is NOT a problem.}} -->\n<!-- % The confidence interval is not about the lengths of individual fish, it is about  -->\n<!-- % the mean for the population of fish.  (Side note: It looks like many of the measurements -->\n<!-- % were rounded to the nearest 10 cm, and since our confidence interval fit between 150 and 160, -->\n<!-- % there were no fish of those lengths.) -->\n<!-- %  -->\n<!-- %\\item 3-year-old fish -->\n<!-- % <<tidy = FALSE>>= -->\n<!-- % fish3 <- subset(lakemary, Age==3)  # note the double == here -->\n<!-- % qqmath(~Length, data = fish3) -->\n<!-- % int <- confint(t.test(~Length, data = fish3)) -->\n<!-- % int -->\n<!-- % lo <- int[2]; hi <- int[3] -->\n<!-- % lo -->\n<!-- % hi -->\n<!-- % tally ( ~ (lo < Length & Length < hi), data = fish3) -->\n<!-- % @ -->\n<!-- %\\item -->\n<!-- % \t\tSince the two confidence intervals do not overlap, we have a pretty strong -->\n<!-- % \t\tindication that the 4-year-olds are (on average) longer.  This isn't really -->\n<!-- % \t\tthe right way to test for this -- we'll learn better ways soon. -->\n<!-- %  -->\n<!-- % \t\tHere is a plot that shows how length varies by age. -->\n<!-- % <<>>= -->\n<!-- % bwplot(Length ~ factor(Age), data = lakemary) -->\n<!-- % xyplot(Length ~ factor(Age), data = lakemary, cex = 2, alpha = 0.3) -->\n<!-- % @ -->\n<!-- % \\end{enumerate} -->\n<!-- % \\end{solution} -->\n\n::: {.problem #.exr-e-glass-fiber}\n**E-glass fiber**\n\n<!--  Devore7; prob 7.32 -->\nA random sample of size $n = 8$ E-glass fiber test specimens of a certain \ntype yielded a sample mean interfacial shear yield stress of 30.2 and a sample\nstandard deviation of 3.1.  Assuming that the population of interfacial shear \nyield stress measurements is approximately normal, compute a 95% confidence\ninterval for the true average stress.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSE <- 3.1 / sqrt(8); SE              # standard error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.096016\n```\n:::\n\n```{.r .cell-code}\nt_star <- qt(0.975, df = 7); t_star  # critical value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.364624\n```\n:::\n\n```{.r .cell-code}\nme <- t_star * SE; me                # margin of error\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.591665\n```\n:::\n\n```{.r .cell-code}\n30.2 + c(-1,1) * me                  # the 95<!--  confidence interval -->\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27.60834 32.79166\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-polymerization-degree}\n**Polymerization of paper**\n\n<!--  Devore7; prob 7.33 -->\nThe code below will create a data set containing a sample of observations\nof polymerization degree for some paper specimens.  The data have been \nsorted to assist in typing.  (If the data actually occurred in this order,\nwe would probably be doing a different sort of analysis.)\n\n::: {.cell}\n\n```{.r .cell-code}\nPaper <- \n\ttibble(\n\t  polymer = \n\t    c(418, 421, 421, 422, 425, 427, 431, 434, \n        437, 439, 446, 447, 448, 453, 454, 463, 465))\n```\n:::\n\n\n:::: {.enumerate}\n\na. Create a normal-quantile plot to see if there are any reasons\n\t\t\tto worry about the assumption that the population is approximately normal.\nb. Calculate \n\t\t\t\ta 95% confidence interval for the mean\n\t\t\t\tdegree of polymerization for the population of such paper runs.\n\t\t\t\tThe authors of the paper did this too.\nc. Based on your confidence interval, is 440 a plausible value\n\t\t\tfor the mean degree of polymerization?  Explain.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_qq( ~ polymer, data = Paper)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-33-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_density( ~ polymer, data = Paper)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-33-2.png){width=432}\n:::\n\n```{.r .cell-code}\nt.test( ~ polymer, data = Paper)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  polymer\nt = 119.33, df = 16, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 430.5077 446.0805\nsample estimates:\nmean of x \n 438.2941 \n```\n:::\n:::\n\n\nThe tails of the sample distribution do not stretch quite as far as we would expect \nfor a normal distribution, but the distribution is unimodal and not heavily skewed,\nso we are probably still OK.\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-alice-and-bob-cis}\n**Alice and Bob**\n\nUsing the same data, Alice constructs a 95% confidence interval\nand Bob creates a 98% confidence interval.  Which interval will be wider?  Why?\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nBob's interval will be wider because he used a higher level of confidence.  (This will\ncause $t_*$ to be larger.  They will both have the same standard error.)\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-charlie-and-denise-physics-lab}\n**Lab partners**\n\nCharlie and Denise are working on the same physics lab.  Charlie leaves lab early\nand only has a sample size of $n = 15$.  Denise stays longer and has a sample\nsize of $n = 45$.  Each of them construct a 95% confidence interval from their\nsamples.\n\n:::: {.enumerate}\n\n#. Whose confidence interval would you expect to be wider?\n#. Under what conditions could it be the other way around?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nWe would expect the two standard deviations to be fairly close, but since Denise \nhas quite a bit more data, her standard error will be smaller.  This will make her\ninterval narrower.  So Charlie's interval will be wider, unless Charlie gets an unusually \nsmall standard deviation and Denise gets an unusually large standard deviation.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-find-article-with-ci}\n**Find and article**\n\nFind an article from the engineering or science literature that computes a\nconfidence interval for a mean (be careful, you may see confidence intervals\nfor many other parameters) and also reports the sample mean and\nstandard deviation.  Check their computation to see if you both get the\nsame confidence interval.  Give a full citation for the article you used.\n\nGoogle scholar might be a useful tool for this.  Or you might ask an\nengineering or physics professor for an appropriate engineering journal to\npage through in the library.  Since the chances are small that two students\nwill find the same article if working independently, I expect to see lots\nof different articles used for this problem.\n\nIf your article looks particularly interesting or contains statistical \nthings that you don't understand but would like to understand, let me know,\nand perhaps we can do something later in the semester with your article.\nIt's easiest to do this if you can give me a URL for locating the paper online.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nAnswers will vary.\n::: \n<!-- end solution -->\n\n\n\n## {Review Exercises}\n\n::: {.problem #exr-defective-parts}\n**Defective parts**\n\nEven when things are running smoothly, 5% of the parts \nproduced by a certain manufacturing process are defective.\n\n:::: {.enumerate}\n\na. If you select 10 parts at random, what is the probability\nthat none of them are defective?\n:::: \n<!-- end enumerate -->\n\nSuppose you have a quality control procedure for testing parts \nto see if they are defective, \nbut that the test procedure sometimes makes mistakes:\n\n:::: {.itemize}\n\n* If a part is good, it will fail the quality control test 10% of the time.\n* 20% of the defective parts go undetected by the test.\n:::: \n<!-- end itemize -->\n\n:::: {.enumerate}\n\nb. What percentage of the parts will fail the quality control test?\nc. If a part passes the quality control test, what is the probability\nthat the part is defective?\nd. The parts that fail inspection are sold as \"seconds\".\n\tIf you purchase a \"second\", what is the probability that it is \n\tdefective?\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n:::: {.enumerate}\n\na. $\\Prob(\\mbox{none defective}) = \\Prob(\\mbox{all are good}) = \n0.95^{10} = 0.599 = 59.9%$\nb. Even though only $5$% are defective, nearly $14$% fail the quality control:\n\n$$\n\\begin{aligned}\n\\Prob(\\mbox{fail test}) \n&= \\Prob(\\mbox{good and fail}) + \\Prob(\\mbox{bad and fail}) \n\\\\\n& = \\Prob(\\mbox{good}) \\Prob(\\mbox{fail}\\mid \\mbox{good}) \n\t+ \\Prob(\\mbox{bad}) \\Prob(\\mbox{fail}\\mid \\mbox{bad}) \n\t\\\\\n&= 0.95 (0.10) + 0.05 (.80) = 0.095 + 0.04 = 0.135 = 13.5\\%\n\\end{aligned}\n$$\nc. If a part passes QC, the probability that it is defective\ndrops from 5% to just over 1%:  \n\n$$\n\\begin{aligned}\n\\Prob(\\mbox{bad} \\mid \\mbox{pass}) \n&= \\frac{ \\Prob(\\mbox{bad and pass})}{\\Prob(\\mbox{pass})}\n\\\\\n& = \n\\frac{ 0.05 (0.20) }{ 0.865 }\n=\n0.01156\n\\end{aligned}\n$$\n\nThe cost to get this improvement \nin quality is the cost of the QC test plus the cost of discarding $9.5$% of \ngood parts in the QC process.\n\nd. \n\n$$\n\t\\Prob(\\mbox{bad} \\mid \\mbox{fail}) = \n\t\\Prob(\\mbox{bad} and  \\mbox{fail}) = \n\t\\Prob(\\mbox{fail})  \n\t= 0.04 / (0.04 + 0.095)\n\t= 0.2962963\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\n0.04 / (0.04 + 0.095)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2962963\n```\n:::\n:::\n\n\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-qq-to-density}\n**What does the density look like?**\n\nHere is a normal quantile plot from a data set.\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-35-1.png){width=432}\n:::\n:::\n\n\nSketch what a density plot of this same data would look like.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-36-1.png){width=432}\n:::\n:::\n\n\nThe important feature to get right is the direction of the skew.\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-ci-template-problem}\n**Create your own practice problems**\n\nYou should know how to compute confidence intervals for a single \nquantitative variable both \"by hand\" and using `t.test()`\nif you have data.  Here is a template problem you can use to practice\nboth of these.\n\n:::: {.enumerate}\n\na.  Select a data set and a quantitative variable in that data set.\n\t\t\tFor example, `Length` in the `KidsFeet` data\n\t\t\tset.\nb.  Use `df_stats()` to compute some summary statistics.\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_stats ( ~ length, data = KidsFeet)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"response\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"min\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q1\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"median\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"max\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sd\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[9],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"missing\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"length\",\"2\":\"21.6\",\"3\":\"24\",\"4\":\"24.5\",\"5\":\"25.6\",\"6\":\"27.5\",\"7\":\"24.72308\",\"8\":\"1.317586\",\"9\":\"39\",\"10\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nc.  Pick a confidence level.  Example: 90% confidence.\nd.  From this information, compute a confidence interval\ne.  Now check that you got it right using `t.test()`\nf.  It is possible that you chose a variable for which \n\t\t\tthis is not an appropriate procedure.  Be sure to check for that, too.\n:::: \n<!-- end enumerate -->\n\nYou can find other data sets using \n\n::: {.cell}\n\n```{.r .cell-code}\ndata()\n```\n:::\n\n\nYou won't run out of examples before you run out of energy for doing these.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_stats( ~ length, data = KidsFeet)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"response\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"min\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q1\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"median\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"max\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sd\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[9],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"missing\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"length\",\"2\":\"21.6\",\"3\":\"24\",\"4\":\"24.5\",\"5\":\"25.6\",\"6\":\"27.5\",\"7\":\"24.72308\",\"8\":\"1.317586\",\"9\":\"39\",\"10\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nx_bar <- 24.72\nn <- 39; n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 39\n```\n:::\n\n```{.r .cell-code}\ns <- 1.32; s\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.32\n```\n:::\n\n```{.r .cell-code}\nSE <- s / sqrt(n) ; SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2113692\n```\n:::\n\n```{.r .cell-code}\nt_star <- qt(0.95, df = 38) ; t_star\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.685954\n```\n:::\n\n```{.r .cell-code}\nme <- t_star * SE; me\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3563588\n```\n:::\n\n```{.r .cell-code}\nconfint(t.test( ~length, data = KidsFeet)) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mean of x\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lower\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upper\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"level\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"24.72308\",\"2\":\"24.29597\",\"3\":\"25.15019\",\"4\":\"0.95\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ngf_qq( ~length, data = KidsFeet)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-39-1.png){width=432}\n:::\n:::\n\n\nThe normal-quantile plot does not reveal any causes for concern, but \nwe might be concerned that the distributions for boys and girls are different.  Let's do\na graphical check to see if that is a problem.\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_boxplot(length ~ sex, data = KidsFeet)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-40-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_dens( ~ length, color = ~sex, linewidth = 1.5, data = KidsFeet)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-40-2.png){width=432}\n:::\n:::\n\n\nIndeed, it appears that the girls' feet are on average a bit shorter, so perhaps \nwe should create a confidence intervals separately for each group.  Now you have two\nproblems.  You can get started with\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_stats(length ~ sex, data = KidsFeet)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"response\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"sex\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"min\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q1\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"median\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"max\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sd\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"missing\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"length\",\"2\":\"B\",\"3\":\"22.9\",\"4\":\"24.35\",\"5\":\"24.95\",\"6\":\"25.8\",\"7\":\"27.5\",\"8\":\"25.10500\",\"9\":\"1.216758\",\"10\":\"20\",\"11\":\"0\"},{\"1\":\"length\",\"2\":\"G\",\"3\":\"21.6\",\"4\":\"23.65\",\"5\":\"24.20\",\"6\":\"25.1\",\"7\":\"26.7\",\"8\":\"24.32105\",\"9\":\"1.330238\",\"10\":\"19\",\"11\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-working-with-pdf}\n**Working with a pdf**\n\nThe **pdf** for a continuous random variable $X$ is\n\n$$\nf(x) = \\begin{cases}\n4(x - x^3) & \\mbox{when } 0\\le x \\le 1 \\\\\n0 & \\mbox{otherwise}\n\\end{cases}\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-42-1.png){width=432}\n:::\n:::\n\n\n\n:::: {.enumerate}\n\na. Determine $\\Prob(X \\le \\frac{1}{2})$.\nb. Determine the mean and variance of $X$.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mosaicCalc)\n  f <- makeFun(4*(x-x^3) ~ x)\n  F <- antiD(f(x) ~ x)\n xF <- antiD(x * f(x) ~ x)\nxxF <- antiD(x^2 * f(x) ~ x)\nF(1) - F(0)                   # should be 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nm <- xF(1) - xF(0); m         # mean\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5333333\n```\n:::\n\n```{.r .cell-code}\nxxF(1) - xxF(0) - m^2         # variance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04888889\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\t\n\n::: {.problem #exr-working-with-a-kernel}\n**Working with a kernel**\n\nThe kernel of a continuous distribution is given by $k(x) = 4-x^2$\non the interval $[-2,2]$.  \n\n:::: {.enumerate}\n\na.  Determine the pdf of the distribution.\nb.  Compute the mean and variance of the distribution.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nF <- antiD(4 - x^2 ~ x)\nF(2) - F(-2) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.66667\n```\n:::\n:::\n\n\nSo the pdf is \n\n$$\n\tf(x) = \\frac{4 - x^2}{10.6666667}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun((4 - x^2) / C ~ x, C = F(2) - F(-2))\nintegrate(f, -2, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1 with absolute error < 1.1e-14\n```\n:::\n\n```{.r .cell-code}\nF <- antiD(f(x) ~ x)\nF(2) - F(-2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nxF <- antiD(x*f(x) ~ x)\nm <- xF(2) - xF(-2)           # mean\nxxF <- antiD(x*x*f(x) ~ x)  \nxxF(2) - xxF(-2) - m^2        # variance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n\n::: {.problem #exr-working-with-gamma}\n**Gamma distributions**\n\nLet $X \\sim \\Gamm(\\texttt{shape} = 3, \\texttt{rate} = 2)$.\n\n:::: {.enumerate}\n\na. Plot the pdf of the distribution of $X$.\nb. Determine $\\Prob(X \\le 1)$.\nc. What proportion of the distribution is between $1$ and $3$?\nd. Use the table to determine the mean, variance, and standard deviation\n\t\tof $X$.\ne. Use integration to determine the mean, variance, and standard deviation\n\t\t\tof $X$.  (You should get the same answers as above.)\nf. Make up other problems like this for any of the distributions\n\t\t\tin the table if you want additional practice.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_dist(\"gamma\", params = list(shape = 3, rate = 2))\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-46-1.png){width=432}\n:::\n\n```{.r .cell-code}\npgamma(1, shape = 3, rate = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3233236\n```\n:::\n\n```{.r .cell-code}\npgamma(3, shape = 3, rate = 2) -  pgamma(1, shape = 3, rate = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6147076\n```\n:::\n\n```{.r .cell-code}\nF <- antiD(dgamma(x, shape = 3, rate = 2) ~ x)\nF(Inf) - F(0)    # should be 1.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nxF <- antiD(x * dgamma(x, shape = 3, rate = 2) ~ x)\nm <- xF(Inf) - xF(0); m\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.5\n```\n:::\n\n```{.r .cell-code}\nxxF <- antiD(x^2 * dgamma(x, shape = 3, rate = 2) ~ x)\nxxF(Inf) - xxF(0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n\n```{.r .cell-code}\nxxF(Inf) - xxF(0) - m^2 # variance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.75\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n::: {.problem #exr-linear-combos}\n**Linear combinations**\n\nSuppose $X$ an $Y$ are independent random variables with means and \nstandard deviations as given in the following table.\n\n|     | mean   | standard deviation |\n|-----|--------|--------------------|\n|\t$X$ | 40     | 3                  |\n|\t$Y$ | 50     | 4                  |\n\n\nDetermine the mean and standard deviation of the following:\n\na. $X+Y$\nb. $X-Y$\nc. $\\frac{1}{2} X + 7$\n\nReminder: Expected value is another term for mean.\n::: \n<!-- end problem -->\n\n\n\n::: {.solution}\n\n$\\E(X+Y) = \\E(X) + \\E(Y) = 40 + 50 = 90$.\n\n$\\Var(X+Y) = \\Var(X) + \\Var(Y) = 3^2 + 4^2 = 5^2$.\nSo standard deviation = $5$.\n\n$\\E(X-Y) = \\E(X) - \\E(Y) = 40 - 50 = -10$.\n\n$\\Var(X-Y) = \\Var(X + (-Y)) = \\Var(X) + \\Var(-Y) = 3^2 + 4^2 = 5^2$.\nSo standard deviation = $5$.\n\n$\\E(\\frac12 X+7) = \\frac12\\E(X) + 7 = 27$.\n\n$\\Var(\\frac12 X+7) = \\frac14\\Var(X) = \\frac94$.\nSo standard deviation = $\\sqrt{\\frac94}$.\n::: \n<!-- end solution -->\n\n\n\n<!-- \\iffalse -->\n<!-- %% This data was among the CMU data sets that we removed from mosaic -->\n<!-- \\begin{problem} -->\n<!-- \tThe \\dataframe{StudentSurvey} data set  -->\n<!-- \tcontains student responses to a survey. -->\n<!-- \\begin{enumerate} -->\n<!-- %\\item -->\n<!-- %\t\t\tWhat percent of the students in the survey had cell phones? -->\n<!-- \\item What were the mean and standard deviation of the number  -->\n<!-- \t\t\tof minutes students spend exercising on a typical day? -->\n<!-- \\item What percent of students responded that they don't exercise -->\n<!-- \t\t\tat all on a typical day? -->\n<!-- \\item Construct a histogram, a density plot, and a normal-quantile -->\n<!-- \t\t\tplot for number of minutes exercised by these students. -->\n<!-- \t\t\tComment on any interesting features. -->\n<!-- \\item Fit a distribution to this distribution.  What family of distributions -->\n<!-- \t\t\tdid you choose?  Why?  How well does it appear to fit? -->\n<!-- \t\t\t(Remember that for some distributions you need to decide how you will -->\n<!-- \t\t\thandle 0's and missing values.) -->\n<!-- \\end{enumerate} -->\n<!-- \\end{problem} -->\n\n<!-- \\begin{solution} -->\n<!-- Notice how similar all of these commands are.  (Also that if there is missing data, \\R\\ reports -->\n<!-- NA when you ask for the mean and standard deviation.) -->\n<!-- <<eval=TRUE, >>= -->\n<!-- mean( ~ Exer, data = StudentSurvey) -->\n<!-- mean( ~ Exer, data = StudentSurvey, na.rm = TRUE) -->\n<!-- sd( ~ Exer, data = StudentSurvey, na.rm = TRUE) -->\n<!-- tally( ~ (Exer == 0) , data = StudentSurvey, format = \"percent\") -->\n<!-- gf_dhistogram( ~ Exer, data = StudentSurvey) -->\n<!-- gf_density( ~ Exer, data = StudentSurvey) -->\n<!-- gf_qq( ~ Exer, data = StudentSurvey) -->\n<!-- @ -->\n<!-- Let's fit a gamma, since values cannot be below 0 and the distribution is clearly skewed. -->\n<!-- (Weibull might be another choice.) -->\n<!-- Remember that we need to remove any values of 0 before we do the fit.  In this case, we will -->\n<!-- do this by mapping them to a value of 0.5 since the smallest non-zero value in the data set -->\n<!-- is 1.  We also have to remove the two students who did not report. -->\n<!-- <<eval=TRUE, warning=FALSE>>= -->\n<!-- StudentSurvey2 <- StudentSurvey |> -->\n<!--   filter(!is.na(Exer)) |>  # remove non-responders -->\n<!--   mutate(Exer2 = ifelse(Exer <= 0, 0.5, Exer)) -->\n<!-- fitdistr(StudentSurvey2$Exer2, \"gamma\") -->\n<!-- gf_dhistogram( ~ Exer2, data = StudentSurvey2) |>  -->\n<!--   gf_fitdistr(dist = \"gamma\") -->\n<!-- gf_qq( ~ Exer2, data=StudentSurvey2, dist=qgamma, dparams = list(shape=0.442, rate=0.0124)) -->\n<!-- @ -->\n<!-- \\end{solution} -->\n<!-- \\fi -->\n\n::: {.problem #exr-vocab-review}\n**Terminology review**\n\n::: {.enumerate}\n\na. What is the difference between a statistic and a parameter?\nb. What is a sampling distribution?\nc. Create other similar problems using important terms from this class.\n::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n\n<!--  This data was among the CMU data sets that have been pulled from the mosaic package -->\n::: {.problem #exr-flicker}\n**Critical flicker frequency**\n\nCritical flicker frequency (called `Flicker` in the data set below) is the \nlowest flicker rate at which the human eye\ncan detect that a light source (from a flourenscent bulb or a computer screen, for example)\nis flickering.  Knowing the cff is important for product manufacturing (since detectable\nflickering is annoying for the consumer).  The command below loads data from \nfrom a 1973 study that attempted to determine whether the cff, which varies \nfrom person to person, is partially determined by eye color.  \n\n::: {.cell}\n\n```{.r .cell-code}\nFlicker <- read.file(\"http://www.statsci.org/data/general/flicker.txt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nReading data with read.table()\n```\n:::\n:::\n\n\n\nCreate a plot that can be used to visualise the data.  What does your plot suggest\nthe answer might be? (Does eye color matter?)\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\nHere are two possibilities:\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_boxplot(Flicker ~ Colour, data = Flicker)\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-48-1.png){width=432}\n:::\n\n```{.r .cell-code}\ngf_density( ~ Flicker, fill=~Colour, data = Flicker) |>\n  gf_refine(scale_fill_manual(values = c('steelblue', 'tan4', 'forestgreen')))\n```\n\n::: {.cell-output-display}\n![](05-transformations_files/figure-html/unnamed-chunk-48-2.png){width=432}\n:::\n:::\n\n\nThe general trend appears to be that lighter colored eyes (if you consider blue\nto be lighter than green) appear to be able to detect higher flicker rates, but\nthere is overlap among the groups.\n\n{{< pagebreak >}}\n\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-difference-between-two-means}\n**Difference between two means**\n\nLet $\\mean X_1$ be the distibution of sample means from a population with mean $\\mu_1$ and standard deviation $\\sigma_1$.\nLet $\\mean X_2$ be the distibution of sample means from a population with mean $\\mu_2$ and standard deviation $\\sigma_2$.\nWhat is the distribution of $\\mean X_1 - \\mean X_2$? (Hint: start by determining the distributions of \n$\\mean X_1$ and $\\mean X_2$, then use what we know about differences.)\n::: \n<!-- end problem -->\n\n\n::: {.problem #exr-difference-between-two-proportions}\n**Difference between two proportions**\n\nSimilar to the previous problem, we can work out the (approximate) sampling distribution for the difference \nbetween two proportions. Use the fact that the sampling distribution for a sample proportion is approximately\n$\\Norm(p, \\sqrt{\\frac{p(1-p)}{\\sqrt{n}}})$ to work out the (approximate) sampling distribution for the \ndifference between two sample proportions.\n\n{{< pagebreak >}}\n\n\n\n::: \n<!-- end problem -->\n\n\n::: {.problem #exr-concrete-beams-load}\n**Concrete beams**\n\nBelow is the summary of some data from a study \ncomparing two types of concrete beams by measuring their ultimate load (in kN).\n\n| type                   | sample size   | mean   | standard deviation |\n|------------------------|---------------|--------|--------------------|\n|\tfiberglass grid        | 26            | 33.4   | 2.2                |\n|\tcommercial carbon grid | 26            | 42.8   | 4.3                |\n\n\nCreate a 95% confidence interval for the difference in the mean ultimate load for \nthe two types of beam.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSE <- sqrt( 2.2^2 / 26 + 4.3^2 / 26); SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9472633\n```\n:::\n\n```{.r .cell-code}\ntstar <- qt(0.975, df = 25); tstar   # conservative\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.059539\n```\n:::\n\n```{.r .cell-code}\n42.8 - 33.4 + c(-1,1) * tstar * SE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  7.449075 11.350925\n```\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-fabric-extensibility}\n**Fabric extensibility**\n\nThe `Devore7::ex9.23` data set has data on the extensibility (%) of two types of fabric, but it is \narranged awkwardly. Let's fix that:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\nFabric <- \n  Devore7::ex09.23 |> \n  pivot_longer( everything(), names_to = \"quality\", values_to = \"extensibility\",\n                values_drop_na = TRUE) \nhead(Fabric) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"quality\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"extensibility\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"H\",\"2\":\"1.2\"},{\"1\":\"P\",\"2\":\"1.6\"},{\"1\":\"H\",\"2\":\"0.9\"},{\"1\":\"P\",\"2\":\"1.5\"},{\"1\":\"H\",\"2\":\"0.7\"},{\"1\":\"P\",\"2\":\"1.1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ndf_stats(extensibility ~ quality, data = Fabric)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"response\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"quality\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"min\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q1\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"median\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"max\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sd\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"missing\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"extensibility\",\"2\":\"H\",\"3\":\"0.7\",\"4\":\"1.175\",\"5\":\"1.6\",\"6\":\"1.825\",\"7\":\"2.3\",\"8\":\"1.508333\",\"9\":\"0.4442059\",\"10\":\"24\",\"11\":\"0\"},{\"1\":\"extensibility\",\"2\":\"P\",\"3\":\"1.0\",\"4\":\"1.250\",\"5\":\"1.5\",\"6\":\"1.725\",\"7\":\"2.6\",\"8\":\"1.587500\",\"9\":\"0.5303301\",\"10\":\"8\",\"11\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n:::: {.enumerate}\n\na. Use the summary above to compute a 95% confidence interval for the difference in mean extensitibility\nfor the two types of fabric.\nb. Then use `t.test()` to compute the confidence interval.  How do the degrees of freedom\nfrom `t.test()` compare with the upper and lower bounds we have for degrees of freedom?\nWhich end of the range is it closer to?  Why?\nc. Create normal-quantile plots for each group.  Does it seem plausible that that the populations \nare normal? Is it important that this be plausible?\n:::: \n<!-- end enumerate -->\n\n\nExtensibility is related to the shape and drape when fabric is used to make clothing.\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\nFabric <- \n  Devore7::ex09.23 |> \n  pivot_longer( everything(), names_to = \"quality\", values_to = \"extensibility\",\n                values_drop_na = TRUE) \nhead(Fabric)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"quality\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"extensibility\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"H\",\"2\":\"1.2\"},{\"1\":\"P\",\"2\":\"1.6\"},{\"1\":\"H\",\"2\":\"0.9\"},{\"1\":\"P\",\"2\":\"1.5\"},{\"1\":\"H\",\"2\":\"0.7\"},{\"1\":\"P\",\"2\":\"1.1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ndf_stats(extensibility ~ quality, data = Fabric)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"response\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"quality\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"min\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q1\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"median\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Q3\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"max\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sd\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"missing\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"extensibility\",\"2\":\"H\",\"3\":\"0.7\",\"4\":\"1.175\",\"5\":\"1.6\",\"6\":\"1.825\",\"7\":\"2.3\",\"8\":\"1.508333\",\"9\":\"0.4442059\",\"10\":\"24\",\"11\":\"0\"},{\"1\":\"extensibility\",\"2\":\"P\",\"3\":\"1.0\",\"4\":\"1.250\",\"5\":\"1.5\",\"6\":\"1.725\",\"7\":\"2.6\",\"8\":\"1.587500\",\"9\":\"0.5303301\",\"10\":\"8\",\"11\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nt.test(extensibility ~ quality, data = Fabric)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  extensibility by quality\nt = -0.38011, df = 10.482, p-value = 0.7115\nalternative hypothesis: true difference in means between group H and group P is not equal to 0\n95 percent confidence interval:\n -0.5403506  0.3820172\nsample estimates:\nmean in group H mean in group P \n       1.508333        1.587500 \n```\n:::\n:::\n\n::: \n<!-- end solution -->\n\n\n::: {.problem #exr-part-diffprop}\n**A difference in proportions**\n\nA sample of parts from two factories was inspected. At factory A, 172 out of 213 were rated \"highest quality\".\nAt factory B, 184 out of 207 were rated \"highest quality\".\n\n:::: {.enumerate}\n\na. Compute the proportion of highest quality parts produced in each sample.\nb. Compute a 95% confidence interval for the difference in the proportions of parts that receive the \nhighest quality rating from the two factories.  Do this first without using `prop.test()`, then\nrepeat with `prop.test()` and compare the results.\nc. Based on this data, can we be confident that one factory is better than the other? Explain.\n:::: \n<!-- end enumerate -->\n\n::: \n<!-- end problem -->\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test( c(172, 184), c(184, 207)) |> confint()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"prop 1\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"prop 2\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lower\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upper\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"level\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.9347826\",\"2\":\"0.8888889\",\"3\":\"-0.01496743\",\"4\":\"0.1067549\",\"5\":\"0.95\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nprop.test( c(172, 184), c(184, 207), correct = FALSE) |> confint()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"prop 1\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"prop 2\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lower\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upper\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"level\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.9347826\",\"2\":\"0.8888889\",\"3\":\"-0.009834582\",\"4\":\"0.101622\",\"5\":\"0.95\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n::: \n<!-- end solution -->\n\n",
    "supporting": [
      "05-transformations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}